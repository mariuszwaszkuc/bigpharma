{
	"name": "Run All Bronze",
	"properties": {
		"folder": {
			"name": "Arch"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool32",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "8a520db8-8fe1-4d31-a9d6-6153cf1e681d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
				"name": "sparkpool32",
				"type": "Spark",
				"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"##LOAD_DISTRIBUTORS_DATA_PY\n",
					"\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"access_key=mssparkutils.credentials.getSecret('keylakehouse','s3-access-key')\n",
					"\n",
					"secret_key =mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key')\n",
					"\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
					"\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.functions import input_file_name\n",
					"\n",
					"# Ścieżka do DeltaTable z listą przetworzonych plików\n",
					"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/Processed_Files_Logs\"\n",
					"\n",
					"try:\n",
					"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
					"    print(\"Załadowano istniejący plik Delta z przetworzonymi plikami.\")\n",
					"except AnalysisException:\n",
					"   processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
					"   print(\"Plik Delta nie istnieje. Tworzony nowy pusty DataFrame.\")\n",
					"\n",
					"\n",
					"\n",
					"# Parametry MinIO\n",
					"bucket_name = \"distributors\"\n",
					"folder_path = \"data/\"\n",
					"\n",
					"# Wczytanie wszystkich plików CSV z MinIO (z subfolderami)\n",
					"files_metadata = spark.read \\\n",
					"    .option(\"header\", \"true\") \\\n",
					"    .csv(f\"s3a://{bucket_name}/{folder_path}*\") \\\n",
					"    .withColumn(\"input_file\", input_file_name())\n",
					"\n",
					"# Wyciągnięcie nazw plików\n",
					"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
					"\n",
					"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
					"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
					"\n",
					"# Lista już przetworzonych plików\n",
					"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
					"\n",
					"# Wyszukanie nowych plików do przetworzenia\n",
					"new_paths = [paths for paths in all_paths if  paths not in processed_filenames]\n",
					"\n",
					"print(f\"Znaleziono {len(new_paths)} nowych sciezek do przetworzenia.\")\n",
					"\n",
					"# Przetwarzanie nowych plików\n",
					"for path in new_paths:\n",
					"    file_path = path\n",
					"    file_path= file_path.split(\"?\")[0]\n",
					"    print(file_path)\n",
					"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
					"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
					"    print(f\"Wczytywanie sciezki: {file_path}\")\n",
					"    print(f\"Wczytywanie pliku: {file_name}\")\n",
					"    print(f\"Wczytywanie sciezki: {country_name}\")\n",
					"\n",
					"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/{country_name}/{file_name}\"\n",
					"\n",
					"\n",
					"    print(f\" Wczytywanie pliku: {file_path}\")\n",
					"\n",
					"    try:\n",
					"        df_new = spark.read.option(\"header\", \"true\").csv(file_path)\n",
					"        df_new.write.mode(\"overwrite\").csv(target_path)\n",
					"        print(f\"Zapisano plik do ADLS: {target_path}\")\n",
					"    except Exception as e:\n",
					"        print(f\"Błąd podczas przetwarzania pliku {file}: {e}\")\n",
					"\n",
					"# Aktualizacja listy przetworzonych plików\n",
					"if new_paths:\n",
					"    new_files_df = spark.createDataFrame([(path,) for path in new_paths], [\"pathname\"])\n",
					"    processed_files_updated = processed_df.union(new_files_df)\n",
					"    processed_files_updated.show()\n",
					"    # Zapis zaktualizowanej listy w formacie Delta\n",
					"    processed_files_updated.write.format(\"delta\") \\\n",
					"        .mode(\"overwrite\") \\\n",
					"       .save(processed_files_path)\n",
					"\n",
					"    print(f\"Zaktualizowano {processed_files_updated.count()} listę przetworzonych plików w Delta.\")\n",
					"else:\n",
					"    print(\"Brak nowych plików do przetworzenia.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"#LOAD_PHARMACIES_DATA_PY\n",
					"\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"access_key=mssparkutils.credentials.getSecret('keylakehouse','s3-access-key')\n",
					"\n",
					"secret_key =mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key')\n",
					"\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
					"\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.functions import input_file_name\n",
					"\n",
					"# Ścieżka do DeltaTable z listą przetworzonych plików\n",
					"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/Processed_Files_Logs\"\n",
					"\n",
					"try:\n",
					"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
					"    print(\"Załadowano istniejący plik Delta z przetworzonymi plikami.\")\n",
					"except AnalysisException:\n",
					"   processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
					"   print(\"Plik Delta nie istnieje. Tworzony nowy pusty DataFrame.\")\n",
					"\n",
					"\n",
					"\n",
					"# Parametry MinIO\n",
					"bucket_name = \"pharmacies\"\n",
					"folder_path = \"data/\"\n",
					"\n",
					"# Wczytanie wszystkich plików CSV z MinIO (z subfolderami)\n",
					"files_metadata = spark.read \\\n",
					"    .option(\"header\", \"true\") \\\n",
					"    .csv(f\"s3a://{bucket_name}/{folder_path}*\") \\\n",
					"    .withColumn(\"input_file\", input_file_name())\n",
					"\n",
					"# Wyciągnięcie nazw plików\n",
					"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
					"\n",
					"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
					"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
					"\n",
					"# Lista już przetworzonych plików\n",
					"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
					"\n",
					"# Wyszukanie nowych plików do przetworzenia\n",
					"new_paths = [paths for paths in all_paths if  paths not in processed_filenames]\n",
					"\n",
					"print(f\"Znaleziono {len(new_paths)} nowych sciezek do przetworzenia.\")\n",
					"\n",
					"# Przetwarzanie nowych plików\n",
					"for path in new_paths:\n",
					"    file_path = path\n",
					"    file_path= file_path.split(\"?\")[0]\n",
					"    print(file_path)\n",
					"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
					"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
					"    print(f\"Wczytywanie sciezki: {file_path}\")\n",
					"    print(f\"Wczytywanie pliku: {file_name}\")\n",
					"    print(f\"Wczytywanie sciezki: {country_name}\")\n",
					"\n",
					"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/{country_name}/{file_name}\"\n",
					"\n",
					"\n",
					"    print(f\" Wczytywanie pliku: {file_path}\")\n",
					"\n",
					"    try:\n",
					"        df_new = spark.read.option(\"header\", \"true\").csv(file_path)\n",
					"        df_new.write.mode(\"overwrite\").csv(target_path)\n",
					"        print(f\"Zapisano plik do ADLS: {target_path}\")\n",
					"    except Exception as e:\n",
					"        print(f\"Błąd podczas przetwarzania pliku {file}: {e}\")\n",
					"\n",
					"# Aktualizacja listy przetworzonych plików\n",
					"if new_paths:\n",
					"    new_files_df = spark.createDataFrame([(path,) for path in new_paths], [\"pathname\"])\n",
					"    processed_files_updated = processed_df.union(new_files_df)\n",
					"    processed_files_updated.show()\n",
					"    # Zapis zaktualizowanej listy w formacie Delta\n",
					"    processed_files_updated.write.format(\"delta\") \\\n",
					"        .mode(\"overwrite\") \\\n",
					"       .save(processed_files_path)\n",
					"\n",
					"    print(f\"Zaktualizowano {processed_files_updated.count()} listę przetworzonych plików w Delta.\")\n",
					"else:\n",
					"    print(\"Brak nowych plików do przetworzenia.\")\n",
					"\n",
					"\n",
					""
				],
				"execution_count": null
			}
		]
	}
}