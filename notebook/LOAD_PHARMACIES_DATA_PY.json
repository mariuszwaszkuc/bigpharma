{
	"name": "LOAD_PHARMACIES_DATA_PY",
	"properties": {
		"folder": {
			"name": "Load_Bronze"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool32",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "437bbfc6-5857-4e4e-a293-636062b082d7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
				"name": "sparkpool32",
				"type": "Spark",
				"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"#LOAD_PHARMACIES_DATA_PY\n",
					"\n",
					"from notebookutils import mssparkutils\n",
					"from urllib.parse import unquote\n",
					"\n",
					"access_key = mssparkutils.credentials.getSecret('keylakehouse','s3-access-key','AzureKeyVault')\n",
					"secret_key = mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key', 'AzureKeyVault')\n",
					"\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
					"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
					"\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.functions import input_file_name\n",
					"\n",
					"# Path to DeltaTable with the list of processed files\n",
					"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/Processed_Files_Logs\"\n",
					"\n",
					"try:\n",
					"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
					"    print(\"Loaded existing Delta file with processed files.\")\n",
					"except AnalysisException:\n",
					"   processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
					"   print(\"Delta file does not exist. Creating a new empty DataFrame.\")\n",
					"\n",
					"\n",
					"# MinIO parameters\n",
					"bucket_name = \"pharmacies\"\n",
					"folder_path = \"data/country=*/closed_month=*/\"\n",
					"\n",
					"# Read all CSV files from MinIO (including subfolders)\n",
					"files_metadata = spark.read \\\n",
					"    .option(\"header\", \"true\") \\\n",
					"    .csv(f\"s3a://{bucket_name}/{folder_path}*\") \\\n",
					"    .withColumn(\"input_file\", input_file_name())\n",
					"\n",
					"# Extract file names\n",
					"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
					"\n",
					"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
					"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
					"\n",
					"# List of already processed files\n",
					"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
					"\n",
					"# Find new files to process\n",
					"new_paths = [paths for paths in all_paths if unquote(paths) not in processed_filenames]\n",
					"\n",
					"print(f\"Found {len(new_paths)} new paths to process.\")\n",
					"\n",
					"# Process new files\n",
					"for path in new_paths:\n",
					"    file_path = path\n",
					"    file_path = file_path.split(\"?\")[0]\n",
					"    print(file_path)\n",
					"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
					"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
					"    print(f\"Loading path: {file_path}\")\n",
					"    print(f\"Loading file: {file_name}\")\n",
					"    print(f\"Loading path: {country_name}\")\n",
					"\n",
					"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/{country_name}/{file_name}\"\n",
					"\n",
					"    print(f\"Loading file: {file_path}\")\n",
					"\n",
					"    try:\n",
					"        df_new = spark.read.option(\"header\", \"true\").csv(unquote(file_path))\n",
					"        df_new.write.option(\"header\", \"true\").mode(\"overwrite\").csv(unquote(target_path))\n",
					"        print(f\"Saved file to ADLS: {target_path}\")\n",
					"    except Exception as e:\n",
					"        print(f\"Error processing file {file_path}: {e}\")\n",
					"\n",
					"# Update the list of processed files\n",
					"if new_paths:\n",
					"    new_files_df = spark.createDataFrame([(unquote(path),) for path in new_paths], [\"pathname\"])\n",
					"    processed_files_updated = processed_df.union(new_files_df)\n",
					"    processed_files_updated.show()\n",
					"    # Save updated list in Delta format\n",
					"    processed_files_updated.write.format(\"delta\") \\\n",
					"        .mode(\"overwrite\") \\\n",
					"       .save(processed_files_path)\n",
					"\n",
					"    print(f\"Updated the processed files list in Delta with {processed_files_updated.count()} entries.\")\n",
					"else:\n",
					"    print(\"No new files to process.\")\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}