{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "mrgbigpharma"
		},
		"AmazonS3Compatible2_secretAccessKey": {
			"type": "secureString",
			"metadata": "Secure string for 'secretAccessKey' of 'AmazonS3Compatible2'"
		},
		"MinIoS3_secretAccessKey": {
			"type": "secureString",
			"metadata": "Secure string for 'secretAccessKey' of 'MinIoS3'"
		},
		"S3MinIO_secretAccessKey": {
			"type": "secureString",
			"metadata": "Secure string for 'secretAccessKey' of 'S3MinIO'"
		},
		"mrgbigpharma-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'mrgbigpharma-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:mrgbigpharma.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AmazonS3Compatible2_properties_typeProperties_accessKeyId": {
			"type": "string",
			"defaultValue": "minioadmin"
		},
		"MinIoS3_properties_typeProperties_accessKeyId": {
			"type": "string",
			"defaultValue": "minioadmin"
		},
		"MySqlErp_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "@{linkedService().Server}"
		},
		"MySqlErp_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "@{linkedService().Database}"
		},
		"MySqlErp_properties_typeProperties_username": {
			"type": "string",
			"defaultValue": "@{linkedService().User}"
		},
		"MySqlPassword_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://keylakehouse.vault.azure.net/"
		},
		"S3MinIO_properties_typeProperties_accessKeyId": {
			"type": "string",
			"defaultValue": "minioadmin"
		},
		"eceuropaeu_inflation_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/ei_cphi_m$defaultview/?format=TSV&compressed=false"
		},
		"mrgbigpharma-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bigpharma.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Extract Erp Data')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "List of Tables",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "MySqlSource",
								"query": "call Set_Initial_Dates_In_Tables();\n\nSELECT table_name, create_date, update_date, \n\nlast_date FROM logs;"
							},
							"dataset": {
								"referenceName": "MySql_Erp_List_Tables",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Copy For Each Table",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "List of Tables",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('List of Tables').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "If Condition1",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@equals(item().table_name, 'logs') ",
											"type": "Expression"
										},
										"ifFalseActivities": [
											{
												"name": "Extract Table Copy To ADLS",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "MySqlSource",
														"query": {
															"value": "@concat(\n    'SELECT * FROM ', \n    item().table_name,\n    ' WHERE \n    update_date <=  \n    (SELECT MAX(update_date) AS update_date  \n     FROM logs \n     WHERE table_name = ''', item().table_name, ''' )\n    AND update_date >= STR_TO_DATE(''', item().last_date, ''', ''%Y-%m-%dT%H:%i:%s'');'\n)",
															"type": "Expression"
														}
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings",
															"copyBehavior": "FlattenHierarchy"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"translator": {
														"type": "TabularTranslator",
														"typeConversion": true,
														"typeConversionSettings": {
															"allowDataTruncation": true,
															"treatBooleanAsNumber": false
														}
													}
												},
												"inputs": [
													{
														"referenceName": "MySql_Erp_Dynamic_Load",
														"type": "DatasetReference",
														"parameters": {}
													}
												],
												"outputs": [
													{
														"referenceName": "Bronze_Parquet",
														"type": "DatasetReference",
														"parameters": {
															"filename": {
																"value": "@if( equals(item().table_name, 'logs') , \n     concat(item().table_name,'.parquet'),\n     concat(item().table_name,'_',formatDateTime(item().last_date,'yyyy-MM-dd'),'.parquet'))",
																"type": "Expression"
															},
															"foldername": "@item().table_name"
														}
													}
												]
											}
										],
										"ifTrueActivities": [
											{
												"name": "Logs To ADLS",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "MySqlSource",
														"query": {
															"value": "@concat(\n 'Select * from ',\n item().table_name)\n",
															"type": "Expression"
														}
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"translator": {
														"type": "TabularTranslator",
														"typeConversion": true,
														"typeConversionSettings": {
															"allowDataTruncation": true,
															"treatBooleanAsNumber": false
														}
													}
												},
												"inputs": [
													{
														"referenceName": "MySql_Erp_Dynamic_Load",
														"type": "DatasetReference",
														"parameters": {}
													}
												],
												"outputs": [
													{
														"referenceName": "Bronze_Parquet",
														"type": "DatasetReference",
														"parameters": {
															"filename": {
																"value": "@if( equals(item().table_name, 'logs') , \n     concat(item().table_name,'.parquet'),\n     concat(item().table_name,'_',formatDateTime(item().update_date,'yyyy-MM-ddTHH:mm:s'),'.parquet'))",
																"type": "Expression"
															},
															"foldername": "@item().table_name"
														}
													}
												]
											}
										]
									}
								}
							]
						}
					},
					{
						"name": "Setup Last Update Date",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "Copy For Each Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "MySqlSource",
								"query": "call serwer296707_pharma.SetMaxIdForAllTables;\n\nSELECT table_name, create_date, update_date, \n\nlast_date FROM logs;"
							},
							"dataset": {
								"referenceName": "MySql_Erp_List_Tables",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "Brozne"
				},
				"annotations": [],
				"lastPublishTime": "2025-03-27T05:52:42Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/MySql_Erp_List_Tables')]",
				"[concat(variables('workspaceId'), '/datasets/MySql_Erp_Dynamic_Load')]",
				"[concat(variables('workspaceId'), '/datasets/Bronze_Parquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Extract MinIO Data')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Distributors Data MinIo",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "LOAD_DISTRIBUTORS_DATA_PY",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					},
					{
						"name": "Pharmacies Data MinIo",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Distributors Data MinIo",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "LOAD_PHARMACIES_DATA_PY",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "Brozne"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/LOAD_DISTRIBUTORS_DATA_PY')]",
				"[concat(variables('workspaceId'), '/notebooks/LOAD_PHARMACIES_DATA_PY')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Extract_Load_Erp_copy1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "List of Tables With New Max Date",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "MySqlSource",
								"query": "call GetMaxIdForAllTables();\n\nSELECT table_name, create_date, update_date, \n\nlast_date FROM logs;"
							},
							"dataset": {
								"referenceName": "MySql_Erp_List_Tables",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "For Each Table",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "List of Tables With New Max Date",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('List of Tables With New Max Date').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "If Condition1",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@equals(item().table_name, 'logs') ",
											"type": "Expression"
										},
										"ifFalseActivities": [
											{
												"name": "Export Table Copy",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "MySqlSource",
														"query": {
															"value": "@concat(\n    'SELECT * FROM ', \n    item().table_name,\n    ' WHERE \n    create_date <=  \n    (SELECT MAX(create_date) AS create_date  \n     FROM logs \n     WHERE table_name = ''', item().table_name, ''' )\n    AND create_date >= STR_TO_DATE(''', item().last_date, ''', ''%Y-%m-%dT%H:%i:%s'')\n    LIMIT 10;'\n)",
															"type": "Expression"
														}
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings",
															"copyBehavior": "FlattenHierarchy"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"translator": {
														"type": "TabularTranslator",
														"typeConversion": true,
														"typeConversionSettings": {
															"allowDataTruncation": true,
															"treatBooleanAsNumber": false
														}
													}
												},
												"inputs": [
													{
														"referenceName": "MySql_Erp_Dynamic_Load",
														"type": "DatasetReference",
														"parameters": {}
													}
												],
												"outputs": [
													{
														"referenceName": "Bronze_Parquet",
														"type": "DatasetReference",
														"parameters": {
															"filename": {
																"value": "@if( equals(item().table_name, 'logs') , \n     concat(item().table_name,'.parquet'),\n     concat(item().table_name,'_',formatDateTime(item().last_date,'yyyy-MM-dd'),'.parquet'))",
																"type": "Expression"
															},
															"foldername": "@item().table_name"
														}
													}
												]
											}
										],
										"ifTrueActivities": [
											{
												"name": "Export Table Logs",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "MySqlSource",
														"query": {
															"value": "@concat(\n 'Select * from ',\n item().table_name)\n",
															"type": "Expression"
														}
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"translator": {
														"type": "TabularTranslator",
														"typeConversion": true,
														"typeConversionSettings": {
															"allowDataTruncation": true,
															"treatBooleanAsNumber": false
														}
													}
												},
												"inputs": [
													{
														"referenceName": "MySql_Erp_Dynamic_Load",
														"type": "DatasetReference",
														"parameters": {}
													}
												],
												"outputs": [
													{
														"referenceName": "Bronze_Parquet",
														"type": "DatasetReference",
														"parameters": {
															"filename": {
																"value": "@if( equals(item().table_name, 'logs') , \n     concat(item().table_name,'.parquet'),\n     concat(item().table_name,'_',formatDateTime(item().create_date,'yyyy-MM-ddTHH:mm:s'),'.parquet'))",
																"type": "Expression"
															},
															"foldername": "@item().table_name"
														}
													}
												]
											}
										]
									}
								}
							]
						}
					},
					{
						"name": "Setup Max Id",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "For Each Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "MySqlSource",
								"query": "call serwer296707_pharma.SetMaxIdForAllTables;\n\nSELECT table_name, create_date, update_date, \n\nlast_date FROM logs;"
							},
							"dataset": {
								"referenceName": "MySql_Erp_List_Tables",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "Arch"
				},
				"annotations": [],
				"lastPublishTime": "2025-03-27T05:52:42Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/MySql_Erp_List_Tables')]",
				"[concat(variables('workspaceId'), '/datasets/MySql_Erp_Dynamic_Load')]",
				"[concat(variables('workspaceId'), '/datasets/Bronze_Parquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Extract_Load_Eurostat')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Inflation_load",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET",
									"requestTimeout": ""
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "freq,unit,s_adj,indic,geo\\TIME_PERIOD",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "freq,unit,s_adj,indic,geo\\TIME_PERIOD",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-05 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-05 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-06 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-06 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-07 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-07 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-08 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-08 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-09 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-09 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-10 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-10 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-11 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-11 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-12 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-12 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2025-01 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2025-01 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2025-02 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2025-02 ",
											"type": "String",
											"physicalType": "String"
										}
									}
								],
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "Web_Eurostat_Inflation",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "External_Inflation_Csv",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "Arch"
				},
				"annotations": [],
				"lastPublishTime": "2025-03-26T08:44:54Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Web_Eurostat_Inflation')]",
				"[concat(variables('workspaceId'), '/datasets/External_Inflation_Csv')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load_To_Gold')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Dimensions",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "D_DIM_PY",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkpool32",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Planning Book",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Dimensions",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "F_PLANNING_BOOK_PY",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkpool32",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "Gold"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/D_DIM_PY')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkpool32')]",
				"[concat(variables('workspaceId'), '/notebooks/F_PLANNING_BOOK_PY')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Orchestrator Pipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Extract_Load_Erp",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Extract Erp Data",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "Extract_MinIo",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Extract_Load_Erp",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Extract MinIO Data",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "Transform_Silver",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Extract_MinIo",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Transform_Silver",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "Load_To_Gold",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Transform_Silver",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Load_To_Gold",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Extract Erp Data')]",
				"[concat(variables('workspaceId'), '/pipelines/Extract MinIO Data')]",
				"[concat(variables('workspaceId'), '/pipelines/Transform_Silver')]",
				"[concat(variables('workspaceId'), '/pipelines/Load_To_Gold')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Transform_Silver')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "D_CUSTOMERS",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "D_CUSTOMERS_LOAD_SQL",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkpool32",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null,
							"authentication": {
								"type": "MSI"
							}
						}
					},
					{
						"name": "D_PRODUCTS",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "D_CUSTOMERS",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "D_PRODUCT_LOAD_SQL",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkpool32",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "F_FORECAST",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "D_PRODUCTS",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "F_FORCAST_LOAD_SQL",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkpool32",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "F_SALES",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "F_FORECAST",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "F_SALES_LOAD_SQL",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkpool32",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "F_POS",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "F_SALES",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "F_POS_LOAD_SQL",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkpool32",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "F_WH",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "F_POS",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "F_WH_DATA_LOAD_SQL",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkpool32",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "Silver"
				},
				"annotations": [],
				"lastPublishTime": "2025-04-02T06:40:26Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/D_CUSTOMERS_LOAD_SQL')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkpool32')]",
				"[concat(variables('workspaceId'), '/notebooks/D_PRODUCT_LOAD_SQL')]",
				"[concat(variables('workspaceId'), '/notebooks/F_FORCAST_LOAD_SQL')]",
				"[concat(variables('workspaceId'), '/notebooks/F_SALES_LOAD_SQL')]",
				"[concat(variables('workspaceId'), '/notebooks/F_POS_LOAD_SQL')]",
				"[concat(variables('workspaceId'), '/notebooks/F_WH_DATA_LOAD_SQL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Bronze_Csv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "mrgbigpharma-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filename": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Targets_Bronze"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().filename",
							"type": "Expression"
						},
						"folderPath": "Erp",
						"fileSystem": "bronze"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/mrgbigpharma-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Bronze_Log_Parquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "mrgbigpharma-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "Sources_Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "logs.parquet",
						"folderPath": "Erp/logs",
						"fileSystem": "bronze"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "table_name",
						"type": "UTF8"
					},
					{
						"name": "create_date",
						"type": "INT96"
					},
					{
						"name": "update_date",
						"type": "INT96"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/mrgbigpharma-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Bronze_Parquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "mrgbigpharma-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filename": {
						"type": "string"
					},
					"foldername": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Targets_Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().filename",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@concat('Erp/',dataset().foldername)",
							"type": "Expression"
						},
						"fileSystem": "bronze"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/mrgbigpharma-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ERP_DATA_LOCATION')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "mrgbigpharma-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"TblName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().TblName",
							"type": "Expression"
						},
						"folderPath": "Erp",
						"fileSystem": "bronze"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/mrgbigpharma-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/External_Inflation_Csv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "mrgbigpharma-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "Targets_Bronze"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "inflation.csv",
						"folderPath": "ExternalStatistics",
						"fileSystem": "bronze"
					},
					"columnDelimiter": ";",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/mrgbigpharma-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MinIoDataDistributors')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "MinIoS3",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AmazonS3CompatibleLocation",
						"bucketName": "distributors",
						"fileName": "part-00000-b20e9098-7b48-48f2-888f-57de2602203c.c000.csv"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/MinIoS3')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MySql_Erp_Dynamic_Load')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "MySqlErp",
					"type": "LinkedServiceReference",
					"parameters": {
						"User": "serwer296707_pharma",
						"Database": "serwer296707_pharma",
						"Server": "sql133.lh.pl"
					}
				},
				"folder": {
					"name": "Sources_Bronze"
				},
				"annotations": [],
				"type": "MySqlTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/MySqlErp')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MySql_Erp_List_Tables')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "MySqlErp",
					"type": "LinkedServiceReference",
					"parameters": {
						"User": "serwer296707_pharma",
						"Database": "serwer296707_pharma",
						"Server": "sql133.lh.pl"
					}
				},
				"folder": {
					"name": "Sources_Bronze"
				},
				"annotations": [],
				"type": "MySqlTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/MySqlErp')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SinkBronzeDistributors')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "mrgbigpharma-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"FileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().FileName",
							"type": "Expression"
						},
						"folderPath": "MinIoDistributors",
						"fileSystem": "bronze"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/mrgbigpharma-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Web_Eurostat_Inflation')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "eceuropaeu_inflation",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "Sources_Bronze"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					},
					"columnDelimiter": "\t",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "freq,unit,s_adj,indic,geo\\TIME_PERIOD",
						"type": "String"
					},
					{
						"name": "2024-05 ",
						"type": "String"
					},
					{
						"name": "2024-06 ",
						"type": "String"
					},
					{
						"name": "2024-07 ",
						"type": "String"
					},
					{
						"name": "2024-08 ",
						"type": "String"
					},
					{
						"name": "2024-09 ",
						"type": "String"
					},
					{
						"name": "2024-10 ",
						"type": "String"
					},
					{
						"name": "2024-11 ",
						"type": "String"
					},
					{
						"name": "2024-12 ",
						"type": "String"
					},
					{
						"name": "2025-01 ",
						"type": "String"
					},
					{
						"name": "2025-02 ",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/eceuropaeu_inflation')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AmazonS3Compatible2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AmazonS3Compatible",
				"typeProperties": {
					"serviceUrl": "http://20.215.33.25:9000",
					"accessKeyId": "[parameters('AmazonS3Compatible2_properties_typeProperties_accessKeyId')]",
					"secretAccessKey": {
						"type": "SecureString",
						"value": "[parameters('AmazonS3Compatible2_secretAccessKey')]"
					},
					"forcePathStyle": true
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MinIoS3')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AmazonS3Compatible",
				"typeProperties": {
					"serviceUrl": "http://20.215.33.25:9000",
					"accessKeyId": "[parameters('MinIoS3_properties_typeProperties_accessKeyId')]",
					"secretAccessKey": {
						"type": "SecureString",
						"value": "[parameters('MinIoS3_secretAccessKey')]"
					},
					"forcePathStyle": true
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MySqlErp')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"User": {
						"type": "String",
						"defaultValue": "serwer296707_pharma"
					},
					"Database": {
						"type": "String",
						"defaultValue": "serwer296707_pharma"
					},
					"Server": {
						"type": "String",
						"defaultValue": "sql133.lh.pl"
					}
				},
				"annotations": [],
				"type": "MySql",
				"typeProperties": {
					"server": "[parameters('MySqlErp_properties_typeProperties_server')]",
					"port": 3306,
					"database": "[parameters('MySqlErp_properties_typeProperties_database')]",
					"username": "[parameters('MySqlErp_properties_typeProperties_username')]",
					"sslMode": 1,
					"useSystemTrustStore": 0,
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "MySqlPassword",
							"type": "LinkedServiceReference"
						},
						"secretName": "mysqlpassword"
					},
					"driverVersion": "v2"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/MySqlPassword')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MySqlPassword')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('MySqlPassword_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/S3MinIO')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AmazonS3",
				"typeProperties": {
					"serviceUrl": "http://20.215.33.25:9000",
					"accessKeyId": "[parameters('S3MinIO_properties_typeProperties_accessKeyId')]",
					"secretAccessKey": {
						"type": "SecureString",
						"value": "[parameters('S3MinIO_secretAccessKey')]"
					},
					"authenticationType": "AccessKey"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/eceuropaeu_inflation')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('eceuropaeu_inflation_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mrgbigpharma-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('mrgbigpharma-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mrgbigpharma-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('mrgbigpharma-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create External Table D_Products')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'gold_bigpharma_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [gold_bigpharma_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://gold@bigpharma.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE dbo.d_products (\n\t[product_id] int,\n\t[product_name] nvarchar(4000),\n\t[brand_name] nvarchar(4000),\n\t[sub_brand_name] nvarchar(4000),\n\t[category_name] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'd_products/**',\n\tDATA_SOURCE = [gold_bigpharma_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.d_products\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sql_Serverless",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create External Table D_Regions')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'gold_bigpharma_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [gold_bigpharma_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://gold@bigpharma.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE dbo.d_regions (\n\t[country_id] int,\n\t[cluster] nvarchar(4000),\n\t[region_description] nvarchar(4000),\n\t[country] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'd_regions/**',\n\tDATA_SOURCE = [gold_bigpharma_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.d_regions\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sql_Serverless",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create External Table F_Planning_Book')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'gold_bigpharma_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [gold_bigpharma_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://gold@bigpharma.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE dbo.f_planning_book (\n\t[product_id] int,\n\t[country_id] int,\n\t[financial_date] date,\n\t[forecast_snapshot] date,\n\t[quantity] bigint,\n\t[amount] float,\n\t[unit_price] float,\n\t[whrs_sell_in_quantity] bigint,\n\t[whrs_open_quantity] bigint,\n\t[whrs_end_quantity] bigint,\n\t[whrs_sell_in_amount] float,\n\t[whrs_open_amount] float,\n\t[whrs_end_amount] float,\n\t[pos_sell_out_quantity] bigint,\n\t[pos_open_quantity] bigint,\n\t[pos_end_quantity] bigint,\n\t[pos_sell_out_amount] float,\n\t[pos_open_amount] float,\n\t[pos_end_amount] float,\n\t[discount] float\n\t)\n\tWITH (\n\tLOCATION = 'f_planning_book/**',\n\tDATA_SOURCE = [gold_bigpharma_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.f_planning_book\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sql_Serverless",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://bigpharma.dfs.core.windows.net/bronze/Erp/countries/countries_2025-03-24.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    count(*)\nFROM\n    OPENROWSET(\n        BULK 'https://bigpharma.dfs.core.windows.net/bronze/Erp/order_details/order_details_2021-12-25.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_CUSTOMERS_LOAD_PY')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive/Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a42bb111-6bcb-46f2-989f-531f833d9d1d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Zmiana nazw kolumn w join_bronze_df, aby unikn duplikacji nazw\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"file_name = \"customers\"\r\n",
							"\r\n",
							"# Bucket dla warstwy Silver\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"d_customers\"\r\n",
							"\r\n",
							"# cieka dla warstwy Silver\r\n",
							"silver_path =  f\"{silver_bucket_name}{silver_prefix}\"\r\n",
							"\r\n",
							"# cieki do plikw Parquet w MinIO\r\n",
							"path =  f\"{bronze_bucket_name}{bronze_prefix}{file_name}/*.parquet\"\r\n",
							"print(path )\r\n",
							"\r\n",
							"bronze_df = spark.read.load(path, format='parquet')\r\n",
							"join_path = f\"{bronze_bucket_name}{bronze_prefix}countries/*.parquet\"\r\n",
							"\r\n",
							"join_bronze_df = spark.read.load(join_path, format='parquet')\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"source": [
							"\r\n",
							"join_bronze_df = join_bronze_df.select(\r\n",
							"    [col(c).alias(f\"{c}_right\") if c != \"country_id\" else col(c) for c in join_bronze_df.columns]\r\n",
							")\r\n",
							"\r\n",
							"# Wykonanie JOIN po kolumnie \"country_id\"\r\n",
							"joined_df = bronze_df.join(join_bronze_df, on=\"country_id\", how=\"inner\")  # Moesz zmieni \"inner\" na \"left\", \"right\" itp.\r\n",
							"\r\n",
							"# Wywietlenie wynikw\r\n",
							"print(joined_df.columns)\r\n",
							"\r\n",
							"latest_bronze_df = joined_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"customer_id\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"# Wybr odpowiednich kolumn do zapisania w warstwie Silver\r\n",
							"new_bronze_df = latest_bronze_df.select(\r\n",
							"    \"customer_id\", \r\n",
							"    \"company_name\", \r\n",
							"    \"address\", \r\n",
							"    \"country_id\", \r\n",
							"    \"city\",\r\n",
							"    \"region_description_right\",\r\n",
							"    \"country_right\",\r\n",
							"    \"cluster_right\"\r\n",
							")\r\n",
							"new_column_names = [col_name.replace(\"_right\", \"\") for col_name in latest_bronze_df.columns]\r\n",
							"\r\n",
							"# Przypisz nowe nazwy kolumn do DataFrame\r\n",
							"new_bronze_df = latest_bronze_df.toDF(*new_column_names)\r\n",
							"# Wybierz poprawione kolumny\r\n",
							"new_bronze_df = new_bronze_df.select(\r\n",
							"    \"customer_id\",\r\n",
							"    \"country_id\", \r\n",
							"    \"company_name\", \r\n",
							"    \"address\", \r\n",
							"    \"city\",\r\n",
							"    \"region_description\",  # Poprawiona nazwa\r\n",
							"    \"country\",  # Poprawiona nazwa\r\n",
							"    \"cluster\"\r\n",
							")\r\n",
							"new_bronze_df.show()\r\n",
							"# Sprawdzenie, czy tabela Silver ju istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"\r\n",
							"\r\n",
							"if silver_exists:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        new_bronze_df.alias(\"new_bronze_df\"),\r\n",
							"        (F.col(\"silver.customer_id\") == F.col(\"new_bronze_df.customer_id\")),\r\n",
							"        how=\"outer\"\r\n",
							"        )\r\n",
							"\r\n",
							"        # Wybr kolumn, ktre maj zosta zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_bronze_df.customer_id\", \"silver.customer_id\").alias(\"customer_id\"),\r\n",
							"        F.coalesce(\"new_bronze_df.country_id\", \"silver.country_id\").alias(\"country_id\"),\r\n",
							"        F.coalesce(\"new_bronze_df.company_name\", \"silver.company_name\").alias(\"company_name\"),\r\n",
							"        F.coalesce(\"new_bronze_df.address\", \"silver.address\").alias(\"address\"),\r\n",
							"        F.coalesce(\"new_bronze_df.city\", \"silver.city\").alias(\"city\"),\r\n",
							"        F.coalesce(\"new_bronze_df.region_description\", \"silver.region_description\").alias(\"region_description\"),\r\n",
							"        F.coalesce(\"new_bronze_df.country\", \"silver.country\").alias(\"country\"),\r\n",
							"        F.coalesce(\"new_bronze_df.cluster\", \"silver.cluster\").alias(\"cluster\")\r\n",
							"    )\r\n",
							"    final_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"else:\r\n",
							"\r\n",
							"    new_bronze_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"    print(\"Inkrementalne adowanie zakoczone!\")   \r\n",
							"\r\n",
							"spark.stop()"
						],
						"outputs": [],
						"execution_count": 30
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_CUSTOMERS_LOAD_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2a7bbe4c-2300-450f-bced-2a79b7893a27"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"-- test D_CUSTOMERS_LOAD_SQL\n",
							"\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- 1. Register bronze_customers as a temporary view\n",
							"CREATE OR REPLACE TEMP VIEW bronze_customers\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/customers/'\n",
							");\n",
							"\n",
							"-- 2. Register bronze_countries as a temporary view\n",
							"CREATE OR REPLACE TEMP VIEW bronze_countries\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/countries/'\n",
							");\n",
							"\n",
							"-- 3. Prepare data  JOIN + select the most recent by customer_id\n",
							"CREATE OR REPLACE TEMP VIEW new_bronze_d_customers AS\n",
							"SELECT \n",
							"    c.customer_id,\n",
							"    c.country_id,\n",
							"    c.company_name,\n",
							"    c.address,\n",
							"    c.city,\n",
							"    co.region_description,\n",
							"    co.country,\n",
							"    co.cluster,\n",
							"    c.update_date as customer_update_date,\n",
							"    co.update_date as country_update_date\n",
							"FROM bronze_customers c\n",
							"JOIN bronze_countries co\n",
							"  ON c.country_id = co.country_id;\n",
							"\n",
							"-- 4. Create the target table as Delta Lake (if it does not exist)\n",
							"CREATE TABLE IF NOT EXISTS silver.d_customers\n",
							"USING delta\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers'\n",
							"AS\n",
							"SELECT * FROM new_bronze_d_customers\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- 5. Filter new or updated records\n",
							"CREATE OR REPLACE TEMP VIEW filter_bronze_d_customers AS\n",
							"SELECT * FROM new_bronze_d_customers n \n",
							"WHERE NOT EXISTS (\n",
							"  SELECT 1 FROM silver.d_customers s \n",
							"  WHERE s.customer_id = n.customer_id\n",
							"    AND s.country_id = n.country_id\n",
							"    AND s.customer_update_date = n.customer_update_date\n",
							"    AND s.country_update_date = n.country_update_date\n",
							");\n",
							"\n",
							"-- 6. MERGE (UPSERT)\n",
							"MERGE INTO silver.d_customers AS target\n",
							"USING filter_bronze_d_customers AS source\n",
							"ON target.customer_id = source.customer_id\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    country_id = source.country_id,\n",
							"    company_name = source.company_name,\n",
							"    address = source.address,\n",
							"    city = source.city,\n",
							"    region_description = source.region_description,\n",
							"    country = source.country,\n",
							"    cluster = source.cluster,\n",
							"    customer_update_date = source.customer_update_date,\n",
							"    country_update_date = source.country_update_date\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    customer_id,\n",
							"    country_id,\n",
							"    company_name,\n",
							"    address,\n",
							"    city,\n",
							"    region_description,\n",
							"    country,\n",
							"    cluster,\n",
							"    customer_update_date,\n",
							"    country_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.customer_id,\n",
							"    source.country_id,\n",
							"    source.company_name,\n",
							"    source.address,\n",
							"    source.city,\n",
							"    source.region_description,\n",
							"    source.country,\n",
							"    source.cluster,\n",
							"    source.customer_update_date,\n",
							"    source.country_update_date\n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_CUSTOMERS_LOAD_SQL_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ac6f74d9-8136-457b-a5e1-90c548ebc370"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"-- test D_CUSTOMERS_LOAD_SQL\n",
							"\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- 1. Rejestracja bronze_customers jako tymczasowy widok\n",
							"CREATE OR REPLACE TEMP VIEW bronze_customers\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/customers/'\n",
							");\n",
							"\n",
							"-- 2. Rejestracja bronze_countries jako tymczasowy widok\n",
							"CREATE OR REPLACE TEMP VIEW bronze_countries\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/countries/'\n",
							");\n",
							"\n",
							"-- 3. Przygotowanie danych  JOIN + wybr najnowszych po customer_id\n",
							"CREATE OR REPLACE TEMP VIEW new_bronze_d_customers AS\n",
							"SELECT \n",
							"    c.customer_id,\n",
							"    c.country_id,\n",
							"    c.company_name,\n",
							"    c.address,\n",
							"    c.city,\n",
							"    co.region_description,\n",
							"    co.country,\n",
							"    co.cluster,\n",
							"    c.update_date as customer_update_date,\n",
							"    co.update_date as country_update_date\n",
							"FROM bronze_customers c\n",
							"JOIN bronze_countries co\n",
							"  ON c.country_id = co.country_id;\n",
							"\n",
							"-- 4. Stworzenie tabeli docelowej jako Delta Lake (jeli nie istnieje)\n",
							"CREATE TABLE IF NOT EXISTS silver.d_customers\n",
							"USING delta\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers'\n",
							"AS\n",
							"SELECT * FROM new_bronze_d_customers\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- 5. Filtrowanie nowych lub zaktualizowanych rekordw\n",
							"CREATE OR REPLACE TEMP VIEW filter_bronze_d_customers AS\n",
							"SELECT * FROM new_bronze_d_customers n \n",
							"WHERE NOT EXISTS (\n",
							"  SELECT 1 FROM silver.d_customers s \n",
							"  WHERE s.customer_id = n.customer_id\n",
							"    AND s.country_id = n.country_id\n",
							"    AND s.customer_update_date = n.customer_update_date\n",
							"    AND s.country_update_date = n.country_update_date\n",
							");\n",
							"\n",
							"-- 6. MERGE (UPSERT)\n",
							"MERGE INTO silver.d_customers AS target\n",
							"USING filter_bronze_d_customers AS source\n",
							"ON target.customer_id = source.customer_id\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    country_id = source.country_id,\n",
							"    company_name = source.company_name,\n",
							"    address = source.address,\n",
							"    city = source.city,\n",
							"    region_description = source.region_description,\n",
							"    country = source.country,\n",
							"    cluster = source.cluster,\n",
							"    customer_update_date = source.customer_update_date,\n",
							"    country_update_date = source.country_update_date\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    customer_id,\n",
							"    country_id,\n",
							"    company_name,\n",
							"    address,\n",
							"    city,\n",
							"    region_description,\n",
							"    country,\n",
							"    cluster,\n",
							"    customer_update_date,\n",
							"    country_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.customer_id,\n",
							"    source.country_id,\n",
							"    source.company_name,\n",
							"    source.address,\n",
							"    source.city,\n",
							"    source.region_description,\n",
							"    source.country,\n",
							"    source.cluster,\n",
							"    source.customer_update_date,\n",
							"    source.country_update_date\n",
							"  );"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"-- test D_CUSTOMERS_LOAD_SQL\n",
							"\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- 1. Register bronze_customers as a temporary view\n",
							"CREATE OR REPLACE TEMP VIEW bronze_customers\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/customers/'\n",
							");\n",
							"\n",
							"-- 2. Register bronze_countries as a temporary view\n",
							"CREATE OR REPLACE TEMP VIEW bronze_countries\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/countries/'\n",
							");\n",
							"\n",
							"-- 3. Prepare data  JOIN + select the most recent by customer_id\n",
							"CREATE OR REPLACE TEMP VIEW new_bronze_d_customers AS\n",
							"SELECT \n",
							"    c.customer_id,\n",
							"    c.country_id,\n",
							"    c.company_name,\n",
							"    c.address,\n",
							"    c.city,\n",
							"    co.region_description,\n",
							"    co.country,\n",
							"    co.cluster,\n",
							"    c.update_date as customer_update_date,\n",
							"    co.update_date as country_update_date\n",
							"FROM bronze_customers c\n",
							"JOIN bronze_countries co\n",
							"  ON c.country_id = co.country_id;\n",
							"\n",
							"-- 4. Create the target table as Delta Lake (if it does not exist)\n",
							"CREATE TABLE IF NOT EXISTS silver.d_customers\n",
							"USING delta\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers'\n",
							"AS\n",
							"SELECT * FROM new_bronze_d_customers\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- 5. Filter new or updated records\n",
							"CREATE OR REPLACE TEMP VIEW filter_bronze_d_customers AS\n",
							"SELECT * FROM new_bronze_d_customers n \n",
							"WHERE NOT EXISTS (\n",
							"  SELECT 1 FROM silver.d_customers s \n",
							"  WHERE s.customer_id = n.customer_id\n",
							"    AND s.country_id = n.country_id\n",
							"    AND s.customer_update_date = n.customer_update_date\n",
							"    AND s.country_update_date = n.country_update_date\n",
							");\n",
							"\n",
							"-- 6. MERGE (UPSERT)\n",
							"MERGE INTO silver.d_customers AS target\n",
							"USING filter_bronze_d_customers AS source\n",
							"ON target.customer_id = source.customer_id\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    country_id = source.country_id,\n",
							"    company_name = source.company_name,\n",
							"    address = source.address,\n",
							"    city = source.city,\n",
							"    region_description = source.region_description,\n",
							"    country = source.country,\n",
							"    cluster = source.cluster,\n",
							"    customer_update_date = source.customer_update_date,\n",
							"    country_update_date = source.country_update_date\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    customer_id,\n",
							"    country_id,\n",
							"    company_name,\n",
							"    address,\n",
							"    city,\n",
							"    region_description,\n",
							"    country,\n",
							"    cluster,\n",
							"    customer_update_date,\n",
							"    country_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.customer_id,\n",
							"    source.country_id,\n",
							"    source.company_name,\n",
							"    source.address,\n",
							"    source.city,\n",
							"    source.region_description,\n",
							"    source.country,\n",
							"    source.cluster,\n",
							"    source.customer_update_date,\n",
							"    source.country_update_date\n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_DIM_PY')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive/Gold"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4cab2b50-6e67-4a17-9cea-e8df25564fe8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc, lit, add_months\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql.functions import to_date\r\n",
							"\r\n",
							"# Tworzenie sesji Spark\r\n",
							"spark = SparkSession.builder.appName(\"AzureSynapseMigration\").getOrCreate()\r\n",
							"\r\n",
							"# Parametry dla warstwy Silver (ADLS Gen2)\r\n",
							"bucket_name_silver = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"\r\n",
							"# Parametry dla warstwy Gold (ADLS Gen2)\r\n",
							"bucket_name_gold = \"abfss://gold@bigpharma.dfs.core.windows.net/\"\r\n",
							"\r\n",
							"# cieki do plikw Parquet w ADLS Gen2 dla warstwy Silver\r\n",
							"d_products_silver_path = f\"{bucket_name_silver}/d_products\"\r\n",
							"d_regions_silver_path = f\"{bucket_name_silver}/d_customers\"\r\n",
							"\r\n",
							"# Zaaduj dane z warstwy Silver\r\n",
							"d_products = spark.read.parquet(d_products_silver_path)\r\n",
							"d_regions = spark.read.parquet(d_regions_silver_path)\r\n",
							"\r\n",
							"# cieki do plikw Parquet w ADLS Gen2 dla warstwy Gold\r\n",
							"d_products_gold_path = f\"{bucket_name_gold}/d_products\"\r\n",
							"d_regions_gold_path = f\"{bucket_name_gold}/d_regions\"\r\n",
							"\r\n",
							"# Mona rwnie zaadowa dane do warstwy Gold, jeli konieczne\r\n",
							"d_products = spark.read.parquet(d_products_silver_path)\r\n",
							"d_regions = spark.read.parquet(d_regions_silver_path)\r\n",
							"\r\n",
							"\r\n",
							"d_products = d_products.withColumnRenamed('product_id','IdProduct')\\\r\n",
							"                       .withColumnRenamed('product_name','Name')\\\r\n",
							"                       .withColumnRenamed('brand_name','Brand')\\\r\n",
							"                       .withColumnRenamed('sub_brand_name','SubBrand')\\\r\n",
							"                       .withColumnRenamed('category_name','Category')\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"d_products.write.mode(\"overwrite\").parquet(d_products_gold_path)\r\n",
							"\r\n",
							"selected_columns = d_regions[[\"country_id\", \"cluster\", \"region_description\", \"country\"]]\r\n",
							"\r\n",
							"selected_columns = selected_columns.withColumnRenamed('country_id', 'IDCountry').withColumnRenamed('region_description', 'Region').withColumnRenamed('cluster', 'Claster').withColumnRenamed('country', 'Country')\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"unique_d_regions = selected_columns.drop_duplicates()\r\n",
							"unique_d_regions.write.mode(\"overwrite\").parquet(d_regions_gold_path)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_DIM_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load Gold"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b9b2d180-3362-46f2-a3b4-137baa4857c0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS gold;\n",
							"\n",
							"-- Create views on Silver tables\n",
							"CREATE OR REPLACE VIEW silver_d_products AS\n",
							"SELECT * FROM delta.`abfss://silver@bigpharma.dfs.core.windows.net/d_products`;\n",
							"\n",
							"CREATE OR REPLACE VIEW silver_d_regions AS\n",
							"SELECT * FROM delta.`abfss://silver@bigpharma.dfs.core.windows.net/d_customers`;\n",
							"\n",
							"-- Create tables in the Gold layer\n",
							"CREATE TABLE IF NOT EXISTS gold.d_products\n",
							"USING DELTA\n",
							"LOCATION 'abfss://gold@bigpharma.dfs.core.windows.net/d_products'\n",
							"AS\n",
							"SELECT \n",
							"  product_id AS IdProduct,\n",
							"  product_name AS Name,\n",
							"  brand_name AS Brand,\n",
							"  sub_brand_name AS SubBrand,\n",
							"  category_name AS Category \n",
							"FROM silver_d_products\n",
							"WHERE 1 = 0;\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS gold.d_regions\n",
							"USING DELTA\n",
							"LOCATION 'abfss://gold@bigpharma.dfs.core.windows.net/d_customers'\n",
							"AS\n",
							"SELECT \n",
							"  country_id AS IDCountry,\n",
							"  cluster AS Claster,\n",
							"  region_description AS Region,\n",
							"  country AS Country\n",
							"FROM silver_d_regions\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- Rename columns in the d_products table and write to Gold layer\n",
							"INSERT OVERWRITE TABLE gold.d_products\n",
							"SELECT \n",
							"  product_id AS IdProduct,\n",
							"  product_name AS Name,\n",
							"  brand_name AS Brand,\n",
							"  sub_brand_name AS SubBrand,\n",
							"  category_name AS Category\n",
							"FROM silver_d_products;\n",
							"\n",
							"-- Remove duplicates in the d_regions table and write to the Gold table\n",
							"INSERT OVERWRITE TABLE gold.d_regions\n",
							"SELECT DISTINCT \n",
							"  country_id AS IDCountry,\n",
							"  cluster AS Claster,\n",
							"  region_description AS Region,\n",
							"  country AS Country\n",
							"FROM silver_d_regions;\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_DIM_SQL_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "03ea89d6-a413-447b-8202-032e03dde565"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS gold;\n",
							"\n",
							"CREATE OR REPLACE VIEW silver_d_products AS\n",
							"SELECT * FROM delta.`abfss://silver@bigpharma.dfs.core.windows.net/d_products`;\n",
							"\n",
							"CREATE OR REPLACE VIEW silver_d_regions AS\n",
							"SELECT * FROM delta.`abfss://silver@bigpharma.dfs.core.windows.net/d_customers`;\n",
							"\n",
							"-- Tworzenie tabel w warstwie Gold\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS gold.d_products\n",
							"USING DELTA\n",
							"LOCATION 'abfss://gold@bigpharma.dfs.core.windows.net/d_products'\n",
							"AS\n",
							"SELECT product_id AS IdProduct,\n",
							"       product_name AS Name,\n",
							"       brand_name AS Brand,\n",
							"       sub_brand_name AS SubBrand,\n",
							"       category_name AS Category \n",
							" FROM silver_d_products\n",
							"WHERE 1 = 0;\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS gold.d_regions\n",
							"USING DELTA\n",
							"LOCATION 'abfss://gold@bigpharma.dfs.core.windows.net/d_customers'\n",
							"AS\n",
							"SELECT country_id AS IDCountry,\n",
							"       cluster AS Claster,\n",
							"       region_description AS Region,\n",
							"       country AS Country\n",
							"FROM silver_d_regions\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- Zmiana nazw kolumn w tabeli d_products i zapis do warstwy Gold\n",
							"INSERT OVERWRITE TABLE gold.d_products\n",
							"\n",
							"SELECT \n",
							"    product_id AS IdProduct,\n",
							"    product_name AS Name,\n",
							"    brand_name AS Brand,\n",
							"    sub_brand_name AS SubBrand,\n",
							"    category_name AS Category\n",
							"FROM silver_d_products;\n",
							"\n",
							"-- Usuwanie duplikatw w tabeli d_regions i zapis do nowej tabeli Gold\n",
							"INSERT OVERWRITE TABLE gold.d_regions\n",
							"SELECT DISTINCT \n",
							"    country_id AS IDCountry,\n",
							"    cluster AS Claster,\n",
							"    region_description AS Region,\n",
							"    country AS Country\n",
							"FROM silver_d_regions;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS gold;\n",
							"\n",
							"-- Create views on Silver tables\n",
							"CREATE OR REPLACE VIEW silver_d_products AS\n",
							"SELECT * FROM delta.`abfss://silver@bigpharma.dfs.core.windows.net/d_products`;\n",
							"\n",
							"CREATE OR REPLACE VIEW silver_d_regions AS\n",
							"SELECT * FROM delta.`abfss://silver@bigpharma.dfs.core.windows.net/d_customers`;\n",
							"\n",
							"-- Create tables in the Gold layer\n",
							"CREATE TABLE IF NOT EXISTS gold.d_products\n",
							"USING DELTA\n",
							"LOCATION 'abfss://gold@bigpharma.dfs.core.windows.net/d_products'\n",
							"AS\n",
							"SELECT \n",
							"  product_id AS IdProduct,\n",
							"  product_name AS Name,\n",
							"  brand_name AS Brand,\n",
							"  sub_brand_name AS SubBrand,\n",
							"  category_name AS Category \n",
							"FROM silver_d_products\n",
							"WHERE 1 = 0;\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS gold.d_regions\n",
							"USING DELTA\n",
							"LOCATION 'abfss://gold@bigpharma.dfs.core.windows.net/d_customers'\n",
							"AS\n",
							"SELECT \n",
							"  country_id AS IDCountry,\n",
							"  cluster AS Claster,\n",
							"  region_description AS Region,\n",
							"  country AS Country\n",
							"FROM silver_d_regions\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- Rename columns in the d_products table and write to Gold layer\n",
							"INSERT OVERWRITE TABLE gold.d_products\n",
							"SELECT \n",
							"  product_id AS IdProduct,\n",
							"  product_name AS Name,\n",
							"  brand_name AS Brand,\n",
							"  sub_brand_name AS SubBrand,\n",
							"  category_name AS Category\n",
							"FROM silver_d_products;\n",
							"\n",
							"-- Remove duplicates in the d_regions table and write to the Gold table\n",
							"INSERT OVERWRITE TABLE gold.d_regions\n",
							"SELECT DISTINCT \n",
							"  country_id AS IDCountry,\n",
							"  cluster AS Claster,\n",
							"  region_description AS Region,\n",
							"  country AS Country\n",
							"FROM silver_d_regions;\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_POS_LOAD_PY')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive/Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c61abd19-cd7f-4fd8-9d9e-1f8b7dc3514b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"\r\n",
							"spark = SparkSession.builder.appName(\"AzureSynapseMigration\").getOrCreate()\r\n",
							"\r\n",
							"# Parametry dla warstwy Bronze (ADLS Gen2)\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"file_name = \"inventory_pos_history_details\"\r\n",
							"\r\n",
							"# Parametry dla warstwy Silver (ADLS Gen2)\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"f_pos_data/\"\r\n",
							"\r\n",
							"# cieki do plikw Parquet w ADLS Gen2\r\n",
							"path = f\"{bronze_bucket_name}{bronze_prefix}{file_name}\"\r\n",
							"\r\n",
							"# cieka do warstwy Silver\r\n",
							"silver_path = f\"{silver_bucket_name}{silver_prefix}\"\r\n",
							"\r\n",
							"# Zaaduj dane z ADLS (tabela inventory_pos_history_details)\r\n",
							"bronze_df = spark.read.parquet(path)\r\n",
							"\r\n",
							"\r\n",
							"bronze_df = spark.read.parquet(path)\r\n",
							"\r\n",
							"latest_bronze_df = bronze_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"product_id\", \"country_id\",\"update_date\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"new_bronze_df = latest_bronze_df.select(\r\n",
							"    \"product_id\",\r\n",
							"    \"country_id\", \r\n",
							"    \"transaction_date\", \r\n",
							"    \"unit_price\", \r\n",
							"    \"pos_sell_out_quantity\",\r\n",
							"    \"pos_open_quantity\",  # Poprawiona nazwa\r\n",
							"    \"pos_end_quantity\"\r\n",
							")\r\n",
							"\r\n",
							"new_bronze_df = new_bronze_df.withColumn(\r\n",
							"    \"transaction_date\", trunc(\"transaction_date\",\"month\"))\r\n",
							"\r\n",
							"aggregated_bronze_df = new_bronze_df.groupBy(\"product_id\", \"country_id\", \"transaction_date\").agg(\r\n",
							"                                    # Summing quantity, discount, and amount\r\n",
							"                                     F.sum(\"pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"                                     F.sum(\"pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"                                     F.sum(\"pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_sell_out_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_sell_out_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_open_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_open_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_end_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_end_amount\"),\r\n",
							"                                     F.round((F.sum(F.col(\"pos_sell_out_quantity\") * F.col(\"unit_price\")) / F.sum(\"pos_sell_out_quantity\")),2).alias(\"unit_price\"))\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver ju istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"\r\n",
							"\r\n",
							"# Jeli tabela Silver istnieje, wykonaj operacj \"MERGE\" na podstawie DataFrame\r\n",
							"if silver_exists:\r\n",
							"    # Zaaduj dane z tabeli Silver\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    # Pocz dane Silver z nowymi danymi (na podstawie order_id, customer_id, product_id)\r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        aggregated_bronze_df.alias(\"new_pod\"),\r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_pod.product_id\")) & \r\n",
							"        (F.col(\"silver.country_id\") == F.col(\"new_pod.country_id\")) & \r\n",
							"        (F.col(\"silver.transaction_date\") == F.col(\"new_pod.transaction_date\")),\r\n",
							"        how=\"outer\"\r\n",
							"    )\r\n",
							"\r\n",
							"    # Wybr kolumn, ktre maj zosta zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_pod.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_pod.country_id\", \"silver.country_id\").alias(\"country_id\"),\r\n",
							"        F.coalesce(\"new_pod.transaction_date\", \"silver.transaction_date\").alias(\"transaction_date\"),\r\n",
							"        F.coalesce(\"new_pod.pos_sell_out_quantity\", \"silver.pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_open_quantity\", \"silver.pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_end_quantity\", \"silver.pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_sell_out_amount\", \"silver.pos_sell_out_amount\").alias(\"pos_sell_out_amount\"),\r\n",
							"        F.coalesce(\"new_pod.pos_open_amount\", \"silver.pos_open_amount\").alias(\"pos_open_amount\"),\r\n",
							"        F.coalesce(\"new_pod.pos_end_amount\", \"silver.pos_end_amount\").alias(\"pos_end_amount\"),\r\n",
							"        F.coalesce(\"new_pod.unit_price\", \"silver.unit_price\").alias(\"unit_price\")\r\n",
							"        )\r\n",
							"    # Display or save the resulting DataFrame\r\n",
							"    # Zapisanie zaktualizowanego DataFrame do warstwy Silver\r\n",
							"    final_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"else:\r\n",
							"    aggregated_bronze_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakoczenie\r\n",
							"print(\"Inkrementalne adowanie zakoczone!\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"\r\n",
							"bronze_df = spark.read.parquet(path)\r\n",
							"\r\n",
							"latest_bronze_df = bronze_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"product_id\", \"country_id\",\"update_date\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"new_bronze_df = latest_bronze_df.select(\r\n",
							"    \"product_id\",\r\n",
							"    \"country_id\", \r\n",
							"    \"transaction_date\", \r\n",
							"    \"unit_price\", \r\n",
							"    \"pos_sell_out_quantity\",\r\n",
							"    \"pos_open_quantity\",  # Poprawiona nazwa\r\n",
							"    \"pos_end_quantity\"\r\n",
							")\r\n",
							"\r\n",
							"new_bronze_df = new_bronze_df.withColumn(\r\n",
							"    \"transaction_date\", trunc(\"transaction_date\",\"month\"))\r\n",
							"\r\n",
							"aggregated_bronze_df = new_bronze_df.groupBy(\"product_id\", \"country_id\", \"transaction_date\").agg(\r\n",
							"                                    # Summing quantity, discount, and amount\r\n",
							"                                     F.sum(\"pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"                                     F.sum(\"pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"                                     F.sum(\"pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_sell_out_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_sell_out_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_open_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_open_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_end_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_end_amount\"),\r\n",
							"                                     F.round((F.sum(F.col(\"pos_sell_out_quantity\") * F.col(\"unit_price\")) / F.sum(\"pos_sell_out_quantity\")),2).alias(\"unit_price\"))\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver ju istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"\r\n",
							"\r\n",
							"# Jeli tabela Silver istnieje, wykonaj operacj \"MERGE\" na podstawie DataFrame\r\n",
							"if silver_exists:\r\n",
							"    # Zaaduj dane z tabeli Silver\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    # Pocz dane Silver z nowymi danymi (na podstawie order_id, customer_id, product_id)\r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        aggregated_bronze_df.alias(\"new_pod\"),\r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_pod.product_id\")) & \r\n",
							"        (F.col(\"silver.country_id\") == F.col(\"new_pod.country_id\")) & \r\n",
							"        (F.col(\"silver.transaction_date\") == F.col(\"new_pod.transaction_date\")),\r\n",
							"        how=\"outer\"\r\n",
							"    )\r\n",
							"\r\n",
							"    # Wybr kolumn, ktre maj zosta zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_pod.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_pod.country_id\", \"silver.country_id\").alias(\"country_id\"),\r\n",
							"        F.coalesce(\"new_pod.transaction_date\", \"silver.transaction_date\").alias(\"transaction_date\"),\r\n",
							"        F.coalesce(\"new_pod.pos_sell_out_quantity\", \"silver.pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_open_quantity\", \"silver.pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_end_quantity\", \"silver.pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_sell_out_amount\", \"silver.pos_sell_out_amount\").alias(\"pos_sell_out_amount\"),\r\n",
							"        F.coalesce(\"new_pod.pos_open_amount\", \"silver.pos_open_amount\").alias(\"pos_open_amount\"),\r\n",
							"        F.coalesce(\"new_pod.pos_end_amount\", \"silver.pos_end_amount\").alias(\"pos_end_amount\"),\r\n",
							"        F.coalesce(\"new_pod.unit_price\", \"silver.unit_price\").alias(\"unit_price\")\r\n",
							"        )\r\n",
							"    # Display or save the resulting DataFrame\r\n",
							"    # Zapisanie zaktualizowanego DataFrame do warstwy Silver\r\n",
							"    final_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"else:\r\n",
							"    aggregated_bronze_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakoczenie\r\n",
							"print(\"Inkrementalne adowanie zakoczone!\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_PRODUCT_LOAD_PY')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive/Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d3080dcd-c316-4e2a-9aed-0f0945691bd3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"# Parametry dla warstwy Bronze (ADLS Gen2)\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"file_name = \"products\"  # Zmieniamy na odpowiedni nazw pliku\r\n",
							"\r\n",
							"# Parametry dla warstwy Silver (ADLS Gen2)\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"d_products/\"\r\n",
							"\r\n",
							"# cieki do plikw Parquet w ADLS Gen2\r\n",
							"path = f\"{bronze_bucket_name}{bronze_prefix}{file_name}\"\r\n",
							"\r\n",
							"# cieka do warstwy Silver\r\n",
							"silver_path = f\"{silver_bucket_name}{silver_prefix}\"\r\n",
							"\r\n",
							"# Zaaduj dane z ADLS (tabela products)\r\n",
							"products_df = spark.read.parquet(path)\r\n",
							"\r\n",
							"\r\n",
							"#products_df.show()\r\n",
							"\r\n",
							"latest_products_df = products_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"product_id\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"\r\n",
							"# Wybr odpowiednich kolumn do zapisania w warstwie Silver\r\n",
							"new_products_df = latest_products_df.select(\r\n",
							"    \"product_id\", \r\n",
							"    \"product_name\", \r\n",
							"    \"brand_name\", \r\n",
							"    \"sub_brand_name\", \r\n",
							"    \"category_name\")\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver ju istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"if silver_exists:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        new_products_df.alias(\"new_products\"),\r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_products.product_id\")),\r\n",
							"        how=\"outer\"\r\n",
							"        )\r\n",
							"\r\n",
							"        # Wybr kolumn, ktre maj zosta zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_products.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_products.product_name\", \"silver.product_name\").alias(\"product_name\"),\r\n",
							"        F.coalesce(\"new_products.brand_name\", \"silver.brand_name\").alias(\"brand_name\"),\r\n",
							"        F.coalesce(\"new_products.sub_brand_name\", \"silver.sub_brand_name\").alias(\"sub_brand_name\"),\r\n",
							"        F.coalesce(\"new_products.category_name\", \"silver.category_name\").alias(\"category_name\")\r\n",
							"    )\r\n",
							"else:\r\n",
							"\r\n",
							"    new_products_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakoczenie\r\n",
							"print(\"Inkrementalne adowanie zakoczone!\")    \r\n",
							"\r\n",
							"\r\n",
							"spark.stop()\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"\r\n",
							"#products_df.show()\r\n",
							"\r\n",
							"latest_products_df = products_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"product_id\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"\r\n",
							"# Wybr odpowiednich kolumn do zapisania w warstwie Silver\r\n",
							"new_products_df = latest_products_df.select(\r\n",
							"    \"product_id\", \r\n",
							"    \"product_name\", \r\n",
							"    \"brand_name\", \r\n",
							"    \"sub_brand_name\", \r\n",
							"    \"category_name\")\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver ju istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"if silver_exists:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        new_products_df.alias(\"new_products\"),\r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_products.product_id\")),\r\n",
							"        how=\"outer\"\r\n",
							"        )\r\n",
							"\r\n",
							"        # Wybr kolumn, ktre maj zosta zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_products.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_products.product_name\", \"silver.product_name\").alias(\"product_name\"),\r\n",
							"        F.coalesce(\"new_products.brand_name\", \"silver.brand_name\").alias(\"brand_name\"),\r\n",
							"        F.coalesce(\"new_products.sub_brand_name\", \"silver.sub_brand_name\").alias(\"sub_brand_name\"),\r\n",
							"        F.coalesce(\"new_products.category_name\", \"silver.category_name\").alias(\"category_name\")\r\n",
							"    )\r\n",
							"else:\r\n",
							"\r\n",
							"    new_products_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakoczenie\r\n",
							"print(\"Inkrementalne adowanie zakoczone!\")    \r\n",
							"\r\n",
							"\r\n",
							"spark.stop()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_PRODUCT_LOAD_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "246692f1-644b-4b6d-9551-3844358198b9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- ============================================\n",
							"-- 1. Register data from Bronze as a temporary view\n",
							"-- ============================================\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW bronze_products\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/products'\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 2. Select the most recent data by update_date\n",
							"-- ============================================\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW new_d_products AS\n",
							"SELECT \n",
							"    product_id,\n",
							"    product_name,\n",
							"    brand_name,\n",
							"    sub_brand_name,\n",
							"    category_name,\n",
							"    update_date AS product_update_date\n",
							"FROM bronze_products;\n",
							"\n",
							"-- ============================================\n",
							"-- 3. Create the target (Silver) table if it does not exist\n",
							"-- ============================================\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS silver.d_products\n",
							"USING delta\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/d_products'\n",
							"AS\n",
							"SELECT * FROM new_d_products\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- Filter new or updated records\n",
							"CREATE OR REPLACE TEMP VIEW filter_d_products AS\n",
							"SELECT * FROM new_d_products n \n",
							"WHERE NOT EXISTS (\n",
							"  SELECT 1 FROM silver.d_products s \n",
							"  WHERE s.product_id = n.product_id \n",
							"    AND s.product_update_date = n.product_update_date\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 4. MERGE INTO (UPSERT) data into the Silver table\n",
							"-- ============================================\n",
							"\n",
							"MERGE INTO silver.d_products AS target\n",
							"USING filter_d_products AS source\n",
							"ON target.product_id = source.product_id\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.product_name = source.product_name,\n",
							"    target.brand_name = source.brand_name,\n",
							"    target.sub_brand_name = source.sub_brand_name,\n",
							"    target.category_name = source.category_name,\n",
							"    target.product_update_date = source.product_update_date\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id,\n",
							"    product_name,\n",
							"    brand_name,\n",
							"    sub_brand_name,\n",
							"    category_name,\n",
							"    product_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id,\n",
							"    source.product_name,\n",
							"    source.brand_name,\n",
							"    source.sub_brand_name,\n",
							"    source.category_name,\n",
							"    source.product_update_date\n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_PRODUCT_LOAD_SQL_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cb285df3-aa4a-4420-b0de-de1ff30f492b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- ============================================\n",
							"-- 1. Rejestracja danych z Bronze jako tymczasowy widok\n",
							"-- ============================================\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW bronze_products\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/products'\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 2. Wybr najnowszych danych wg update_date\n",
							"-- ============================================\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW new_d_products AS\n",
							"SELECT \n",
							"    product_id,\n",
							"    product_name,\n",
							"    brand_name,\n",
							"    sub_brand_name,\n",
							"    category_name,\n",
							"    update_date AS product_update_date\n",
							"FROM bronze_products;\n",
							"\n",
							"-- ============================================\n",
							"-- 3. Stworzenie tabeli docelowej (Silver) jeli nie istnieje\n",
							"-- ============================================\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS silver.d_products\n",
							"USING delta\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/d_products'\n",
							"AS\n",
							"SELECT * FROM new_d_products\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- Filtrowanie nowych lub zaktualizowanych rekordw\n",
							"CREATE OR REPLACE TEMP VIEW filter_d_products AS\n",
							"SELECT * FROM new_d_products n \n",
							"WHERE NOT EXISTS (\n",
							"  SELECT 1 FROM silver.d_products s \n",
							"  WHERE s.product_id = n.product_id \n",
							"    AND s.product_update_date = n.product_update_date\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 4. MERGE INTO (UPSERT) danych do tabeli Silver\n",
							"-- ============================================\n",
							"\n",
							"MERGE INTO silver.d_products AS target\n",
							"USING filter_d_products AS source\n",
							"ON target.product_id = source.product_id\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.product_name = source.product_name,\n",
							"    target.brand_name = source.brand_name,\n",
							"    target.sub_brand_name = source.sub_brand_name,\n",
							"    target.category_name = source.category_name,\n",
							"    target.product_update_date = source.product_update_date\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id,\n",
							"    product_name,\n",
							"    brand_name,\n",
							"    sub_brand_name,\n",
							"    category_name,\n",
							"    product_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id,\n",
							"    source.product_name,\n",
							"    source.brand_name,\n",
							"    source.sub_brand_name,\n",
							"    source.category_name,\n",
							"    source.product_update_date);\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- ============================================\n",
							"-- 1. Register data from Bronze as a temporary view\n",
							"-- ============================================\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW bronze_products\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/products'\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 2. Select the most recent data by update_date\n",
							"-- ============================================\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW new_d_products AS\n",
							"SELECT \n",
							"    product_id,\n",
							"    product_name,\n",
							"    brand_name,\n",
							"    sub_brand_name,\n",
							"    category_name,\n",
							"    update_date AS product_update_date\n",
							"FROM bronze_products;\n",
							"\n",
							"-- ============================================\n",
							"-- 3. Create the target (Silver) table if it does not exist\n",
							"-- ============================================\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS silver.d_products\n",
							"USING delta\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/d_products'\n",
							"AS\n",
							"SELECT * FROM new_d_products\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- Filter new or updated records\n",
							"CREATE OR REPLACE TEMP VIEW filter_d_products AS\n",
							"SELECT * FROM new_d_products n \n",
							"WHERE NOT EXISTS (\n",
							"  SELECT 1 FROM silver.d_products s \n",
							"  WHERE s.product_id = n.product_id \n",
							"    AND s.product_update_date = n.product_update_date\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 4. MERGE INTO (UPSERT) data into the Silver table\n",
							"-- ============================================\n",
							"\n",
							"MERGE INTO silver.d_products AS target\n",
							"USING filter_d_products AS source\n",
							"ON target.product_id = source.product_id\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.product_name = source.product_name,\n",
							"    target.brand_name = source.brand_name,\n",
							"    target.sub_brand_name = source.sub_brand_name,\n",
							"    target.category_name = source.category_name,\n",
							"    target.product_update_date = source.product_update_date\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id,\n",
							"    product_name,\n",
							"    brand_name,\n",
							"    sub_brand_name,\n",
							"    category_name,\n",
							"    product_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id,\n",
							"    source.product_name,\n",
							"    source.brand_name,\n",
							"    source.sub_brand_name,\n",
							"    source.category_name,\n",
							"    source.product_update_date\n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_FORCAST_LOAD_PY')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive/Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3b4c0281-515b-4971-b576-bdeefeb17ba3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"\r\n",
							"# Definicja cieek dla warstwy Bronze (ADLS Gen2)\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"file_name = \"forecast_details\"\r\n",
							"\r\n",
							"# Definicja cieek dla warstwy Silver (ADLS Gen2)\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"f_forecast/\"\r\n",
							"\r\n",
							"silver_path = f\"{silver_bucket_name}{silver_prefix}\"\r\n",
							"\r\n",
							"# cieki do plikw Parquet w ADLS Gen2\r\n",
							"path = f\"{bronze_bucket_name}{bronze_prefix}{file_name}\"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"\r\n",
							"bronze_df = spark.read.parquet(path)\r\n",
							"\r\n",
							"new_bronze_df = bronze_df.withColumn(\r\n",
							"    \"forecast_date\", trunc(\"forecast_date\",\"month\")).withColumn(\r\n",
							"    \"forecast_snapshot\", trunc(\"forecast_snapshot\",\"month\"))\r\n",
							"\r\n",
							"latest_bronze_df = new_bronze_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"product_id\", \"country_id\",\"forecast_date\",\"forecast_snapshot\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"new_bronze_df = latest_bronze_df.select(\r\n",
							"    \"product_id\",\r\n",
							"    \"country_id\", \r\n",
							"    \"forecast_date\", \r\n",
							"    \"forecast_snapshot\",\r\n",
							"    \"unit_price\",\r\n",
							"    \"quantity\",\r\n",
							"    \"whrs_sell_in_quantity\",\r\n",
							"    \"whrs_open_quantity\",\r\n",
							"    \"whrs_end_quantity\",\r\n",
							"    \"pos_sell_out_quantity\",\r\n",
							"    \"pos_open_quantity\",\r\n",
							"    \"pos_end_quantity\"\r\n",
							")\r\n",
							"\r\n",
							"\r\n",
							"aggregated_bronze_df = new_bronze_df.groupBy(\"product_id\", \"country_id\", \"forecast_date\",\"forecast_snapshot\").agg(\r\n",
							"                                    # Summing quantity, discount, and amount\r\n",
							"                                     F.sum(\"quantity\").alias(\"quantity\"),\r\n",
							"                                     F.sum(\"whrs_sell_in_quantity\").alias(\"whrs_sell_in_quantity\"),\r\n",
							"                                     F.sum(\"whrs_open_quantity\").alias(\"whrs_open_quantity\"),\r\n",
							"                                     F.sum(\"whrs_end_quantity\").alias(\"whrs_end_quantity\"),\r\n",
							"                                     F.sum(\"pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"                                     F.sum(\"pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"                                     F.sum(\"pos_end_quantity\").alias(\"pos_end_quantity\"),         \r\n",
							"                                     F.round(F.sum(F.col(\"quantity\") * F.col(\"unit_price\")),2).alias(\"amount\"),                                                    \r\n",
							"                                     F.round(F.sum(F.col(\"whrs_sell_in_quantity\") * F.col(\"unit_price\")),2).alias(\"whrs_sell_in_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"whrs_open_quantity\") * F.col(\"unit_price\")),2).alias(\"whrs_open_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"whrs_end_quantity\") * F.col(\"unit_price\")),2).alias(\"whrs_end_amount\"),                                                                     \r\n",
							"                                     F.round(F.sum(F.col(\"pos_sell_out_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_sell_out_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_open_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_open_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_end_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_end_amount\"),\r\n",
							"                                     F.round((F.sum(F.col(\"quantity\") * F.col(\"unit_price\")) / F.sum(\"quantity\")),2).alias(\"unit_price\"))\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver ju istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"\r\n",
							"\r\n",
							"# Jeli tabela Silver istnieje, wykonaj operacj \"MERGE\" na podstawie DataFrame\r\n",
							"if silver_exists:\r\n",
							"    # Zaaduj dane z tabeli Silver\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    # Pocz dane Silver z nowymi danymi (na podstawie order_id, customer_id, product_id)\r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        aggregated_bronze_df.alias(\"new_pod\"),\r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_pod.product_id\")) & \r\n",
							"        (F.col(\"silver.forecast_snapshot\") == F.col(\"new_pod.forecast_snapshot\")) & \r\n",
							"        (F.col(\"silver.country_id\") == F.col(\"new_pod.country_id\")) & \r\n",
							"        (F.col(\"silver.forecast_date\") == F.col(\"new_pod.forecast_date\")),\r\n",
							"        how=\"outer\"\r\n",
							"    )\r\n",
							"\r\n",
							"    # Wybr kolumn, ktre maj zosta zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_pod.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_pod.country_id\", \"silver.country_id\").alias(\"country_id\"),\r\n",
							"        F.coalesce(\"new_pod.forecast_date\", \"silver.forecast_date\").alias(\"forecast_date\"),\r\n",
							"        F.coalesce(\"new_pod.forecast_snapshot\", \"silver.forecast_snapshot\").alias(\"forecast_snapshot\"),\r\n",
							"   \r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.quantity\", \"silver.quantity\").alias(\"quantity\"),\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.whrs_sell_in_quantity\", \"silver.whrs_sell_in_quantity\").alias(\"whrs_sell_in_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_open_quantity\", \"silver.whrs_open_quantity\").alias(\"whrs_open_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_end_quantity\", \"silver.whrs_end_quantity\").alias(\"whrs_end_quantity\"),  \r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.pos_sell_out_quantity\", \"silver.pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_open_quantity\", \"silver.pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_end_quantity\", \"silver.pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.amount\", \"silver.amount\").alias(\"amount\"),\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.whrs_sell_in_amount\", \"silver.whrs_sell_in_amount\").alias(\"whrs_sell_in_amount\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_open_amount\", \"silver.whrs_open_amount\").alias(\"whrs_open_amount\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_end_amount\", \"silver.whrs_end_amount\").alias(\"whrs_end_amount\"),\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.pos_sell_out_amount\", \"silver.pos_sell_out_amount\").alias(\"pos_sell_out_amount\"),\r\n",
							"        F.coalesce(\"new_pod.pos_open_amount\", \"silver.pos_open_amount\").alias(\"pos_open_amount\"),\r\n",
							"        F.coalesce(\"new_pod.pos_end_amount\", \"silver.pos_end_amount\").alias(\"pos_end_amount\"),\r\n",
							"\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.unit_price\", \"silver.unit_price\").alias(\"unit_price\")\r\n",
							"        )\r\n",
							"    # Display or save the resulting DataFrame\r\n",
							"    # Zapisanie zaktualizowanego DataFrame do warstwy Silver\r\n",
							"    final_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"else:\r\n",
							"    aggregated_bronze_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakoczenie\r\n",
							"print(\"Inkrementalne adowanie zakoczone!\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_FORCAST_LOAD_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7695b1ce-d578-4022-9fc7-cea0e643475a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- ============================================\n",
							"-- 1. Load data from the Bronze layer as a temporary view\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW bronze_forecast\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/forecast_details'\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 2. Aggregate the most recent data at the monthly level\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_forecast AS\n",
							"SELECT \n",
							"    product_id,\n",
							"    country_id,\n",
							"    max(update_date) as forecast_update_date,\n",
							"    TRUNC(forecast_date, \"MM\") AS forecast_date,\n",
							"    TRUNC(forecast_snapshot, \"MM\") AS forecast_snapshot,\n",
							"    \n",
							"    SUM(quantity) AS quantity,\n",
							"    SUM(whrs_sell_in_quantity) AS whrs_sell_in_quantity,\n",
							"    SUM(whrs_open_quantity) AS whrs_open_quantity,\n",
							"    SUM(whrs_end_quantity) AS whrs_end_quantity,\n",
							"    \n",
							"    SUM(pos_sell_out_quantity) AS pos_sell_out_quantity,\n",
							"    SUM(pos_open_quantity) AS pos_open_quantity,\n",
							"    SUM(pos_end_quantity) AS pos_end_quantity,\n",
							"    \n",
							"    ROUND(SUM(quantity * unit_price), 2) AS amount,\n",
							"    ROUND(SUM(whrs_sell_in_quantity * unit_price), 2) AS whrs_sell_in_amount,\n",
							"    ROUND(SUM(whrs_open_quantity * unit_price), 2) AS whrs_open_amount,\n",
							"    ROUND(SUM(whrs_end_quantity * unit_price), 2) AS whrs_end_amount,\n",
							"    \n",
							"    ROUND(SUM(pos_sell_out_quantity * unit_price), 2) AS pos_sell_out_amount,\n",
							"    ROUND(SUM(pos_open_quantity * unit_price), 2) AS pos_open_amount,\n",
							"    ROUND(SUM(pos_end_quantity * unit_price), 2) AS pos_end_amount,\n",
							"    \n",
							"    ROUND(SUM(quantity * unit_price) / NULLIF(SUM(quantity), 0), 2) AS unit_price\n",
							"FROM  bronze_forecast\n",
							"GROUP BY product_id, country_id, TRUNC(forecast_date, 'MM'), TRUNC(forecast_snapshot, 'MM');\n",
							"\n",
							"-- ============================================\n",
							"-- 3. Create the Silver table if it does not exist\n",
							"-- ============================================\n",
							"CREATE TABLE IF NOT EXISTS silver.f_forecast\n",
							"USING delta\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/f_forecast'\n",
							"AS\n",
							"SELECT * FROM aggregated_forecast\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- Filter new or updated records\n",
							"CREATE OR REPLACE TEMP VIEW filter_aggregated_forecast AS\n",
							"SELECT * from aggregated_forecast n \n",
							"where  NOT EXISTS (select product_id From silver.f_forecast s \n",
							"                    where s.product_id = n.product_id \n",
							"                      and s.country_id = n.country_id\n",
							"                      and s.forecast_update_date = n.forecast_update_date\n",
							"                      and s.forecast_date = n.forecast_date\n",
							"                      and s.forecast_snapshot = n.forecast_snapshot);\n",
							"\n",
							"-- ============================================\n",
							"-- 4. Perform MERGE INTO to update or insert new data\n",
							"-- ============================================\n",
							"\n",
							"MERGE INTO silver.f_forecast AS target\n",
							"USING filter_aggregated_forecast AS source\n",
							"ON target.product_id = source.product_id\n",
							"   AND target.country_id = source.country_id\n",
							"   AND target.forecast_date = source.forecast_date\n",
							"   AND target.forecast_snapshot = source.forecast_snapshot\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.quantity = source.quantity,\n",
							"    target.whrs_sell_in_quantity = source.whrs_sell_in_quantity,\n",
							"    target.whrs_open_quantity = source.whrs_open_quantity,\n",
							"    target.whrs_end_quantity = source.whrs_end_quantity,\n",
							"    \n",
							"    target.pos_sell_out_quantity = source.pos_sell_out_quantity,\n",
							"    target.pos_open_quantity = source.pos_open_quantity,\n",
							"    target.pos_end_quantity = source.pos_end_quantity,\n",
							"    \n",
							"    target.amount = source.amount,\n",
							"    target.whrs_sell_in_amount = source.whrs_sell_in_amount,\n",
							"    target.whrs_open_amount = source.whrs_open_amount,\n",
							"    target.whrs_end_amount = source.whrs_end_amount,\n",
							"    \n",
							"    target.pos_sell_out_amount = source.pos_sell_out_amount,\n",
							"    target.pos_open_amount = source.pos_open_amount,\n",
							"    target.pos_end_amount = source.pos_end_amount,\n",
							"    \n",
							"    target.unit_price = source.unit_price,\n",
							"    target.forecast_update_date = source.forecast_update_date\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id, country_id, forecast_date, forecast_snapshot,\n",
							"    quantity, whrs_sell_in_quantity, whrs_open_quantity, whrs_end_quantity,\n",
							"    pos_sell_out_quantity, pos_open_quantity, pos_end_quantity,\n",
							"    amount, whrs_sell_in_amount, whrs_open_amount, whrs_end_amount,\n",
							"    pos_sell_out_amount, pos_open_amount, pos_end_amount, unit_price, forecast_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id, source.country_id, source.forecast_date, source.forecast_snapshot,\n",
							"    source.quantity, source.whrs_sell_in_quantity, source.whrs_open_quantity, source.whrs_end_quantity,\n",
							"    source.pos_sell_out_quantity, source.pos_open_quantity, source.pos_end_quantity,\n",
							"    source.amount, source.whrs_sell_in_amount, source.whrs_open_amount, source.whrs_end_amount,\n",
							"    source.pos_sell_out_amount, source.pos_open_amount, source.pos_end_amount, source.unit_price, source.forecast_update_date\n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_FORCAST_LOAD_SQL_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b1b2806f-7bde-4887-ba25-ef1a1bc6d0fc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- ============================================\n",
							"-- 1. Zaaduj dane z warstwy Bronze jako tymczasowy widok\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW bronze_forecast\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/forecast_details'\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 2. Agregacja najnowszych danych na poziomie miesica\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_forecast AS\n",
							"SELECT \n",
							"    product_id,\n",
							"    country_id,\n",
							"    max(update_date) as forecast_update_date,\n",
							"    TRUNC(forecast_date, \"MM\") AS forecast_date,\n",
							"    TRUNC(forecast_snapshot, \"MM\") AS forecast_snapshot,\n",
							"    \n",
							"    SUM(quantity) AS quantity,\n",
							"    SUM(whrs_sell_in_quantity) AS whrs_sell_in_quantity,\n",
							"    SUM(whrs_open_quantity) AS whrs_open_quantity,\n",
							"    SUM(whrs_end_quantity) AS whrs_end_quantity,\n",
							"    \n",
							"    SUM(pos_sell_out_quantity) AS pos_sell_out_quantity,\n",
							"    SUM(pos_open_quantity) AS pos_open_quantity,\n",
							"    SUM(pos_end_quantity) AS pos_end_quantity,\n",
							"    \n",
							"    ROUND(SUM(quantity * unit_price), 2) AS amount,\n",
							"    ROUND(SUM(whrs_sell_in_quantity * unit_price), 2) AS whrs_sell_in_amount,\n",
							"    ROUND(SUM(whrs_open_quantity * unit_price), 2) AS whrs_open_amount,\n",
							"    ROUND(SUM(whrs_end_quantity * unit_price), 2) AS whrs_end_amount,\n",
							"    \n",
							"    ROUND(SUM(pos_sell_out_quantity * unit_price), 2) AS pos_sell_out_amount,\n",
							"    ROUND(SUM(pos_open_quantity * unit_price), 2) AS pos_open_amount,\n",
							"    ROUND(SUM(pos_end_quantity * unit_price), 2) AS pos_end_amount,\n",
							"    \n",
							"    ROUND(SUM(quantity * unit_price) / NULLIF(SUM(quantity), 0), 2) AS unit_price\n",
							"FROM  bronze_forecast\n",
							"GROUP BY product_id, country_id, TRUNC(forecast_date, 'MM'), TRUNC(forecast_snapshot, 'MM');\n",
							"\n",
							"-- ============================================\n",
							"-- 3. Stwrz tabel Silver, jeli nie istnieje\n",
							"-- ============================================\n",
							"CREATE TABLE IF NOT EXISTS silver.f_forecast\n",
							"USING delta\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/f_forecast'\n",
							"AS\n",
							"SELECT * FROM aggregated_forecast\n",
							"WHERE 1 = 0;\n",
							"\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW filter_aggregated_forecast AS\n",
							"SELECT * from aggregated_forecast n \n",
							"where  NOT EXISTS (select product_id From silver.f_forecast s \n",
							"                    where s.product_id =n.product_id \n",
							"                      and s.country_id = n.country_id\n",
							"                      and s.forecast_update_date = n.forecast_update_date\n",
							"                      and s.forecast_date = n.forecast_date\n",
							"                      and s.forecast_snapshot= n.forecast_snapshot);\n",
							"\n",
							"-- ============================================\n",
							"-- 4. Wykonaj MERGE INTO, aby zaktualizowa lub doda nowe dane\n",
							"-- ============================================\n",
							"\n",
							"MERGE INTO silver.f_forecast AS target\n",
							"USING filter_aggregated_forecast AS source\n",
							"ON target.product_id = source.product_id\n",
							"   AND target.country_id = source.country_id\n",
							"   AND target.forecast_date = source.forecast_date\n",
							"   AND target.forecast_snapshot = source.forecast_snapshot\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.quantity = source.quantity,\n",
							"    target.whrs_sell_in_quantity = source.whrs_sell_in_quantity,\n",
							"    target.whrs_open_quantity = source.whrs_open_quantity,\n",
							"    target.whrs_end_quantity = source.whrs_end_quantity,\n",
							"    \n",
							"    target.pos_sell_out_quantity = source.pos_sell_out_quantity,\n",
							"    target.pos_open_quantity = source.pos_open_quantity,\n",
							"    target.pos_end_quantity = source.pos_end_quantity,\n",
							"    \n",
							"    target.amount = source.amount,\n",
							"    target.whrs_sell_in_amount = source.whrs_sell_in_amount,\n",
							"    target.whrs_open_amount = source.whrs_open_amount,\n",
							"    target.whrs_end_amount = source.whrs_end_amount,\n",
							"    \n",
							"    target.pos_sell_out_amount = source.pos_sell_out_amount,\n",
							"    target.pos_open_amount = source.pos_open_amount,\n",
							"    target.pos_end_amount = source.pos_end_amount,\n",
							"    \n",
							"    target.unit_price = source.unit_price,\n",
							"    target.forecast_update_date = source.forecast_update_date\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id, country_id, forecast_date, forecast_snapshot,\n",
							"    quantity, whrs_sell_in_quantity, whrs_open_quantity, whrs_end_quantity,\n",
							"    pos_sell_out_quantity, pos_open_quantity, pos_end_quantity,\n",
							"    amount, whrs_sell_in_amount, whrs_open_amount, whrs_end_amount,\n",
							"    pos_sell_out_amount, pos_open_amount, pos_end_amount, unit_price,forecast_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id, source.country_id, source.forecast_date, source.forecast_snapshot,\n",
							"    source.quantity, source.whrs_sell_in_quantity, source.whrs_open_quantity, source.whrs_end_quantity,\n",
							"    source.pos_sell_out_quantity, source.pos_open_quantity, source.pos_end_quantity,\n",
							"    source.amount, source.whrs_sell_in_amount, source.whrs_open_amount, source.whrs_end_amount,\n",
							"    source.pos_sell_out_amount, source.pos_open_amount, source.pos_end_amount, source.unit_price,source.forecast_update_date\n",
							"  );"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- ============================================\n",
							"-- 1. Load data from the Bronze layer as a temporary view\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW bronze_forecast\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/forecast_details'\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 2. Aggregate the most recent data at the monthly level\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_forecast AS\n",
							"SELECT \n",
							"    product_id,\n",
							"    country_id,\n",
							"    max(update_date) as forecast_update_date,\n",
							"    TRUNC(forecast_date, \"MM\") AS forecast_date,\n",
							"    TRUNC(forecast_snapshot, \"MM\") AS forecast_snapshot,\n",
							"    \n",
							"    SUM(quantity) AS quantity,\n",
							"    SUM(whrs_sell_in_quantity) AS whrs_sell_in_quantity,\n",
							"    SUM(whrs_open_quantity) AS whrs_open_quantity,\n",
							"    SUM(whrs_end_quantity) AS whrs_end_quantity,\n",
							"    \n",
							"    SUM(pos_sell_out_quantity) AS pos_sell_out_quantity,\n",
							"    SUM(pos_open_quantity) AS pos_open_quantity,\n",
							"    SUM(pos_end_quantity) AS pos_end_quantity,\n",
							"    \n",
							"    ROUND(SUM(quantity * unit_price), 2) AS amount,\n",
							"    ROUND(SUM(whrs_sell_in_quantity * unit_price), 2) AS whrs_sell_in_amount,\n",
							"    ROUND(SUM(whrs_open_quantity * unit_price), 2) AS whrs_open_amount,\n",
							"    ROUND(SUM(whrs_end_quantity * unit_price), 2) AS whrs_end_amount,\n",
							"    \n",
							"    ROUND(SUM(pos_sell_out_quantity * unit_price), 2) AS pos_sell_out_amount,\n",
							"    ROUND(SUM(pos_open_quantity * unit_price), 2) AS pos_open_amount,\n",
							"    ROUND(SUM(pos_end_quantity * unit_price), 2) AS pos_end_amount,\n",
							"    \n",
							"    ROUND(SUM(quantity * unit_price) / NULLIF(SUM(quantity), 0), 2) AS unit_price\n",
							"FROM  bronze_forecast\n",
							"GROUP BY product_id, country_id, TRUNC(forecast_date, 'MM'), TRUNC(forecast_snapshot, 'MM');\n",
							"\n",
							"-- ============================================\n",
							"-- 3. Create the Silver table if it does not exist\n",
							"-- ============================================\n",
							"CREATE TABLE IF NOT EXISTS silver.f_forecast\n",
							"USING delta\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/f_forecast'\n",
							"AS\n",
							"SELECT * FROM aggregated_forecast\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- Filter new or updated records\n",
							"CREATE OR REPLACE TEMP VIEW filter_aggregated_forecast AS\n",
							"SELECT * from aggregated_forecast n \n",
							"where  NOT EXISTS (select product_id From silver.f_forecast s \n",
							"                    where s.product_id = n.product_id \n",
							"                      and s.country_id = n.country_id\n",
							"                      and s.forecast_update_date = n.forecast_update_date\n",
							"                      and s.forecast_date = n.forecast_date\n",
							"                      and s.forecast_snapshot = n.forecast_snapshot);\n",
							"\n",
							"-- ============================================\n",
							"-- 4. Perform MERGE INTO to update or insert new data\n",
							"-- ============================================\n",
							"\n",
							"MERGE INTO silver.f_forecast AS target\n",
							"USING filter_aggregated_forecast AS source\n",
							"ON target.product_id = source.product_id\n",
							"   AND target.country_id = source.country_id\n",
							"   AND target.forecast_date = source.forecast_date\n",
							"   AND target.forecast_snapshot = source.forecast_snapshot\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.quantity = source.quantity,\n",
							"    target.whrs_sell_in_quantity = source.whrs_sell_in_quantity,\n",
							"    target.whrs_open_quantity = source.whrs_open_quantity,\n",
							"    target.whrs_end_quantity = source.whrs_end_quantity,\n",
							"    \n",
							"    target.pos_sell_out_quantity = source.pos_sell_out_quantity,\n",
							"    target.pos_open_quantity = source.pos_open_quantity,\n",
							"    target.pos_end_quantity = source.pos_end_quantity,\n",
							"    \n",
							"    target.amount = source.amount,\n",
							"    target.whrs_sell_in_amount = source.whrs_sell_in_amount,\n",
							"    target.whrs_open_amount = source.whrs_open_amount,\n",
							"    target.whrs_end_amount = source.whrs_end_amount,\n",
							"    \n",
							"    target.pos_sell_out_amount = source.pos_sell_out_amount,\n",
							"    target.pos_open_amount = source.pos_open_amount,\n",
							"    target.pos_end_amount = source.pos_end_amount,\n",
							"    \n",
							"    target.unit_price = source.unit_price,\n",
							"    target.forecast_update_date = source.forecast_update_date\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id, country_id, forecast_date, forecast_snapshot,\n",
							"    quantity, whrs_sell_in_quantity, whrs_open_quantity, whrs_end_quantity,\n",
							"    pos_sell_out_quantity, pos_open_quantity, pos_end_quantity,\n",
							"    amount, whrs_sell_in_amount, whrs_open_amount, whrs_end_amount,\n",
							"    pos_sell_out_amount, pos_open_amount, pos_end_amount, unit_price, forecast_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id, source.country_id, source.forecast_date, source.forecast_snapshot,\n",
							"    source.quantity, source.whrs_sell_in_quantity, source.whrs_open_quantity, source.whrs_end_quantity,\n",
							"    source.pos_sell_out_quantity, source.pos_open_quantity, source.pos_end_quantity,\n",
							"    source.amount, source.whrs_sell_in_amount, source.whrs_open_amount, source.whrs_end_amount,\n",
							"    source.pos_sell_out_amount, source.pos_open_amount, source.pos_end_amount, source.unit_price, source.forecast_update_date\n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_PLANNING_BOOK_PY')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load Gold"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d829cf06-f94d-4cd8-bd98-1b931a14e992"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc, lit, add_months\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql.functions import to_date\r\n",
							"\r\n",
							"# Create Spark session\r\n",
							"spark = SparkSession.builder.appName(\"Load Planning Book\").getOrCreate()\r\n",
							"\r\n",
							"# Parameters for the Silver layer (ADLS Gen2)\r\n",
							"bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"bucket_name_gold = \"abfss://gold@bigpharma.dfs.core.windows.net/\"\r\n",
							"\r\n",
							"# Paths to Parquet files in ADLS Gen2\r\n",
							"f_sales_path = f\"{bucket_name}/f_sales\"\r\n",
							"f_forecast_path = f\"{bucket_name}/f_forecast\"\r\n",
							"f_pos_data_path = f\"{bucket_name}/f_pos_data\"\r\n",
							"f_wh_data_path = f\"{bucket_name}/f_wh_data\"\r\n",
							"\r\n",
							"# Path to the Gold layer (f_planning_book)\r\n",
							"f_sales_path_gold = f\"{bucket_name_gold}/f_planning_book\"\r\n",
							"\r\n",
							"# Load data from ADLS (tables f_sales, f_forecast, f_pos_data, f_wh_data)\r\n",
							"f_sales = spark.read.parquet(f_sales_path)\r\n",
							"f_forecast = spark.read.parquet(f_forecast_path)\r\n",
							"f_pos_data = spark.read.parquet(f_pos_data_path)\r\n",
							"f_wh_data = spark.read.parquet(f_wh_data_path)\r\n",
							"\r\n",
							"# Get the maximum value of forecast_snapshot from f_forecast\r\n",
							"current_month_snapshot = f_forecast.agg(F.max(\"forecast_snapshot\")).collect()[0][0]\r\n",
							"current_month_snapshot = to_date(lit(current_month_snapshot), \"yyyy-MM-dd\")\r\n",
							"last_month_snapshot = F.add_months(F.lit(current_month_snapshot), -1)\r\n",
							"\r\n",
							"f_forecast_filtered = f_forecast.filter(F.col(\"forecast_snapshot\").isin(current_month_snapshot, last_month_snapshot))\r\n",
							"\r\n",
							"# Filter for the current and last month\r\n",
							"f_forecast_current = f_forecast_filtered.filter((F.col(\"forecast_date\") >= current_month_snapshot) & (F.col(\"forecast_snapshot\") == current_month_snapshot))\r\n",
							"f_forecast_last = f_forecast_filtered.filter((F.col(\"forecast_date\") >= last_month_snapshot) & (F.col(\"forecast_snapshot\") == last_month_snapshot))\r\n",
							"\r\n",
							"# Filter f_sales\r\n",
							"f_sales_current = f_sales.filter(F.col(\"shipped_date\") < current_month_snapshot)\r\n",
							"f_sales_last = f_sales.filter(F.col(\"shipped_date\") < last_month_snapshot)\r\n",
							"\r\n",
							"f_pos_data_current = f_pos_data.filter(F.col(\"transaction_date\") < current_month_snapshot)\r\n",
							"f_pos_data_last = f_pos_data.filter(F.col(\"transaction_date\") < last_month_snapshot)\r\n",
							"\r\n",
							"f_wh_data_current = f_wh_data.filter(F.col(\"transaction_date\") < current_month_snapshot)\r\n",
							"f_wh_data_last = f_wh_data.filter(F.col(\"transaction_date\") < last_month_snapshot)\r\n",
							"\r\n",
							"# Unified column list\r\n",
							"common_columns = [\r\n",
							"    \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"    \"quantity\", \"amount\", \"unit_price\",\r\n",
							"    \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"    \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"    \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"    \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"    \"discount\"\r\n",
							"]\r\n",
							"\r\n",
							"# Normalize f_forecast (keeps the original forecast_snapshot)\r\n",
							"f_forecast_current_norm = f_forecast_current \\\r\n",
							"    .withColumnRenamed(\"forecast_date\", \"financial_date\") \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0.0 as discount\"  # If discount is of type double\r\n",
							"    )\r\n",
							"\r\n",
							"f_forecast_last_norm = f_forecast_last \\\r\n",
							"    .withColumnRenamed(\"forecast_date\", \"financial_date\") \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0.0 as discount\"  # If discount is of type double\r\n",
							"    )\r\n",
							"\r\n",
							"# Normalize f_sales\r\n",
							"f_sales_current_norm = f_sales_current \\\r\n",
							"    .withColumnRenamed(\"shipped_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", current_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"f_sales_last_norm = f_sales_last \\\r\n",
							"    .withColumnRenamed(\"shipped_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", last_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"# Normalize f_pos_data_path\r\n",
							"f_pos_data_current_norm = f_pos_data_current \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", current_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"f_pos_data_last_norm = f_pos_data_last \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", last_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"# Normalize f_wh_data_path\r\n",
							"f_wh_data_current_norm = f_wh_data_current \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", current_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"f_wh_data_last_norm = f_wh_data_last \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", last_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"    # UNION of all DataFrames\r\n",
							"\r\n",
							"f_forecast_current_norm\r\n",
							"f_forecast_last_norm\r\n",
							"f_sales_current_norm \r\n",
							"f_sales_last_norm\r\n",
							"f_pos_data_current_norm\r\n",
							"f_pos_data_last_norm\r\n",
							"f_wh_data_current_norm\r\n",
							"f_wh_data_last_norm\r\n",
							"\r\n",
							"final_df = f_forecast_current_norm.unionByName(f_forecast_last_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_sales_current_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_sales_last_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_pos_data_current_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_pos_data_last_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_wh_data_current_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_wh_data_last_norm, allowMissingColumns=True) \r\n",
							"\r\n",
							"bucket_name_gold = \"gold\"\r\n",
							"\r\n",
							"# Load Data\r\n",
							"f_sales_path = f\"s3a://{bucket_name}/f_planning_book\"\r\n",
							"\r\n",
							"aggregated_df = final_df.groupBy(\r\n",
							"    \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\"\r\n",
							").agg(\r\n",
							"    # Sum for numerical columns\r\n",
							"    F.sum(\"quantity\").alias(\"quantity\"),\r\n",
							"    F.sum(\"amount\").alias(\"amount\"),\r\n",
							"    F.avg(\"unit_price\").alias(\"unit_price\"),\r\n",
							"    \r\n",
							"    # Sum for other columns\r\n",
							"    F.sum(\"whrs_sell_in_quantity\").alias(\"whrs_sell_in_quantity\"),\r\n",
							"    F.sum(\"whrs_open_quantity\").alias(\"whrs_open_quantity\"),\r\n",
							"    F.sum(\"whrs_end_quantity\").alias(\"whrs_end_quantity\"),\r\n",
							"    F.sum(\"whrs_sell_in_amount\").alias(\"whrs_sell_in_amount\"),\r\n",
							"    F.sum(\"whrs_open_amount\").alias(\"whrs_open_amount\"),\r\n",
							"    F.sum(\"whrs_end_amount\").alias(\"whrs_end_amount\"),\r\n",
							"    \r\n",
							"    F.sum(\"pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"    F.sum(\"pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"    F.sum(\"pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"    F.sum(\"pos_sell_out_amount\").alias(\"pos_sell_out_amount\"),\r\n",
							"    F.sum(\"pos_open_amount\").alias(\"pos_open_amount\"),\r\n",
							"    F.sum(\"pos_end_amount\").alias(\"pos_end_amount\"),\r\n",
							"    \r\n",
							"    # Aggregation for discount (average)\r\n",
							"    F.sum(\"discount\").alias(\"discount\")\r\n",
							")\r\n",
							"\r\n",
							"aggregated_df.write.mode(\"overwrite\").parquet(f_sales_path_gold)\r\n",
							"\r\n",
							"import pandas as pd\r\n",
							"\r\n",
							"# Load data\r\n",
							"df_pandas = aggregated_df.toPandas()\r\n",
							"\r\n",
							"df_pandas = df_pandas.drop(columns=['discount', 'unit_price'])\r\n",
							"\r\n",
							"# Unpivoting (melt)\r\n",
							"df_melted = df_pandas.melt(\r\n",
							"   id_vars=['product_id', 'country_id', 'financial_date', 'forecast_snapshot'],\r\n",
							"    var_name=\"original_column\",\r\n",
							"    value_name=\"value\"\r\n",
							")\r\n",
							"\r\n",
							"# Create a new 'measure' column based on the presence of 'quantity' or 'amount' in the column name\r\n",
							"df_melted[\"Measure\"] = df_melted[\"original_column\"].apply(\r\n",
							"    lambda x: \"EA\" if \"quantity\" in x.lower() else \"GTS\" if \"amount\" in x.lower() else \"unknown\"\r\n",
							")\r\n",
							"\r\n",
							"# Mapping column names to new formats\r\n",
							"measure_mapping = {\r\n",
							"    'quantity': 'Ex-Factory',\r\n",
							"    'amount': 'Ex-Factory',\r\n",
							"    'whrs_sell_in_quantity': 'Sales to pharmacies',\r\n",
							"    'whrs_open_quantity': 'Open Stock',\r\n",
							"    'whrs_end_quantity': 'Close Stock',\r\n",
							"    'whrs_sell_in_amount': 'Sales to pharmacies',\r\n",
							"    'whrs_open_amount': 'Open Stock',\r\n",
							"    'whrs_end_amount': 'Close Stock',\r\n",
							"    'pos_sell_out_quantity': 'Consumer Off Take',\r\n",
							"    'pos_open_quantity': 'Open Stock Pharmacies',\r\n",
							"    'pos_end_quantity': 'Close Stock Pharmacies',\r\n",
							"    'pos_sell_out_amount': 'Consumer Off Take',\r\n",
							"    'pos_open_amount': 'Open Stock Pharmacies',\r\n",
							"    'pos_end_amount': 'Close Stock Pharmacies'\r\n",
							"}\r\n",
							"\r\n",
							"# Apply the mapping\r\n",
							"df_melted[\"original_column\"] = df_melted[\"original_column\"].replace(measure_mapping)\r\n",
							"\r\n",
							"# Remove rows with missing values\r\n",
							"df_melted = df_melted.dropna(subset=[\"original_column\", \"value\"])\r\n",
							"\r\n",
							"# Pivoting data\r\n",
							"df_pivoted = df_melted.pivot_table(\r\n",
							"    index=['product_id', 'country_id', 'financial_date', 'forecast_snapshot', 'Measure'],  \r\n",
							"    columns='original_column',\r\n",
							"    values='value',\r\n",
							"    aggfunc='first'  # or 'sum', if you want to sum values\r\n",
							").reset_index()\r\n",
							"\r\n",
							"# Remove the column name after pivoting\r\n",
							"df_pivoted.columns.name = None\r\n",
							"\r\n",
							"# Find the maximum date\r\n",
							"max_forecast_snapshot = df_pivoted[\"forecast_snapshot\"].max()\r\n",
							"\r\n",
							"# Add a status column\r\n",
							"df_pivoted[\"Version\"] =  df_pivoted[\"forecast_snapshot\"].apply(\r\n",
							"    lambda x: \"Current\" if x == max_forecast_snapshot else \"Last\"\r\n",
							")\r\n",
							"\r\n",
							"bucket_name_gold = \"abfss://gold@bigpharma.dfs.core.windows.net/f_planning_book_unpivot\"\r\n",
							"\r\n",
							"# Path to the Gold layer (f_planning_book)\r\n",
							"f_sales_path_gold = f\"{bucket_name_gold}/f_planning_book_unpivot\"\r\n",
							"\r\n",
							"df_spark = spark.createDataFrame(df_pivoted)\r\n",
							"\r\n",
							"df_spark = df_spark.withColumnRenamed('product_id', 'IdProduct') \\\r\n",
							"    .withColumnRenamed('country_id', 'IDCountry') \\\r\n",
							"    .withColumnRenamed('financial_date', 'Date') \\\r\n",
							"    .withColumnRenamed('Close Stock', 'Close_Stock') \\\r\n",
							"    .withColumnRenamed('Ex-Factory', 'Ex_Factory') \\\r\n",
							"    .withColumnRenamed('Open Stock', 'Open_Stock') \\\r\n",
							"    .withColumnRenamed('Open Stock Pharmacies', 'Open_Stock_Pharmacies') \\\r\n",
							"    .withColumnRenamed('Sales to pharmacies', 'Sales_to_pharmacies') \\\r\n",
							"    .withColumnRenamed('Close Stock Pharmacies', 'Close_Stock_Pharmacies')\\\r\n",
							"    .withColumnRenamed('Consumer Off Take', 'Consumer_Off_Take')\r\n",
							"\r\n",
							"df_spark.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").save(\"abfss://gold@bigpharma.dfs.core.windows.net/f_planning_book_unpivot\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_PLANNING_BOOK_PY_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7770b994-ee3a-4149-b7bf-41f6162c0a3c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc, lit, add_months\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql.functions import to_date\r\n",
							"\r\n",
							"# Tworzenie sesji Spark\r\n",
							"spark = SparkSession.builder.appName(\"Load Planning Book\").getOrCreate()\r\n",
							"\r\n",
							"# Parametry dla warstwy Silver (ADLS Gen2)\r\n",
							"bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"bucket_name_gold = \"abfss://gold@bigpharma.dfs.core.windows.net/\"\r\n",
							"\r\n",
							"# cieki do plikw Parquet w ADLS Gen2\r\n",
							"f_sales_path = f\"{bucket_name}/f_sales\"\r\n",
							"f_forecast_path = f\"{bucket_name}/f_forecast\"\r\n",
							"f_pos_data_path = f\"{bucket_name}/f_pos_data\"\r\n",
							"f_wh_data_path = f\"{bucket_name}/f_wh_data\"\r\n",
							"\r\n",
							"# cieka do warstwy Gold (f_planning_book)\r\n",
							"f_sales_path_gold = f\"{bucket_name_gold}/f_planning_book\"\r\n",
							"\r\n",
							"# Zaaduj dane z ADLS (tabele f_sales, f_forecast, f_pos_data, f_wh_data)\r\n",
							"f_sales = spark.read.parquet(f_sales_path)\r\n",
							"f_forecast = spark.read.parquet(f_forecast_path)\r\n",
							"f_pos_data = spark.read.parquet(f_pos_data_path)\r\n",
							"f_wh_data = spark.read.parquet(f_wh_data_path)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Pobranie maksymalnej wartoci forecast_snapshot z f_forecast\r\n",
							"current_month_snapshot = f_forecast.agg(F.max(\"forecast_snapshot\")).collect()[0][0]\r\n",
							"current_month_snapshot = to_date(lit(current_month_snapshot), \"yyyy-MM-dd\")\r\n",
							"last_month_snapshot = F.add_months(F.lit(current_month_snapshot), -1)\r\n",
							"\r\n",
							"f_forecast_filtered = f_forecast.filter(F.col(\"forecast_snapshot\").isin(current_month_snapshot, last_month_snapshot))\r\n",
							"\r\n",
							"# Filtrowanie dla obecnego i poprzedniego miesica\r\n",
							"f_forecast_current = f_forecast_filtered.filter((F.col(\"forecast_date\") >= current_month_snapshot) &(F.col(\"forecast_snapshot\") == current_month_snapshot))\r\n",
							"f_forecast_last = f_forecast_filtered.filter((F.col(\"forecast_date\") >= last_month_snapshot) &  (F.col(\"forecast_snapshot\") == last_month_snapshot))\r\n",
							"\r\n",
							"# Filtrowanie f_sales\r\n",
							"\r\n",
							"f_sales_current = f_sales.filter(F.col(\"shipped_date\") < current_month_snapshot)\r\n",
							"f_sales_last = f_sales.filter(F.col(\"shipped_date\") < last_month_snapshot)\r\n",
							"\r\n",
							"f_pos_data_current = f_pos_data.filter(F.col(\"transaction_date\") < current_month_snapshot)\r\n",
							"f_pos_data_last = f_pos_data.filter(F.col(\"transaction_date\") < last_month_snapshot)\r\n",
							"\r\n",
							"f_wh_data_current = f_wh_data.filter(F.col(\"transaction_date\") < current_month_snapshot)\r\n",
							"f_wh_data_last = f_wh_data.filter(F.col(\"transaction_date\") < last_month_snapshot)\r\n",
							"\r\n",
							"\r\n",
							"# Ujednolicona lista kolumn\r\n",
							"common_columns = [\r\n",
							"    \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"    \"quantity\", \"amount\", \"unit_price\",\r\n",
							"    \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"    \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"    \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"    \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"    \"discount\"\r\n",
							"]\r\n",
							"\r\n",
							"# Normalizacja f_forecast (zachowuje oryginalny forecast_snapshot)\r\n",
							"f_forecast_current_norm = f_forecast_current \\\r\n",
							"    .withColumnRenamed(\"forecast_date\", \"financial_date\") \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0.0 as discount\"  # Jeeli discount jest typu double\r\n",
							"    )\r\n",
							"\r\n",
							"f_forecast_last_norm = f_forecast_last \\\r\n",
							"    .withColumnRenamed(\"forecast_date\", \"financial_date\") \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0.0 as discount\"  # Jeeli discount jest typu double\r\n",
							"    )\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Normalizacja f_sales\r\n",
							"f_sales_current_norm = f_sales_current \\\r\n",
							"    .withColumnRenamed(\"shipped_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\",  current_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"f_sales_last_norm = f_sales_last \\\r\n",
							"    .withColumnRenamed(\"shipped_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", last_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"# Normalizacja f_pos_data_path\r\n",
							"f_pos_data_current_norm = f_pos_data_current \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", current_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"f_pos_data_last_norm = f_pos_data_last \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", last_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"# Normalizacja f_wh_data_path\r\n",
							"f_wh_data_current_norm = f_wh_data_current \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\",  current_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"f_wh_data_last_norm = f_wh_data_last \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", last_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"\r\n",
							"# UNION wszystkich DataFrame'w\r\n",
							"\r\n",
							"f_forecast_current_norm\r\n",
							"f_forecast_last_norm\r\n",
							"f_sales_current_norm \r\n",
							"f_sales_last_norm\r\n",
							"f_pos_data_current_norm\r\n",
							"f_pos_data_last_norm\r\n",
							"f_wh_data_current_norm\r\n",
							"f_wh_data_last_norm\r\n",
							"\r\n",
							"final_df = f_forecast_current_norm.unionByName(f_forecast_last_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_sales_current_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_sales_last_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_pos_data_current_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_pos_data_last_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_wh_data_current_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_wh_data_last_norm, allowMissingColumns=True) \r\n",
							"\r\n",
							"\r\n",
							"bucket_name_gold = \"gold\"\r\n",
							"\r\n",
							"# Load Data\r\n",
							"f_sales_path = f\"s3a://{bucket_name}/f_planning_book\"\r\n",
							"\r\n",
							"aggregated_df = final_df.groupBy(\r\n",
							"    \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\"\r\n",
							").agg(\r\n",
							"    # Suma dla kolumn liczbowych\r\n",
							"    F.sum(\"quantity\").alias(\"quantity\"),\r\n",
							"    F.sum(\"amount\").alias(\"amount\"),\r\n",
							"    F.avg(\"unit_price\").alias(\"unit_price\"),\r\n",
							"    \r\n",
							"    # Suma dla innych kolumn\r\n",
							"    F.sum(\"whrs_sell_in_quantity\").alias(\"whrs_sell_in_quantity\"),\r\n",
							"    F.sum(\"whrs_open_quantity\").alias(\"whrs_open_quantity\"),\r\n",
							"    F.sum(\"whrs_end_quantity\").alias(\"whrs_end_quantity\"),\r\n",
							"    F.sum(\"whrs_sell_in_amount\").alias(\"whrs_sell_in_amount\"),\r\n",
							"    F.sum(\"whrs_open_amount\").alias(\"whrs_open_amount\"),\r\n",
							"    F.sum(\"whrs_end_amount\").alias(\"whrs_end_amount\"),\r\n",
							"    \r\n",
							"    F.sum(\"pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"    F.sum(\"pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"    F.sum(\"pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"    F.sum(\"pos_sell_out_amount\").alias(\"pos_sell_out_amount\"),\r\n",
							"    F.sum(\"pos_open_amount\").alias(\"pos_open_amount\"),\r\n",
							"    F.sum(\"pos_end_amount\").alias(\"pos_end_amount\"),\r\n",
							"    \r\n",
							"    # Agregacja dla discount (rednia)\r\n",
							"    F.sum(\"discount\").alias(\"discount\")\r\n",
							")\r\n",
							"\r\n",
							"\r\n",
							"aggregated_df.write.mode(\"overwrite\").parquet(f_sales_path_gold)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"import pandas as pd\r\n",
							"\r\n",
							"# Zaaduj dane\r\n",
							"df_pandas = aggregated_df.toPandas()\r\n",
							"\r\n",
							"df_pandas = df_pandas.drop(columns=['discount', 'unit_price'])\r\n",
							"\r\n",
							"# Unpivotowanie (melt)\r\n",
							"df_melted = df_pandas.melt(\r\n",
							"   id_vars=['product_id', 'country_id', 'financial_date', 'forecast_snapshot'],\r\n",
							"    var_name=\"original_column\",\r\n",
							"    value_name=\"value\"\r\n",
							")\r\n",
							"\r\n",
							"\r\n",
							"# Tworzenie nowej kolumny 'measure' na podstawie obecnoci 'quantity' lub 'amount' w nazwie kolumny\r\n",
							"df_melted[\"Measure\"] = df_melted[\"original_column\"].apply(lambda x: \"EA\" if \"quantity\" in x.lower() else \"GTS\" if \"amount\" in x.lower() else \"unknown\")\r\n",
							"\r\n",
							"# Mapowanie nazw kolumn na nowe formaty\r\n",
							"measure_mapping = {\r\n",
							"    'quantity': 'Ex-Factory',\r\n",
							"    'amount': 'Ex-Factory',\r\n",
							"    'whrs_sell_in_quantity': 'Sales to pharmacies',\r\n",
							"    'whrs_open_quantity': 'Open Stock',\r\n",
							"    'whrs_end_quantity': 'Close Stock',\r\n",
							"    'whrs_sell_in_amount': 'Sales to pharmacies',\r\n",
							"    'whrs_open_amount': 'Open Stock',\r\n",
							"    'whrs_end_amount': 'Close Stock',\r\n",
							"    'pos_sell_out_quantity': 'Sales to pharmacies',\r\n",
							"    'pos_open_quantity': 'Open Stock Pharmacies',\r\n",
							"    'pos_end_quantity': 'Close Stock Pharmacies',\r\n",
							"    'pos_sell_out_amount': 'Sales to pharmacies',\r\n",
							"    'pos_open_amount': 'Open Stock Pharmacies',\r\n",
							"    'pos_end_amount': 'Close Stock Pharmacies'\r\n",
							"}\r\n",
							"\r\n",
							"# Zastosowanie mapowania\r\n",
							"df_melted[\"original_column\"] = df_melted[\"original_column\"].replace(measure_mapping)\r\n",
							"\r\n",
							"# Usuwamy wiersze z brakujcymi wartociami\r\n",
							"df_melted = df_melted.dropna(subset=[\"original_column\", \"value\"])\r\n",
							"\r\n",
							"# Pivotowanie danych\r\n",
							"df_pivoted = df_melted.pivot_table(\r\n",
							"    index=['product_id', 'country_id', 'financial_date', 'forecast_snapshot', 'Measure'],  \r\n",
							"    columns='original_column',\r\n",
							"    values='value',\r\n",
							"    aggfunc='first'  # lub 'sum', jeli chcesz sumowa wartoci\r\n",
							").reset_index()\r\n",
							"\r\n",
							"# Usunicie nazwy kolumny po pivotowaniu\r\n",
							"df_pivoted.columns.name = None\r\n",
							"\r\n",
							"\r\n",
							"# Znalezienie maksymalnej daty\r\n",
							"max_forecast_snapshot = df_pivoted[\"forecast_snapshot\"].max()\r\n",
							"\r\n",
							"# Dodanie kolumny status\r\n",
							"df_pivoted[\"Version\"] =  df_pivoted[\"forecast_snapshot\"].apply(lambda x: \"Current\" if x == max_forecast_snapshot  else \"Last\")\r\n",
							"\r\n",
							"bucket_name_gold = \"abfss://gold@bigpharma.dfs.core.windows.net/f_planning_book_unpivot\"\r\n",
							"\r\n",
							"# cieka do warstwy Gold (f_planning_book)\r\n",
							"f_sales_path_gold = f\"{bucket_name_gold}/f_planning_book_unpivot\"\r\n",
							"\r\n",
							"df_spark = spark.createDataFrame(df_pivoted)\r\n",
							"\r\n",
							"df_spark=df_spark.withColumnRenamed('product_id', 'IdProduct').withColumnRenamed('country_id', 'IDCountry').withColumnRenamed('financial_date', 'Date').withColumnRenamed('Close Stock', 'Close_Stock').withColumnRenamed('Ex-Factory', 'Ex_Factory').withColumnRenamed('Open Stock', 'Open_Stock').withColumnRenamed('Open Stock Pharmacies', 'Open_Stock_Pharmacies').withColumnRenamed('Sales to pharmacies', 'Sales_to_pharmacies').withColumnRenamed('Close Stock Pharmacies', 'Close_Stock_Pharmacies')\r\n",
							"\r\n",
							"\r\n",
							"df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://gold@bigpharma.dfs.core.windows.net/f_planning_book_unpivot\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"\r\n",
							"spark.sql(\"\"\"\r\n",
							"CREATE TABLE IF NOT EXISTS gold.f_planning_book_unpivot\r\n",
							"USING DELTA\r\n",
							"LOCATION 'abfss://gold@bigpharma.dfs.core.windows.net/f_planning_book_unpivot'\r\n",
							"\"\"\")\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc, lit, add_months\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql.functions import to_date\r\n",
							"\r\n",
							"# Create Spark session\r\n",
							"spark = SparkSession.builder.appName(\"Load Planning Book\").getOrCreate()\r\n",
							"\r\n",
							"# Parameters for the Silver layer (ADLS Gen2)\r\n",
							"bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"bucket_name_gold = \"abfss://gold@bigpharma.dfs.core.windows.net/\"\r\n",
							"\r\n",
							"# Paths to Parquet files in ADLS Gen2\r\n",
							"f_sales_path = f\"{bucket_name}/f_sales\"\r\n",
							"f_forecast_path = f\"{bucket_name}/f_forecast\"\r\n",
							"f_pos_data_path = f\"{bucket_name}/f_pos_data\"\r\n",
							"f_wh_data_path = f\"{bucket_name}/f_wh_data\"\r\n",
							"\r\n",
							"# Path to the Gold layer (f_planning_book)\r\n",
							"f_sales_path_gold = f\"{bucket_name_gold}/f_planning_book\"\r\n",
							"\r\n",
							"# Load data from ADLS (tables f_sales, f_forecast, f_pos_data, f_wh_data)\r\n",
							"f_sales = spark.read.parquet(f_sales_path)\r\n",
							"f_forecast = spark.read.parquet(f_forecast_path)\r\n",
							"f_pos_data = spark.read.parquet(f_pos_data_path)\r\n",
							"f_wh_data = spark.read.parquet(f_wh_data_path)\r\n",
							"\r\n",
							"# Get the maximum value of forecast_snapshot from f_forecast\r\n",
							"current_month_snapshot = f_forecast.agg(F.max(\"forecast_snapshot\")).collect()[0][0]\r\n",
							"current_month_snapshot = to_date(lit(current_month_snapshot), \"yyyy-MM-dd\")\r\n",
							"last_month_snapshot = F.add_months(F.lit(current_month_snapshot), -1)\r\n",
							"\r\n",
							"f_forecast_filtered = f_forecast.filter(F.col(\"forecast_snapshot\").isin(current_month_snapshot, last_month_snapshot))\r\n",
							"\r\n",
							"# Filter for the current and last month\r\n",
							"f_forecast_current = f_forecast_filtered.filter((F.col(\"forecast_date\") >= current_month_snapshot) & (F.col(\"forecast_snapshot\") == current_month_snapshot))\r\n",
							"f_forecast_last = f_forecast_filtered.filter((F.col(\"forecast_date\") >= last_month_snapshot) & (F.col(\"forecast_snapshot\") == last_month_snapshot))\r\n",
							"\r\n",
							"# Filter f_sales\r\n",
							"f_sales_current = f_sales.filter(F.col(\"shipped_date\") < current_month_snapshot)\r\n",
							"f_sales_last = f_sales.filter(F.col(\"shipped_date\") < last_month_snapshot)\r\n",
							"\r\n",
							"f_pos_data_current = f_pos_data.filter(F.col(\"transaction_date\") < current_month_snapshot)\r\n",
							"f_pos_data_last = f_pos_data.filter(F.col(\"transaction_date\") < last_month_snapshot)\r\n",
							"\r\n",
							"f_wh_data_current = f_wh_data.filter(F.col(\"transaction_date\") < current_month_snapshot)\r\n",
							"f_wh_data_last = f_wh_data.filter(F.col(\"transaction_date\") < last_month_snapshot)\r\n",
							"\r\n",
							"# Unified column list\r\n",
							"common_columns = [\r\n",
							"    \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"    \"quantity\", \"amount\", \"unit_price\",\r\n",
							"    \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"    \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"    \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"    \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"    \"discount\"\r\n",
							"]\r\n",
							"\r\n",
							"# Normalize f_forecast (keeps the original forecast_snapshot)\r\n",
							"f_forecast_current_norm = f_forecast_current \\\r\n",
							"    .withColumnRenamed(\"forecast_date\", \"financial_date\") \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0.0 as discount\"  # If discount is of type double\r\n",
							"    )\r\n",
							"\r\n",
							"f_forecast_last_norm = f_forecast_last \\\r\n",
							"    .withColumnRenamed(\"forecast_date\", \"financial_date\") \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0.0 as discount\"  # If discount is of type double\r\n",
							"    )\r\n",
							"\r\n",
							"# Normalize f_sales\r\n",
							"f_sales_current_norm = f_sales_current \\\r\n",
							"    .withColumnRenamed(\"shipped_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", current_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"f_sales_last_norm = f_sales_last \\\r\n",
							"    .withColumnRenamed(\"shipped_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", last_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"# Normalize f_pos_data_path\r\n",
							"f_pos_data_current_norm = f_pos_data_current \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", current_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"f_pos_data_last_norm = f_pos_data_last \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", last_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"# Normalize f_wh_data_path\r\n",
							"f_wh_data_current_norm = f_wh_data_current \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", current_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"f_wh_data_last_norm = f_wh_data_last \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", last_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"    # UNION of all DataFrames\r\n",
							"\r\n",
							"f_forecast_current_norm\r\n",
							"f_forecast_last_norm\r\n",
							"f_sales_current_norm \r\n",
							"f_sales_last_norm\r\n",
							"f_pos_data_current_norm\r\n",
							"f_pos_data_last_norm\r\n",
							"f_wh_data_current_norm\r\n",
							"f_wh_data_last_norm\r\n",
							"\r\n",
							"final_df = f_forecast_current_norm.unionByName(f_forecast_last_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_sales_current_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_sales_last_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_pos_data_current_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_pos_data_last_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_wh_data_current_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_wh_data_last_norm, allowMissingColumns=True) \r\n",
							"\r\n",
							"bucket_name_gold = \"gold\"\r\n",
							"\r\n",
							"# Load Data\r\n",
							"f_sales_path = f\"s3a://{bucket_name}/f_planning_book\"\r\n",
							"\r\n",
							"aggregated_df = final_df.groupBy(\r\n",
							"    \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\"\r\n",
							").agg(\r\n",
							"    # Sum for numerical columns\r\n",
							"    F.sum(\"quantity\").alias(\"quantity\"),\r\n",
							"    F.sum(\"amount\").alias(\"amount\"),\r\n",
							"    F.avg(\"unit_price\").alias(\"unit_price\"),\r\n",
							"    \r\n",
							"    # Sum for other columns\r\n",
							"    F.sum(\"whrs_sell_in_quantity\").alias(\"whrs_sell_in_quantity\"),\r\n",
							"    F.sum(\"whrs_open_quantity\").alias(\"whrs_open_quantity\"),\r\n",
							"    F.sum(\"whrs_end_quantity\").alias(\"whrs_end_quantity\"),\r\n",
							"    F.sum(\"whrs_sell_in_amount\").alias(\"whrs_sell_in_amount\"),\r\n",
							"    F.sum(\"whrs_open_amount\").alias(\"whrs_open_amount\"),\r\n",
							"    F.sum(\"whrs_end_amount\").alias(\"whrs_end_amount\"),\r\n",
							"    \r\n",
							"    F.sum(\"pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"    F.sum(\"pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"    F.sum(\"pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"    F.sum(\"pos_sell_out_amount\").alias(\"pos_sell_out_amount\"),\r\n",
							"    F.sum(\"pos_open_amount\").alias(\"pos_open_amount\"),\r\n",
							"    F.sum(\"pos_end_amount\").alias(\"pos_end_amount\"),\r\n",
							"    \r\n",
							"    # Aggregation for discount (average)\r\n",
							"    F.sum(\"discount\").alias(\"discount\")\r\n",
							")\r\n",
							"\r\n",
							"aggregated_df.write.mode(\"overwrite\").parquet(f_sales_path_gold)\r\n",
							"\r\n",
							"import pandas as pd\r\n",
							"\r\n",
							"# Load data\r\n",
							"df_pandas = aggregated_df.toPandas()\r\n",
							"\r\n",
							"df_pandas = df_pandas.drop(columns=['discount', 'unit_price'])\r\n",
							"\r\n",
							"# Unpivoting (melt)\r\n",
							"df_melted = df_pandas.melt(\r\n",
							"   id_vars=['product_id', 'country_id', 'financial_date', 'forecast_snapshot'],\r\n",
							"    var_name=\"original_column\",\r\n",
							"    value_name=\"value\"\r\n",
							")\r\n",
							"\r\n",
							"# Create a new 'measure' column based on the presence of 'quantity' or 'amount' in the column name\r\n",
							"df_melted[\"Measure\"] = df_melted[\"original_column\"].apply(\r\n",
							"    lambda x: \"EA\" if \"quantity\" in x.lower() else \"GTS\" if \"amount\" in x.lower() else \"unknown\"\r\n",
							")\r\n",
							"\r\n",
							"# Mapping column names to new formats\r\n",
							"measure_mapping = {\r\n",
							"    'quantity': 'Ex-Factory',\r\n",
							"    'amount': 'Ex-Factory',\r\n",
							"    'whrs_sell_in_quantity': 'Sales to pharmacies',\r\n",
							"    'whrs_open_quantity': 'Open Stock',\r\n",
							"    'whrs_end_quantity': 'Close Stock',\r\n",
							"    'whrs_sell_in_amount': 'Sales to pharmacies',\r\n",
							"    'whrs_open_amount': 'Open Stock',\r\n",
							"    'whrs_end_amount': 'Close Stock',\r\n",
							"    'pos_sell_out_quantity': 'Sales to pharmacies',\r\n",
							"    'pos_open_quantity': 'Open Stock Pharmacies',\r\n",
							"    'pos_end_quantity': 'Close Stock Pharmacies',\r\n",
							"    'pos_sell_out_amount': 'Sales to pharmacies',\r\n",
							"    'pos_open_amount': 'Open Stock Pharmacies',\r\n",
							"    'pos_end_amount': 'Close Stock Pharmacies'\r\n",
							"}\r\n",
							"\r\n",
							"# Apply the mapping\r\n",
							"df_melted[\"original_column\"] = df_melted[\"original_column\"].replace(measure_mapping)\r\n",
							"\r\n",
							"# Remove rows with missing values\r\n",
							"df_melted = df_melted.dropna(subset=[\"original_column\", \"value\"])\r\n",
							"\r\n",
							"# Pivoting data\r\n",
							"df_pivoted = df_melted.pivot_table(\r\n",
							"    index=['product_id', 'country_id', 'financial_date', 'forecast_snapshot', 'Measure'],  \r\n",
							"    columns='original_column',\r\n",
							"    values='value',\r\n",
							"    aggfunc='first'  # or 'sum', if you want to sum values\r\n",
							").reset_index()\r\n",
							"\r\n",
							"# Remove the column name after pivoting\r\n",
							"df_pivoted.columns.name = None\r\n",
							"\r\n",
							"# Find the maximum date\r\n",
							"max_forecast_snapshot = df_pivoted[\"forecast_snapshot\"].max()\r\n",
							"\r\n",
							"# Add a status column\r\n",
							"df_pivoted[\"Version\"] =  df_pivoted[\"forecast_snapshot\"].apply(\r\n",
							"    lambda x: \"Current\" if x == max_forecast_snapshot else \"Last\"\r\n",
							")\r\n",
							"\r\n",
							"bucket_name_gold = \"abfss://gold@bigpharma.dfs.core.windows.net/f_planning_book_unpivot\"\r\n",
							"\r\n",
							"# Path to the Gold layer (f_planning_book)\r\n",
							"f_sales_path_gold = f\"{bucket_name_gold}/f_planning_book_unpivot\"\r\n",
							"\r\n",
							"df_spark = spark.createDataFrame(df_pivoted)\r\n",
							"\r\n",
							"df_spark = df_spark.withColumnRenamed('product_id', 'IdProduct') \\\r\n",
							"    .withColumnRenamed('country_id', 'IDCountry') \\\r\n",
							"    .withColumnRenamed('financial_date', 'Date') \\\r\n",
							"    .withColumnRenamed('Close Stock', 'Close_Stock') \\\r\n",
							"    .withColumnRenamed('Ex-Factory', 'Ex_Factory') \\\r\n",
							"    .withColumnRenamed('Open Stock', 'Open_Stock') \\\r\n",
							"    .withColumnRenamed('Open Stock Pharmacies', 'Open_Stock_Pharmacies') \\\r\n",
							"    .withColumnRenamed('Sales to pharmacies', 'Sales_to_pharmacies') \\\r\n",
							"    .withColumnRenamed('Close Stock Pharmacies', 'Close_Stock_Pharmacies')\r\n",
							"\r\n",
							"df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://gold@bigpharma.dfs.core.windows.net/f_planning_book_unpivot\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_POS_LOAD_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4398870e-8606-47eb-99a5-cdb452c6d027"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"## 1. Load data from Bronze and Customers as temporary views\n",
							"df_bronze = spark.read.option(\"header\", \"true\").csv(\n",
							"    \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/*/*/*.csv\"\n",
							")\n",
							"df_bronze.createOrReplaceTempView(\"bronze_pos\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- ============================================\n",
							"-- 2. Prepare data  latest update + monthly aggregation\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_bronze_pos AS\n",
							"SELECT \n",
							"    product_id,\n",
							"    country_id,\n",
							"    max(update_date) as pos_update_date,    \n",
							"    TRUNC(transaction_date, \"MM\") AS transaction_date,\n",
							"    SUM(pos_sell_out_quantity) AS pos_sell_out_quantity,\n",
							"    SUM(pos_open_quantity) AS pos_open_quantity,\n",
							"    SUM(pos_end_quantity) AS pos_end_quantity,\n",
							"    ROUND(SUM(pos_sell_out_quantity * unit_price), 2) AS pos_sell_out_amount,\n",
							"    ROUND(SUM(pos_open_quantity * unit_price), 2) AS pos_open_amount,\n",
							"    ROUND(SUM(pos_end_quantity * unit_price), 2) AS pos_end_amount,\n",
							"    ROUND(SUM(pos_sell_out_quantity * unit_price) / NULLIF(SUM(pos_sell_out_quantity), 0), 2) AS unit_price\n",
							"FROM  bronze_pos\n",
							"GROUP BY product_id, country_id, TRUNC(transaction_date, \"MM\");\n",
							"\n",
							"-- ============================================\n",
							"-- 3. Create the target table (Silver) if it does not exist\n",
							"-- ============================================\n",
							"CREATE TABLE IF NOT EXISTS silver.f_pos_data\n",
							"USING delta\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/f_pos_data'\n",
							"AS\n",
							"SELECT * FROM aggregated_bronze_pos\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- Filter new or updated records\n",
							"CREATE OR REPLACE TEMP VIEW filter_aggregated_bronze_pos AS\n",
							"SELECT * from aggregated_bronze_pos n \n",
							"where NOT EXISTS (\n",
							"  SELECT product_id FROM silver.f_pos_data s \n",
							"  WHERE s.product_id = n.product_id \n",
							"    AND s.country_id = n.country_id\n",
							"    AND s.pos_update_date = n.pos_update_date\n",
							"    AND s.transaction_date = n.transaction_date\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 4. MERGE INTO (UPSERT) data into the Delta table\n",
							"-- ============================================\n",
							"\n",
							"MERGE INTO silver.f_pos_data AS target\n",
							"USING filter_aggregated_bronze_pos AS source\n",
							"ON target.product_id = source.product_id\n",
							"   AND target.country_id = source.country_id\n",
							"   AND target.transaction_date = source.transaction_date\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.pos_sell_out_quantity = source.pos_sell_out_quantity,\n",
							"    target.pos_open_quantity = source.pos_open_quantity,\n",
							"    target.pos_end_quantity = source.pos_end_quantity,\n",
							"    target.pos_sell_out_amount = source.pos_sell_out_amount,\n",
							"    target.pos_open_amount = source.pos_open_amount,\n",
							"    target.pos_end_amount = source.pos_end_amount,\n",
							"    target.unit_price = source.unit_price,\n",
							"    target.pos_update_date = source.pos_update_date \n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id,\n",
							"    country_id,\n",
							"    transaction_date,\n",
							"    pos_sell_out_quantity,\n",
							"    pos_open_quantity,\n",
							"    pos_end_quantity,\n",
							"    pos_sell_out_amount,\n",
							"    pos_open_amount,\n",
							"    pos_end_amount,\n",
							"    unit_price,\n",
							"    pos_update_date \n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id,\n",
							"    source.country_id,\n",
							"    source.transaction_date,\n",
							"    source.pos_sell_out_quantity,\n",
							"    source.pos_open_quantity,\n",
							"    source.pos_end_quantity,\n",
							"    source.pos_sell_out_amount,\n",
							"    source.pos_open_amount,\n",
							"    source.pos_end_amount,\n",
							"    source.unit_price,\n",
							"    source.pos_update_date \n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_POS_LOAD_SQL_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9852f239-fff5-44ed-a66a-c40c6a5ac280"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"## 1. Zaaduj dane z Bronze i Customers jako tymczasowe widoki\n",
							"df_bronze = spark.read.option(\"header\", \"true\").csv(\n",
							"    \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/*/*/*.csv\"\n",
							")\n",
							"df_bronze.createOrReplaceTempView(\"bronze_pos\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- ============================================\n",
							"-- 2. Przygotowanie danych  najnowsze update + agregacja miesiczna\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_bronze_pos AS\n",
							"SELECT \n",
							"    product_id,\n",
							"    country_id,\n",
							"    max(update_date) as pos_update_date,    \n",
							"    TRUNC(transaction_date, \"MM\") AS transaction_date,\n",
							"    SUM(pos_sell_out_quantity) AS pos_sell_out_quantity,\n",
							"    SUM(pos_open_quantity) AS pos_open_quantity,\n",
							"    SUM(pos_end_quantity) AS pos_end_quantity,\n",
							"    ROUND(SUM(pos_sell_out_quantity * unit_price), 2) AS pos_sell_out_amount,\n",
							"    ROUND(SUM(pos_open_quantity * unit_price), 2) AS pos_open_amount,\n",
							"    ROUND(SUM(pos_end_quantity * unit_price), 2) AS pos_end_amount,\n",
							"    ROUND(SUM(pos_sell_out_quantity * unit_price) / NULLIF(SUM(pos_sell_out_quantity), 0), 2) AS unit_price\n",
							"FROM  bronze_pos\n",
							"GROUP BY product_id, country_id, TRUNC(transaction_date, \"MM\");\n",
							"\n",
							"\n",
							"-- ============================================\n",
							"-- 3. Stworzenie tabeli docelowej (Silver) jeli nie istnieje\n",
							"-- ============================================\n",
							"CREATE TABLE IF NOT EXISTS silver.f_pos_data\n",
							"USING delta\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/f_pos_data'\n",
							"AS\n",
							"SELECT * FROM aggregated_bronze_pos\n",
							"WHERE 1 = 0;\n",
							"\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW filter_aggregated_bronze_pos AS\n",
							"SELECT * from aggregated_bronze_pos n \n",
							"where  NOT EXISTS (select product_id From silver.f_pos_data s \n",
							"                    where s.product_id =n.product_id \n",
							"                      and s.country_id = n.country_id\n",
							"                      and s.pos_update_date = n.pos_update_date\n",
							"                      and s.transaction_date = n.transaction_date);\n",
							"\n",
							"\n",
							"\n",
							"-- ============================================\n",
							"-- 4. MERGE INTO (UPSERT) danych do tabeli Delta\n",
							"-- ============================================\n",
							"\n",
							"MERGE INTO silver.f_pos_data AS target\n",
							"USING filter_aggregated_bronze_pos AS source\n",
							"ON target.product_id = source.product_id\n",
							"   AND target.country_id = source.country_id\n",
							"   AND target.transaction_date = source.transaction_date\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.pos_sell_out_quantity = source.pos_sell_out_quantity,\n",
							"    target.pos_open_quantity = source.pos_open_quantity,\n",
							"    target.pos_end_quantity = source.pos_end_quantity,\n",
							"    target.pos_sell_out_amount = source.pos_sell_out_amount,\n",
							"    target.pos_open_amount = source.pos_open_amount,\n",
							"    target.pos_end_amount = source.pos_end_amount,\n",
							"    target.unit_price = source.unit_price,\n",
							"    target.pos_update_date = source.pos_update_date \n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id,\n",
							"    country_id,\n",
							"    transaction_date,\n",
							"    pos_sell_out_quantity,\n",
							"    pos_open_quantity,\n",
							"    pos_end_quantity,\n",
							"    pos_sell_out_amount,\n",
							"    pos_open_amount,\n",
							"    pos_end_amount,\n",
							"    unit_price,\n",
							"    pos_update_date \n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id,\n",
							"    source.country_id,\n",
							"    source.transaction_date,\n",
							"    source.pos_sell_out_quantity,\n",
							"    source.pos_open_quantity,\n",
							"    source.pos_end_quantity,\n",
							"    source.pos_sell_out_amount,\n",
							"    source.pos_open_amount,\n",
							"    source.pos_end_amount,\n",
							"    source.unit_price,\n",
							"    source.pos_update_date \n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"## 1. Load data from Bronze and Customers as temporary views\n",
							"df_bronze = spark.read.option(\"header\", \"true\").csv(\n",
							"    \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/*/*/*.csv\"\n",
							")\n",
							"df_bronze.createOrReplaceTempView(\"bronze_pos\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- ============================================\n",
							"-- 2. Prepare data  latest update + monthly aggregation\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_bronze_pos AS\n",
							"SELECT \n",
							"    product_id,\n",
							"    country_id,\n",
							"    max(update_date) as pos_update_date,    \n",
							"    TRUNC(transaction_date, \"MM\") AS transaction_date,\n",
							"    SUM(pos_sell_out_quantity) AS pos_sell_out_quantity,\n",
							"    SUM(pos_open_quantity) AS pos_open_quantity,\n",
							"    SUM(pos_end_quantity) AS pos_end_quantity,\n",
							"    ROUND(SUM(pos_sell_out_quantity * unit_price), 2) AS pos_sell_out_amount,\n",
							"    ROUND(SUM(pos_open_quantity * unit_price), 2) AS pos_open_amount,\n",
							"    ROUND(SUM(pos_end_quantity * unit_price), 2) AS pos_end_amount,\n",
							"    ROUND(SUM(pos_sell_out_quantity * unit_price) / NULLIF(SUM(pos_sell_out_quantity), 0), 2) AS unit_price\n",
							"FROM  bronze_pos\n",
							"GROUP BY product_id, country_id, TRUNC(transaction_date, \"MM\");\n",
							"\n",
							"-- ============================================\n",
							"-- 3. Create the target table (Silver) if it does not exist\n",
							"-- ============================================\n",
							"CREATE TABLE IF NOT EXISTS silver.f_pos_data\n",
							"USING delta\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/f_pos_data'\n",
							"AS\n",
							"SELECT * FROM aggregated_bronze_pos\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- Filter new or updated records\n",
							"CREATE OR REPLACE TEMP VIEW filter_aggregated_bronze_pos AS\n",
							"SELECT * from aggregated_bronze_pos n \n",
							"where NOT EXISTS (\n",
							"  SELECT product_id FROM silver.f_pos_data s \n",
							"  WHERE s.product_id = n.product_id \n",
							"    AND s.country_id = n.country_id\n",
							"    AND s.pos_update_date = n.pos_update_date\n",
							"    AND s.transaction_date = n.transaction_date\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 4. MERGE INTO (UPSERT) data into the Delta table\n",
							"-- ============================================\n",
							"\n",
							"MERGE INTO silver.f_pos_data AS target\n",
							"USING filter_aggregated_bronze_pos AS source\n",
							"ON target.product_id = source.product_id\n",
							"   AND target.country_id = source.country_id\n",
							"   AND target.transaction_date = source.transaction_date\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.pos_sell_out_quantity = source.pos_sell_out_quantity,\n",
							"    target.pos_open_quantity = source.pos_open_quantity,\n",
							"    target.pos_end_quantity = source.pos_end_quantity,\n",
							"    target.pos_sell_out_amount = source.pos_sell_out_amount,\n",
							"    target.pos_open_amount = source.pos_open_amount,\n",
							"    target.pos_end_amount = source.pos_end_amount,\n",
							"    target.unit_price = source.unit_price,\n",
							"    target.pos_update_date = source.pos_update_date \n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id,\n",
							"    country_id,\n",
							"    transaction_date,\n",
							"    pos_sell_out_quantity,\n",
							"    pos_open_quantity,\n",
							"    pos_end_quantity,\n",
							"    pos_sell_out_amount,\n",
							"    pos_open_amount,\n",
							"    pos_end_amount,\n",
							"    unit_price,\n",
							"    pos_update_date \n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id,\n",
							"    source.country_id,\n",
							"    source.transaction_date,\n",
							"    source.pos_sell_out_quantity,\n",
							"    source.pos_open_quantity,\n",
							"    source.pos_end_quantity,\n",
							"    source.pos_sell_out_amount,\n",
							"    source.pos_open_amount,\n",
							"    source.pos_end_amount,\n",
							"    source.unit_price,\n",
							"    source.pos_update_date \n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_SALES_LOAD_PY')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive/Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "63357187-1908-4994-baf9-53ab7cdf78c5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"\r\n",
							"#dla warstwy Bronze (ADLS Gen2)\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"\r\n",
							"orders_path = f\"{bronze_bucket_name}{bronze_prefix}orders\"\r\n",
							"order_details_path = f\"{bronze_bucket_name}{bronze_prefix}order_details\"\r\n",
							"\r\n",
							"# Definicja cieek dla warstwy Silver (ADLS Gen2)\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"f_sales/\"\r\n",
							"\r\n",
							"silver_path = f\"{silver_bucket_name}{silver_prefix}\"\r\n",
							"silver_customers = f\"{silver_bucket_name}d_customers\"\r\n",
							"\r\n",
							"# Zaaduj dane z ADLS (tabele orders, order_details, customers)\r\n",
							"orders_df = spark.read.parquet(orders_path)\r\n",
							"order_details_df = spark.read.parquet(order_details_path)\r\n",
							"customers_df = spark.read.parquet(silver_customers)\r\n",
							"\r\n",
							"# Wywietlenie pierwszych wierszy dla weryfikacji\r\n",
							"orders_df.show(5)\r\n",
							"order_details_df.show(5)\r\n",
							"customers_df.show(5)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"orders_df = orders_df.withColumn(\r\n",
							"    \"order_date\", trunc(\"order_date\",\"month\")\r\n",
							").withColumn(\r\n",
							"    \"required_date\", trunc(\"required_date\",\"month\")\r\n",
							").withColumn(\r\n",
							"    \"shipped_date\", trunc(\"shipped_date\",\"month\")\r\n",
							")\r\n",
							"\r\n",
							"\r\n",
							"joined_df = orders_df.join(order_details_df, \"order_id\").join(customers_df,\"customer_id\")\r\n",
							"\r\n",
							"# Uycie okna do filtrowania najnowszych zamwie\r\n",
							"latest_order_df = joined_df.withColumn(\r\n",
							"    \"max_order_date\", F.max(\"order_date\").over(Window.partitionBy(\"order_id\"))\r\n",
							").filter(F.col(\"order_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"# Wybr odpowiednich kolumn do zapisania w warstwie Silver\r\n",
							"new_order_details_df = latest_order_df.select(\r\n",
							"    \"country_id\",\r\n",
							"    \"product_id\",  \r\n",
							"    \"shipped_date\", \r\n",
							"    \"unit_price\", \r\n",
							"    \"quantity\", \r\n",
							"    \"discount\"\r\n",
							")\r\n",
							"\r\n",
							"\r\n",
							"aggregated_df = new_order_details_df.groupBy(\"country_id\", \"product_id\", \"shipped_date\").agg(\r\n",
							"                                     F.sum(\"quantity\").alias(\"quantity\"),\r\n",
							"                                     F.sum(\"discount\").alias(\"discount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"quantity\") * F.col(\"unit_price\")),2).alias(\"amount\"),\r\n",
							"                                     F.round((F.sum(F.col(\"quantity\") * F.col(\"unit_price\")) / F.sum(\"quantity\")),2).alias(\"unit_price\"))\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver ju istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"\r\n",
							"\r\n",
							"# Jeli tabela Silver istnieje, wykonaj operacj \"MERGE\" na podstawie DataFrame\r\n",
							"if silver_exists:\r\n",
							"    # Zaaduj dane z tabeli Silver\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    # Pocz dane Silver z nowymi danymi (na podstawie order_id, customer_id, product_id)\r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        aggregated_df.alias(\"new_orders\"),\r\n",
							"        (F.col(\"silver.shipped_date\") == F.col(\"new_orders.shipped_date\")) & \r\n",
							"        (F.col(\"silver.country_id\") == F.col(\"new_orders.country_id\")) & \r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_orders.product_id\")),\r\n",
							"        how=\"outer\"\r\n",
							"    )\r\n",
							"\r\n",
							"    # Wybr kolumn, ktre maj zosta zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_orders.country_id\", \"silver.country_id\").alias(\"country_id\"),\r\n",
							"        F.coalesce(\"new_orders.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_orders.shipped_date\", \"silver.shipped_date\").alias(\"shipped_date\"), \r\n",
							"        F.coalesce(\"new_orders.quantity\", \"silver.quantity\").alias(\"quantity\"),\r\n",
							"        F.coalesce(\"new_orders.discount\", \"silver.discount\").alias(\"discount\"),\r\n",
							"        F.coalesce(\"new_orders.amount\", \"silver.amount\").alias(\"amount\"),\r\n",
							"       F.coalesce(\"new_orders.unit_price\", \"silver.unit_price\").alias(\"unit_price\")\r\n",
							"    )\r\n",
							"\r\n",
							"    # Zapisanie zaktualizowanego DataFrame do warstwy Silver\r\n",
							"\r\n",
							"    final_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"else:\r\n",
							"\r\n",
							"    # Zapisanie zaktualizowanego DataFrame do warstwy Silver\r\n",
							"\r\n",
							"    aggregated_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakoczenie\r\n",
							"print(\"Inkrementalne adowanie zakoczone!\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_SALES_LOAD_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "93155bff-ba01-40f8-98df-6113ebf0084b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- 1. Temporary views on files from ADLS Gen2\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW bronze_orders\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/orders'\n",
							");\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW bronze_order_details\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/order_details'\n",
							");\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW d_customers\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers'\n",
							");\n",
							"\n",
							"-- 2. Create target table (if not exists)\n",
							"CREATE TABLE IF NOT EXISTS silver.f_sales (\n",
							"    country_id STRING,\n",
							"    product_id STRING,\n",
							"    shipped_date DATE,\n",
							"    quantity DOUBLE,\n",
							"    discount DOUBLE,\n",
							"    amount DOUBLE,\n",
							"    unit_price DOUBLE,\n",
							"    order_details_update_date DATE,\n",
							"    order_update_date DATE\n",
							")\n",
							"USING DELTA\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/f_sales/';\n",
							"\n",
							"-- 2. Prepare data  latest update + monthly-level aggregation\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_bronze_orders AS\n",
							"WITH orders_trunc AS (\n",
							"    SELECT\n",
							"        order_id,\n",
							"        customer_id,\n",
							"        TRUNC(order_date, 'MM') AS order_date,\n",
							"        TRUNC(required_date, 'MM') AS required_date,\n",
							"        TRUNC(shipped_date, 'MM') AS shipped_date,\n",
							"        update_date AS order_update_date\n",
							"    FROM bronze_orders\n",
							"),\n",
							"joined_orders AS (\n",
							"    SELECT\n",
							"        o.order_id,\n",
							"        c.country_id,\n",
							"        od.product_id,\n",
							"        o.shipped_date,\n",
							"        od.unit_price,\n",
							"        od.quantity,\n",
							"        od.discount,\n",
							"        o.order_date,\n",
							"        od.update_date AS order_details_update_date,\n",
							"        o.order_update_date\n",
							"    FROM orders_trunc o\n",
							"    JOIN bronze_order_details od ON o.order_id = od.order_id\n",
							"    JOIN d_customers c ON o.customer_id = c.customer_id\n",
							"),\n",
							"final_orders AS (\n",
							"    SELECT\n",
							"        country_id,\n",
							"        product_id,\n",
							"        shipped_date,\n",
							"        MAX(order_details_update_date) AS order_details_update_date,\n",
							"        MAX(order_update_date) AS order_update_date,\n",
							"        SUM(quantity) AS quantity,\n",
							"        SUM(discount) AS discount,\n",
							"        ROUND(SUM(quantity * unit_price), 2) AS amount,\n",
							"        ROUND(SUM(quantity * unit_price) / NULLIF(SUM(quantity), 0), 2) AS unit_price\n",
							"    FROM joined_orders\n",
							"    GROUP BY country_id, product_id, shipped_date\n",
							")\n",
							"SELECT * FROM final_orders;\n",
							"\n",
							"-- Filter new or updated records\n",
							"CREATE OR REPLACE TEMP VIEW filter_aggregated_bronze_orders AS\n",
							"SELECT * FROM aggregated_bronze_orders n \n",
							"WHERE NOT EXISTS (\n",
							"  SELECT product_id FROM silver.f_sales s \n",
							"  WHERE s.product_id = n.product_id \n",
							"    AND s.shipped_date = n.shipped_date\n",
							"    AND s.order_details_update_date = n.order_details_update_date\n",
							"    AND s.order_update_date = n.order_update_date\n",
							");\n",
							"\n",
							"-- 3. Transform and write data to Silver table using MERGE\n",
							"MERGE INTO silver.f_sales AS target\n",
							"USING filter_aggregated_bronze_orders AS source\n",
							"ON target.country_id = source.country_id\n",
							"   AND target.product_id = source.product_id\n",
							"   AND target.shipped_date = source.shipped_date\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    quantity = source.quantity,\n",
							"    discount = source.discount,\n",
							"    amount = source.amount,\n",
							"    unit_price = source.unit_price,\n",
							"    order_details_update_date = source.order_details_update_date,\n",
							"    order_update_date = source.order_update_date \n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    country_id, product_id, shipped_date,\n",
							"    quantity, discount, amount, unit_price, order_details_update_date, order_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.country_id, source.product_id, source.shipped_date,\n",
							"    source.quantity, source.discount, source.amount, source.unit_price, source.order_details_update_date, source.order_update_date\n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_SALES_LOAD_SQL_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1f595d20-34f5-46f5-ba5a-b642db657275"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": true
							},
							"collapsed": false
						},
						"source": [
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"-- 1. Tymczasowe widoki na pliki z ADLS Gen2\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW bronze_orders\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/orders'\n",
							");\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW bronze_order_details\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/order_details'\n",
							");\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW d_customers\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers'\n",
							");\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS silver.f_sales (\n",
							"    country_id STRING,\n",
							"    product_id STRING,\n",
							"    shipped_date DATE,\n",
							"    quantity DOUBLE,\n",
							"    discount DOUBLE,\n",
							"    amount DOUBLE,\n",
							"    unit_price DOUBLE,\n",
							"    order_details_update_date DATE,\n",
							"    order_update_date DATE\n",
							"\n",
							")\n",
							"USING DELTA\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/f_sales/';\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_bronze_orders AS\n",
							"WITH orders_trunc AS (\n",
							"    SELECT\n",
							"        order_id,\n",
							"        customer_id,\n",
							"        TRUNC(order_date, 'MM') AS order_date,\n",
							"        TRUNC(required_date, 'MM') AS required_date,\n",
							"        TRUNC(shipped_date, 'MM') AS shipped_date,\n",
							"        update_date AS order_update_date\n",
							"    FROM bronze_orders\n",
							"),\n",
							"joined_orders AS (\n",
							"    SELECT\n",
							"        o.order_id,\n",
							"        c.country_id,\n",
							"        od.product_id,\n",
							"        o.shipped_date,\n",
							"        od.unit_price,\n",
							"        od.quantity,\n",
							"        od.discount,\n",
							"        o.order_date,\n",
							"        od.update_date AS order_details_update_date,\n",
							"        o.order_update_date\n",
							"    FROM orders_trunc o\n",
							"    JOIN bronze_order_details od ON o.order_id = od.order_id\n",
							"    JOIN d_customers c ON o.customer_id = c.customer_id\n",
							"),\n",
							"final_orders AS (\n",
							"    SELECT\n",
							"        country_id,\n",
							"        product_id,\n",
							"        shipped_date,\n",
							"        MAX(order_details_update_date) AS order_details_update_date,\n",
							"        MAX(order_update_date) AS order_update_date,\n",
							"        SUM(quantity) AS quantity,\n",
							"        SUM(discount) AS discount,\n",
							"        ROUND(SUM(quantity * unit_price), 2) AS amount,\n",
							"        ROUND(SUM(quantity * unit_price) / NULLIF(SUM(quantity), 0), 2) AS unit_price\n",
							"    FROM joined_orders\n",
							"    GROUP BY country_id, product_id, shipped_date)\n",
							"SELECT * FROM final_orders; \n",
							"\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW filter_aggregated_bronze_orders AS\n",
							"SELECT * from aggregated_bronze_orders  n \n",
							"where  NOT EXISTS (select product_id From silver.f_sales s \n",
							"                    where s.product_id =n.product_id \n",
							"                      and s.shipped_date = n.shipped_date\n",
							"                      and s.order_details_update_date = n.order_details_update_date\n",
							"                      and s.order_update_date = n.order_update_date);\n",
							"\n",
							"\n",
							"-- 3. Przeksztacenie i zapis danych do tabeli Silver z uyciem MERGE\n",
							"MERGE INTO silver.f_sales AS target\n",
							"USING filter_aggregated_bronze_orders AS source\n",
							"ON target.country_id = source.country_id\n",
							"   AND target.product_id = source.product_id\n",
							"   AND target.shipped_date = source.shipped_date\n",
							"WHEN MATCHED THEN\n",
							"    UPDATE SET\n",
							"        quantity = source.quantity,\n",
							"        discount = source.discount,\n",
							"        amount = source.amount,\n",
							"        unit_price = source.unit_price,\n",
							"        order_details_update_date = source.order_details_update_date,\n",
							"        order_update_date = source.order_update_date \n",
							"WHEN NOT MATCHED THEN\n",
							"    INSERT (\n",
							"        country_id, product_id, shipped_date,\n",
							"        quantity, discount, amount, unit_price, order_details_update_date, order_update_date \n",
							"    )\n",
							"    VALUES (\n",
							"        source.country_id, source.product_id, source.shipped_date,\n",
							"        source.quantity, source.discount, source.amount, source.unit_price,source.order_details_update_date,source.order_update_date \n",
							"    );\n",
							""
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- 1. Temporary views on files from ADLS Gen2\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW bronze_orders\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/orders'\n",
							");\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW bronze_order_details\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/order_details'\n",
							");\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW d_customers\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers'\n",
							");\n",
							"\n",
							"-- 2. Create target table (if not exists)\n",
							"CREATE TABLE IF NOT EXISTS silver.f_sales (\n",
							"    country_id STRING,\n",
							"    product_id STRING,\n",
							"    shipped_date DATE,\n",
							"    quantity DOUBLE,\n",
							"    discount DOUBLE,\n",
							"    amount DOUBLE,\n",
							"    unit_price DOUBLE,\n",
							"    order_details_update_date DATE,\n",
							"    order_update_date DATE\n",
							")\n",
							"USING DELTA\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/f_sales/';\n",
							"\n",
							"-- 2. Prepare data  latest update + monthly-level aggregation\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_bronze_orders AS\n",
							"WITH orders_trunc AS (\n",
							"    SELECT\n",
							"        order_id,\n",
							"        customer_id,\n",
							"        TRUNC(order_date, 'MM') AS order_date,\n",
							"        TRUNC(required_date, 'MM') AS required_date,\n",
							"        TRUNC(shipped_date, 'MM') AS shipped_date,\n",
							"        update_date AS order_update_date\n",
							"    FROM bronze_orders\n",
							"),\n",
							"joined_orders AS (\n",
							"    SELECT\n",
							"        o.order_id,\n",
							"        c.country_id,\n",
							"        od.product_id,\n",
							"        o.shipped_date,\n",
							"        od.unit_price,\n",
							"        od.quantity,\n",
							"        od.discount,\n",
							"        o.order_date,\n",
							"        od.update_date AS order_details_update_date,\n",
							"        o.order_update_date\n",
							"    FROM orders_trunc o\n",
							"    JOIN bronze_order_details od ON o.order_id = od.order_id\n",
							"    JOIN d_customers c ON o.customer_id = c.customer_id\n",
							"),\n",
							"final_orders AS (\n",
							"    SELECT\n",
							"        country_id,\n",
							"        product_id,\n",
							"        shipped_date,\n",
							"        MAX(order_details_update_date) AS order_details_update_date,\n",
							"        MAX(order_update_date) AS order_update_date,\n",
							"        SUM(quantity) AS quantity,\n",
							"        SUM(discount) AS discount,\n",
							"        ROUND(SUM(quantity * unit_price), 2) AS amount,\n",
							"        ROUND(SUM(quantity * unit_price) / NULLIF(SUM(quantity), 0), 2) AS unit_price\n",
							"    FROM joined_orders\n",
							"    GROUP BY country_id, product_id, shipped_date\n",
							")\n",
							"SELECT * FROM final_orders;\n",
							"\n",
							"-- Filter new or updated records\n",
							"CREATE OR REPLACE TEMP VIEW filter_aggregated_bronze_orders AS\n",
							"SELECT * FROM aggregated_bronze_orders n \n",
							"WHERE NOT EXISTS (\n",
							"  SELECT product_id FROM silver.f_sales s \n",
							"  WHERE s.product_id = n.product_id \n",
							"    AND s.shipped_date = n.shipped_date\n",
							"    AND s.order_details_update_date = n.order_details_update_date\n",
							"    AND s.order_update_date = n.order_update_date\n",
							");\n",
							"\n",
							"-- 3. Transform and write data to Silver table using MERGE\n",
							"MERGE INTO silver.f_sales AS target\n",
							"USING filter_aggregated_bronze_orders AS source\n",
							"ON target.country_id = source.country_id\n",
							"   AND target.product_id = source.product_id\n",
							"   AND target.shipped_date = source.shipped_date\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    quantity = source.quantity,\n",
							"    discount = source.discount,\n",
							"    amount = source.amount,\n",
							"    unit_price = source.unit_price,\n",
							"    order_details_update_date = source.order_details_update_date,\n",
							"    order_update_date = source.order_update_date \n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    country_id, product_id, shipped_date,\n",
							"    quantity, discount, amount, unit_price, order_details_update_date, order_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.country_id, source.product_id, source.shipped_date,\n",
							"    source.quantity, source.discount, source.amount, source.unit_price, source.order_details_update_date, source.order_update_date\n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_WH_DATA_LOAD_PY')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive/Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0f42fa21-f94e-48a0-8ece-d7d0c1644aef"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"\r\n",
							"# Tworzenie sesji Spark\r\n",
							"spark = SparkSession.builder.appName(\"AzureSynapseMigration\").getOrCreate()\r\n",
							"\r\n",
							"# Parametry dla warstwy Bronze (ADLS Gen2)\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"file_name = \"inventory_wholesaler_history_details\"\r\n",
							"\r\n",
							"# Parametry dla warstwy Silver (ADLS Gen2)\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"f_wh_data/\"\r\n",
							"\r\n",
							"silver_path = f\"{silver_bucket_name}/{silver_prefix}\"\r\n",
							"\r\n",
							"# cieki do plikw Parquet w ADLS Gen2\r\n",
							"path = f\"{bronze_bucket_name}{bronze_prefix}{file_name}\"\r\n",
							"path_customers =  f\"{silver_bucket_name}d_customers\"\r\n",
							"\r\n",
							"# Zaaduj dane z ADLS (tabela inventory_wholesaler_history_details)\r\n",
							"bronze_df = spark.read.parquet(path)\r\n",
							"customers_df = spark.read.parquet(path_customers)\r\n",
							"\r\n",
							"# Wykonaj JOIN na obu DataFrame'ach na podstawie customer_id\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"\r\n",
							"# Tworzenie sesji Spark\r\n",
							"spark = SparkSession.builder.appName(\"AzureSynapseMigration\").getOrCreate()\r\n",
							"\r\n",
							"# Parametry dla warstwy Bronze (ADLS Gen2)\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"file_name = \"inventory_wholesaler_history_details\"\r\n",
							"\r\n",
							"# Parametry dla warstwy Silver (ADLS Gen2)\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"f_wh_data/\"\r\n",
							"\r\n",
							"silver_path = f\"{silver_bucket_name}/{silver_prefix}\"\r\n",
							"\r\n",
							"# cieki do plikw Parquet w ADLS Gen2\r\n",
							"path = f\"{bronze_bucket_name}{bronze_prefix}{file_name}\"\r\n",
							"path_customers =  f\"{silver_bucket_name}d_customers\"\r\n",
							"\r\n",
							"# Zaaduj dane z ADLS (tabela inventory_wholesaler_history_details)\r\n",
							"bronze_df = spark.read.parquet(path)\r\n",
							"customers_df = spark.read.parquet(path_customers)\r\n",
							"\r\n",
							"# Wykonaj JOIN na obu DataFrame'ach na podstawie customer_id\r\n",
							"\r\n",
							"\r\n",
							"bronze_df=bronze_df.join(customers_df,\"customer_id\")\r\n",
							"\r\n",
							"latest_bronze_df = bronze_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"product_id\", \"customer_id\",\"update_date\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"new_bronze_df = latest_bronze_df.select(\r\n",
							"    \"product_id\",\r\n",
							"    \"country_id\", \r\n",
							"    \"transaction_date\", \r\n",
							"    \"unit_price\", \r\n",
							"    \"whrs_sell_in_quantity\",\r\n",
							"    \"whrs_open_quantity\",  # Poprawiona nazwa\r\n",
							"    \"whrs_end_quantity\"\r\n",
							")\r\n",
							"\r\n",
							"new_bronze_df = new_bronze_df.withColumn(\r\n",
							"    \"transaction_date\", trunc(\"transaction_date\",\"month\"))\r\n",
							"\r\n",
							"aggregated_bronze_df = new_bronze_df.groupBy(\"product_id\", \"country_id\", \"transaction_date\").agg(\r\n",
							"                                    # Summing quantity, discount, and amount\r\n",
							"                                     F.sum(\"whrs_sell_in_quantity\").alias(\"whrs_sell_in_quantity\"),\r\n",
							"                                     F.sum(\"whrs_open_quantity\").alias(\"whrs_open_quantity\"),\r\n",
							"                                     F.sum(\"whrs_end_quantity\").alias(\"whrs_end_quantity\"),\r\n",
							"                                     F.round(F.sum(F.col(\"whrs_sell_in_quantity\") * F.col(\"unit_price\")),2).alias(\"whrs_sell_in_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"whrs_open_quantity\") * F.col(\"unit_price\")),2).alias(\"whrs_open_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"whrs_end_quantity\") * F.col(\"unit_price\")),2).alias(\"whrs_end_amount\"),\r\n",
							"                                     F.round((F.sum(F.col(\"whrs_sell_in_quantity\") * F.col(\"unit_price\")) / F.sum(\"whrs_sell_in_quantity\")),2).alias(\"unit_price\"))\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver ju istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"\r\n",
							"\r\n",
							"# Jeli tabela Silver istnieje, wykonaj operacj \"MERGE\" na podstawie DataFrame\r\n",
							"if silver_exists:\r\n",
							"    # Zaaduj dane z tabeli Silver\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    # Pocz dane Silver z nowymi danymi (na podstawie order_id, customer_id, product_id)\r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        aggregated_bronze_df.alias(\"new_pod\"),\r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_pod.product_id\")) & \r\n",
							"        (F.col(\"silver.country_id\") == F.col(\"new_pod.country_id\")) & \r\n",
							"        (F.col(\"silver.transaction_date\") == F.col(\"new_pod.transaction_date\")),\r\n",
							"        how=\"outer\"\r\n",
							"    )\r\n",
							"\r\n",
							"    # Wybr kolumn, ktre maj zosta zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_pod.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_pod.country_id\", \"silver.country_id\").alias(\"country_id\"),\r\n",
							"        F.coalesce(\"new_pod.transaction_date\", \"silver.transaction_date\").alias(\"transaction_date\"),\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.whrs_sell_in_quantity\", \"silver.whrs_sell_in_quantity\").alias(\"whrs_sell_in_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_open_quantity\", \"silver.whrs_open_quantity\").alias(\"whrs_open_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_end_quantity\", \"silver.whrs_end_quantity\").alias(\"whrs_end_quantity\"),\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.whrs_sell_in_amount\", \"silver.whrs_sell_in_amount\").alias(\"whrs_sell_in_amount\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_open_amount\", \"silver.whrs_open_amount\").alias(\"whrs_open_amount\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_end_amount\", \"silver.whrs_end_amount\").alias(\"whrs_end_amount\"),\r\n",
							"        F.coalesce(\"new_pod.unit_price\", \"silver.unit_price\").alias(\"unit_price\")\r\n",
							"        )\r\n",
							"    # Display or save the resulting DataFrame\r\n",
							"    # Zapisanie zaktualizowanego DataFrame do warstwy Silver\r\n",
							"    final_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"else:\r\n",
							"    aggregated_bronze_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakoczenie\r\n",
							"print(\"Inkrementalne adowanie zakoczone!\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_WH_DATA_LOAD_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ec2437e6-437c-4c8c-a06f-b9f176fb7ab6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"## 1. Load data from Bronze and Customers as temporary views\n",
							"df_bronze = spark.read.option(\"header\", \"true\").csv(\n",
							"    \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/*/*/*.csv\"\n",
							")\n",
							"df_bronze.createOrReplaceTempView(\"bronze_inventory\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- 1. Create a temporary view from the Silver d_customers table\n",
							"CREATE OR REPLACE TEMP VIEW silver_d_customers\n",
							"USING delta\n",
							"OPTIONS (\n",
							"  path 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers'\n",
							");\n",
							"\n",
							"-- 2. Create the target Silver table as Delta (if it does not exist)\n",
							"CREATE TABLE IF NOT EXISTS silver.f_wh_data (\n",
							"  product_id STRING,\n",
							"  country_id STRING,\n",
							"  transaction_date DATE,\n",
							"  whrs_sell_in_quantity DOUBLE,\n",
							"  whrs_open_quantity DOUBLE,\n",
							"  whrs_end_quantity DOUBLE,\n",
							"  whrs_sell_in_amount DOUBLE,\n",
							"  whrs_open_amount DOUBLE,\n",
							"  whrs_end_amount DOUBLE,\n",
							"  unit_price DOUBLE,\n",
							"  whrs_update_date DATE\n",
							")\n",
							"USING DELTA\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/f_wh_data/';\n",
							"\n",
							"-- 3. Join and aggregate warehouse data\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_bronze_wh AS\n",
							"    WITH join_wh AS (\n",
							"        SELECT\n",
							"            inv.*,\n",
							"            c.country_id\n",
							"        FROM bronze_inventory inv\n",
							"        JOIN silver_d_customers c ON inv.customer_id = c.customer_id\n",
							"    ),\n",
							"    select_wh AS (\n",
							"        SELECT\n",
							"            product_id,\n",
							"            country_id,\n",
							"            update_date AS whrs_update_date,\n",
							"            TRUNC(transaction_date, 'MM') AS transaction_date,\n",
							"            unit_price,\n",
							"            whrs_sell_in_quantity,\n",
							"            whrs_open_quantity,\n",
							"            whrs_end_quantity\n",
							"        FROM join_wh\n",
							"    ),\n",
							"    aggregated AS (\n",
							"        SELECT\n",
							"            product_id,\n",
							"            country_id,\n",
							"            transaction_date,\n",
							"            MAX(whrs_update_date) AS whrs_update_date,\n",
							"            SUM(whrs_sell_in_quantity) AS whrs_sell_in_quantity,\n",
							"            SUM(whrs_open_quantity) AS whrs_open_quantity,\n",
							"            SUM(whrs_end_quantity) AS whrs_end_quantity,\n",
							"            ROUND(SUM(whrs_sell_in_quantity * unit_price), 2) AS whrs_sell_in_amount,\n",
							"            ROUND(SUM(whrs_open_quantity * unit_price), 2) AS whrs_open_amount,\n",
							"            ROUND(SUM(whrs_end_quantity * unit_price), 2) AS whrs_end_amount,\n",
							"            ROUND(SUM(whrs_sell_in_quantity * unit_price) / NULLIF(SUM(whrs_sell_in_quantity), 0), 2) AS unit_price\n",
							"        FROM select_wh\n",
							"        GROUP BY product_id, country_id, transaction_date\n",
							"    )\n",
							"SELECT * FROM aggregated;\n",
							"\n",
							"-- 4. Filter new or updated records\n",
							"CREATE OR REPLACE TEMP VIEW filter_aggregated_bronze_wh AS\n",
							"SELECT * FROM aggregated_bronze_wh n \n",
							"WHERE NOT EXISTS (\n",
							"  SELECT product_id FROM silver.f_wh_data s \n",
							"  WHERE s.product_id = n.product_id \n",
							"    AND s.country_id = n.country_id\n",
							"    AND s.transaction_date = n.transaction_date\n",
							"    AND s.whrs_update_date = n.whrs_update_date\n",
							");\n",
							"\n",
							"-- 5. MERGE the data into the Silver table with aggregation and filtering\n",
							"MERGE INTO silver.f_wh_data AS target\n",
							"USING filter_aggregated_bronze_wh AS source\n",
							"ON target.product_id = source.product_id\n",
							"   AND target.country_id = source.country_id\n",
							"   AND target.transaction_date = source.transaction_date\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    whrs_sell_in_quantity = source.whrs_sell_in_quantity,\n",
							"    whrs_open_quantity = source.whrs_open_quantity,\n",
							"    whrs_end_quantity = source.whrs_end_quantity,\n",
							"    whrs_sell_in_amount = source.whrs_sell_in_amount,\n",
							"    whrs_open_amount = source.whrs_open_amount,\n",
							"    whrs_end_amount = source.whrs_end_amount,\n",
							"    unit_price = source.unit_price,\n",
							"    whrs_update_date = source.whrs_update_date\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id, country_id, transaction_date,\n",
							"    whrs_sell_in_quantity, whrs_open_quantity, whrs_end_quantity,\n",
							"    whrs_sell_in_amount, whrs_open_amount, whrs_end_amount,\n",
							"    unit_price, whrs_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id, source.country_id, source.transaction_date,\n",
							"    source.whrs_sell_in_quantity, source.whrs_open_quantity, source.whrs_end_quantity,\n",
							"    source.whrs_sell_in_amount, source.whrs_open_amount, source.whrs_end_amount,\n",
							"    source.unit_price, source.whrs_update_date\n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_WH_DATA_LOAD_SQL_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bff5e975-b52f-4a23-a651-6a4dd95abe6d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"## 1. Zaaduj dane z Bronze i Customers jako tymczasowe widoki\n",
							"df_bronze = spark.read.option(\"header\", \"true\").csv(\n",
							"    \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/*/*/*.csv\"\n",
							")\n",
							"df_bronze.createOrReplaceTempView(\"bronze_inventory\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW silver_d_customers\n",
							"USING delta\n",
							"OPTIONS (\n",
							"  path 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers'\n",
							");\n",
							"\n",
							"-- 2. Utwrz tabel docelow Silver jako Delta (jeli nie istnieje)\n",
							"CREATE TABLE IF NOT EXISTS silver.f_wh_data (\n",
							"  product_id STRING,\n",
							"  country_id STRING,\n",
							"  transaction_date DATE,\n",
							"  whrs_sell_in_quantity DOUBLE,\n",
							"  whrs_open_quantity DOUBLE,\n",
							"  whrs_end_quantity DOUBLE,\n",
							"  whrs_sell_in_amount DOUBLE,\n",
							"  whrs_open_amount DOUBLE,\n",
							"  whrs_end_amount DOUBLE,\n",
							"  unit_price DOUBLE,\n",
							"  whrs_update_date DATE\n",
							")\n",
							"USING DELTA\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/f_wh_data/';\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_bronze_wh AS\n",
							"    WITH join_wh AS (\n",
							"        SELECT\n",
							"            inv.*,\n",
							"            c.country_id\n",
							"        FROM bronze_inventory inv\n",
							"        JOIN silver_d_customers c ON inv.customer_id = c.customer_id\n",
							"    ),\n",
							"    select_wh AS (\n",
							"        SELECT\n",
							"            product_id,\n",
							"            country_id,\n",
							"            update_date as whrs_update_date,\n",
							"            TRUNC(transaction_date, 'MM') AS transaction_date,\n",
							"            unit_price,\n",
							"            whrs_sell_in_quantity,\n",
							"            whrs_open_quantity,\n",
							"            whrs_end_quantity\n",
							"        FROM join_wh\n",
							"    ),\n",
							"    aggregated AS (\n",
							"        SELECT\n",
							"            product_id,\n",
							"            country_id,\n",
							"            transaction_date,\n",
							"            max(whrs_update_date) as  whrs_update_date,\n",
							"            SUM(whrs_sell_in_quantity) AS whrs_sell_in_quantity,\n",
							"            SUM(whrs_open_quantity) AS whrs_open_quantity,\n",
							"            SUM(whrs_end_quantity) AS whrs_end_quantity,\n",
							"            ROUND(SUM(whrs_sell_in_quantity * unit_price), 2) AS whrs_sell_in_amount,\n",
							"            ROUND(SUM(whrs_open_quantity * unit_price), 2) AS whrs_open_amount,\n",
							"            ROUND(SUM(whrs_end_quantity * unit_price), 2) AS whrs_end_amount,\n",
							"            ROUND(SUM(whrs_sell_in_quantity * unit_price) / NULLIF(SUM(whrs_sell_in_quantity), 0), 2) AS unit_price\n",
							"        FROM select_wh\n",
							"        GROUP BY product_id, country_id, transaction_date\n",
							"    )\n",
							" SELECT * FROM aggregated;\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW filter_aggregated_bronze_wh AS\n",
							"SELECT * from aggregated_bronze_wh  n \n",
							"where  NOT EXISTS (select product_id From silver.f_wh_data s \n",
							"                    where s.product_id =n.product_id \n",
							"                      and s.country_id = n.country_id\n",
							"                      and s.transaction_date = n.transaction_date\n",
							"                      and s.whrs_update_date = n.whrs_update_date);\n",
							"\n",
							"\n",
							"-- 3. MERGE danych do tabeli Silver z agregacj i filtrowaniem\n",
							"MERGE INTO silver.f_wh_data AS target\n",
							"USING filter_aggregated_bronze_wh AS source\n",
							"ON target.product_id = source.product_id\n",
							"   AND target.country_id = source.country_id\n",
							"   AND target.transaction_date = source.transaction_date\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    whrs_sell_in_quantity = source.whrs_sell_in_quantity,\n",
							"    whrs_open_quantity = source.whrs_open_quantity,\n",
							"    whrs_end_quantity = source.whrs_end_quantity,\n",
							"    whrs_sell_in_amount = source.whrs_sell_in_amount,\n",
							"    whrs_open_amount = source.whrs_open_amount,\n",
							"    whrs_end_amount = source.whrs_end_amount,\n",
							"    unit_price = source.unit_price,\n",
							"    whrs_update_date = source.whrs_update_date\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id, country_id, transaction_date,\n",
							"    whrs_sell_in_quantity, whrs_open_quantity, whrs_end_quantity,\n",
							"    whrs_sell_in_amount, whrs_open_amount, whrs_end_amount,\n",
							"    unit_price, whrs_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id, source.country_id, source.transaction_date,\n",
							"    source.whrs_sell_in_quantity, source.whrs_open_quantity, source.whrs_end_quantity,\n",
							"    source.whrs_sell_in_amount, source.whrs_open_amount, source.whrs_end_amount,\n",
							"    source.unit_price,source.whrs_update_date\n",
							"  );\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"## 1. Load data from Bronze and Customers as temporary views\n",
							"df_bronze = spark.read.option(\"header\", \"true\").csv(\n",
							"    \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/*/*/*.csv\"\n",
							")\n",
							"df_bronze.createOrReplaceTempView(\"bronze_inventory\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- 1. Create a temporary view from the Silver d_customers table\n",
							"CREATE OR REPLACE TEMP VIEW silver_d_customers\n",
							"USING delta\n",
							"OPTIONS (\n",
							"  path 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers'\n",
							");\n",
							"\n",
							"-- 2. Create the target Silver table as Delta (if it does not exist)\n",
							"CREATE TABLE IF NOT EXISTS silver.f_wh_data (\n",
							"  product_id STRING,\n",
							"  country_id STRING,\n",
							"  transaction_date DATE,\n",
							"  whrs_sell_in_quantity DOUBLE,\n",
							"  whrs_open_quantity DOUBLE,\n",
							"  whrs_end_quantity DOUBLE,\n",
							"  whrs_sell_in_amount DOUBLE,\n",
							"  whrs_open_amount DOUBLE,\n",
							"  whrs_end_amount DOUBLE,\n",
							"  unit_price DOUBLE,\n",
							"  whrs_update_date DATE\n",
							")\n",
							"USING DELTA\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/f_wh_data/';\n",
							"\n",
							"-- 3. Join and aggregate warehouse data\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_bronze_wh AS\n",
							"    WITH join_wh AS (\n",
							"        SELECT\n",
							"            inv.*,\n",
							"            c.country_id\n",
							"        FROM bronze_inventory inv\n",
							"        JOIN silver_d_customers c ON inv.customer_id = c.customer_id\n",
							"    ),\n",
							"    select_wh AS (\n",
							"        SELECT\n",
							"            product_id,\n",
							"            country_id,\n",
							"            update_date AS whrs_update_date,\n",
							"            TRUNC(transaction_date, 'MM') AS transaction_date,\n",
							"            unit_price,\n",
							"            whrs_sell_in_quantity,\n",
							"            whrs_open_quantity,\n",
							"            whrs_end_quantity\n",
							"        FROM join_wh\n",
							"    ),\n",
							"    aggregated AS (\n",
							"        SELECT\n",
							"            product_id,\n",
							"            country_id,\n",
							"            transaction_date,\n",
							"            MAX(whrs_update_date) AS whrs_update_date,\n",
							"            SUM(whrs_sell_in_quantity) AS whrs_sell_in_quantity,\n",
							"            SUM(whrs_open_quantity) AS whrs_open_quantity,\n",
							"            SUM(whrs_end_quantity) AS whrs_end_quantity,\n",
							"            ROUND(SUM(whrs_sell_in_quantity * unit_price), 2) AS whrs_sell_in_amount,\n",
							"            ROUND(SUM(whrs_open_quantity * unit_price), 2) AS whrs_open_amount,\n",
							"            ROUND(SUM(whrs_end_quantity * unit_price), 2) AS whrs_end_amount,\n",
							"            ROUND(SUM(whrs_sell_in_quantity * unit_price) / NULLIF(SUM(whrs_sell_in_quantity), 0), 2) AS unit_price\n",
							"        FROM select_wh\n",
							"        GROUP BY product_id, country_id, transaction_date\n",
							"    )\n",
							"SELECT * FROM aggregated;\n",
							"\n",
							"-- 4. Filter new or updated records\n",
							"CREATE OR REPLACE TEMP VIEW filter_aggregated_bronze_wh AS\n",
							"SELECT * FROM aggregated_bronze_wh n \n",
							"WHERE NOT EXISTS (\n",
							"  SELECT product_id FROM silver.f_wh_data s \n",
							"  WHERE s.product_id = n.product_id \n",
							"    AND s.country_id = n.country_id\n",
							"    AND s.transaction_date = n.transaction_date\n",
							"    AND s.whrs_update_date = n.whrs_update_date\n",
							");\n",
							"\n",
							"-- 5. MERGE the data into the Silver table with aggregation and filtering\n",
							"MERGE INTO silver.f_wh_data AS target\n",
							"USING filter_aggregated_bronze_wh AS source\n",
							"ON target.product_id = source.product_id\n",
							"   AND target.country_id = source.country_id\n",
							"   AND target.transaction_date = source.transaction_date\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    whrs_sell_in_quantity = source.whrs_sell_in_quantity,\n",
							"    whrs_open_quantity = source.whrs_open_quantity,\n",
							"    whrs_end_quantity = source.whrs_end_quantity,\n",
							"    whrs_sell_in_amount = source.whrs_sell_in_amount,\n",
							"    whrs_open_amount = source.whrs_open_amount,\n",
							"    whrs_end_amount = source.whrs_end_amount,\n",
							"    unit_price = source.unit_price,\n",
							"    whrs_update_date = source.whrs_update_date\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id, country_id, transaction_date,\n",
							"    whrs_sell_in_quantity, whrs_open_quantity, whrs_end_quantity,\n",
							"    whrs_sell_in_amount, whrs_open_amount, whrs_end_amount,\n",
							"    unit_price, whrs_update_date\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id, source.country_id, source.transaction_date,\n",
							"    source.whrs_sell_in_quantity, source.whrs_open_quantity, source.whrs_end_quantity,\n",
							"    source.whrs_sell_in_amount, source.whrs_open_amount, source.whrs_end_amount,\n",
							"    source.unit_price, source.whrs_update_date\n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LOAD_DISTRIBUTORS_DATA_PY')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Bronze"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9e013c1d-5e9a-4e23-964e-0c6e7de49617"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"##LOAD_DISTRIBUTORS_DATA_PY\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"from urllib.parse import unquote\n",
							"\n",
							"access_key = mssparkutils.credentials.getSecret('keylakehouse','s3-access-key')\n",
							"\n",
							"secret_key = mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key')\n",
							"\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
							"\n",
							"from pyspark.sql.utils import AnalysisException\n",
							"from pyspark.sql.functions import input_file_name\n",
							"\n",
							"# Path to DeltaTable with the list of processed files\n",
							"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/Processed_Files_Logs\"\n",
							"\n",
							"try:\n",
							"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
							"    print(\"Loaded existing Delta file with processed files.\")\n",
							"except AnalysisException:\n",
							"    processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
							"    print(\"Delta file does not exist. Creating a new empty DataFrame.\")\n",
							"\n",
							"\n",
							"# MinIO parameters\n",
							"bucket_name = \"distributors\"\n",
							"folder_path = \"data/country=*/closed_month=*/\"\n",
							"\n",
							"# Read all CSV files from MinIO (including subfolders)\n",
							"files_metadata = spark.read \\\n",
							"    .option(\"header\", \"true\") \\\n",
							"    .csv(f\"s3a://{bucket_name}/{folder_path}\") \\\n",
							"    .withColumn(\"input_file\", input_file_name())\n",
							"# Extract file names\n",
							"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
							"\n",
							"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
							"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
							"\n",
							"# List of already processed files\n",
							"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
							"\n",
							"# Find new files to process\n",
							"new_paths = [paths for paths in all_paths if unquote(paths) not in processed_filenames]\n",
							"\n",
							"print(f\"Found {len(new_paths)} new paths to process.\")\n",
							"# Process new files\n",
							"for path in new_paths:\n",
							"    file_path = path\n",
							"    file_path = file_path.split(\"?\")[0]\n",
							"    print(file_path)\n",
							"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
							"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
							"    print(f\"Loading path: {file_path}\")\n",
							"    print(f\"Loading file: {file_name}\")\n",
							"    print(f\"Loading path: {country_name}\")\n",
							"\n",
							"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/{country_name}/{file_name}\"\n",
							"\n",
							"    print(f\"Target path: {target_path}\")\n",
							"    print(f\"Loading file: {file_path}\")\n",
							"\n",
							"    try:\n",
							"        df_new = spark.read.option(\"header\", \"true\").csv(unquote(file_path))\n",
							"        df_new.write.option(\"header\", \"true\").mode(\"overwrite\").csv(unquote(target_path))\n",
							"        print(f\"Saved file to ADLS: {target_path}\")\n",
							"    except Exception as e:\n",
							"        print(f\"Error processing file {file_path}: {e}\")\n",
							"\n",
							"# Update the list of processed files\n",
							"if new_paths:\n",
							"    new_files_df = spark.createDataFrame([(path,) for path in new_paths], [\"filepaths\"])\n",
							"    processed_files_updated = processed_df.union(new_files_df)\n",
							"    processed_files_updated.show()\n",
							"    # Save updated list in Delta format\n",
							"    processed_files_updated.write.format(\"delta\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save(processed_files_path)\n",
							"\n",
							"    print(f\"Updated the processed files list in Delta with {processed_files_updated.count()} entries.\")\n",
							"else:\n",
							"    print(\"No new files to process.\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LOAD_DISTRIBUTORS_DATA_PY_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a1e4eac5-de42-4c8f-8a88-11759e25e138"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							" ##LOAD_DISTRIBUTORS_DATA_PY\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"from urllib.parse import unquote\n",
							"\n",
							"access_key=mssparkutils.credentials.getSecret('keylakehouse','s3-access-key')\n",
							"\n",
							"secret_key =mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key')\n",
							"\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
							"\n",
							"from pyspark.sql.utils import AnalysisException\n",
							"from pyspark.sql.functions import input_file_name\n",
							"\n",
							"# cieka do DeltaTable z list przetworzonych plikw\n",
							"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/Processed_Files_Logs\"\n",
							"\n",
							"try:\n",
							"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
							"    print(\"Zaadowano istniejcy plik Delta z przetworzonymi plikami.\")\n",
							"except AnalysisException:\n",
							"   processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
							"   \n",
							"   print(\"Plik Delta nie istnieje. Tworzony nowy pusty DataFrame.\")\n",
							"   \n",
							"\n",
							"\n",
							"\n",
							"# Parametry MinIO\n",
							"bucket_name = \"distributors\"\n",
							"folder_path = \"data/country=*/closed_month=*/\"\n",
							"\n",
							"# Wczytanie wszystkich plikw CSV z MinIO (z subfolderami)\n",
							"files_metadata = spark.read \\\n",
							"    .option(\"header\", \"true\") \\\n",
							"    .csv(f\"s3a://{bucket_name}/{folder_path}\") \\\n",
							"    .withColumn(\"input_file\", input_file_name())\n",
							"# Wycignicie nazw plikw\n",
							"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
							"\n",
							"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
							"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
							"\n",
							"# Lista ju przetworzonych plikw\n",
							"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
							"\n",
							"# Wyszukanie nowych plikw do przetworzenia\n",
							"new_paths = [paths for paths in all_paths if  unquote(paths) not in processed_filenames]\n",
							"\n",
							"print(f\"Znaleziono {len(new_paths)} nowych sciezek do przetworzenia.\")\n",
							"# Przetwarzanie nowych plikw\n",
							"for path in new_paths:\n",
							"    file_path = path\n",
							"    file_path= file_path.split(\"?\")[0]\n",
							"    print(file_path)\n",
							"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
							"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
							"    print(f\"Wczytywanie sciezki: {file_path}\")\n",
							"    print(f\"Wczytywanie pliku: {file_name}\")\n",
							"    print(f\"Wczytywanie sciezki: {country_name}\")\n",
							"\n",
							"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/{country_name}/{file_name}\"\n",
							"\n",
							"    print(f\"target pathn{target_path}\")\n",
							"    print(f\" Wczytywanie pliku: {file_path}\")\n",
							"\n",
							"    try:\n",
							"        df_new = spark.read.option(\"header\", \"true\").csv(unquote(file_path))\n",
							"        df_new.write.option(\"header\", \"true\").mode(\"overwrite\").csv(unquote(target_path))\n",
							"        print(f\"Zapisano plik do ADLS: {target_path}\")\n",
							"    except Exception as e:\n",
							"        print(f\"Bd podczas przetwarzania pliku {file_path}: {e}\")\n",
							"\n",
							"# Aktualizacja listy przetworzonych plikw\n",
							"if new_paths:\n",
							"    new_files_df = spark.createDataFrame([(path,) for path in new_paths], [\"filepaths\"])\n",
							"    processed_files_updated = processed_df.union(new_files_df)\n",
							"    processed_files_updated.show()\n",
							"    # Zapis zaktualizowanej listy w formacie Delta\n",
							"    processed_files_updated.write.format(\"delta\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"       .save(processed_files_path)\n",
							"\n",
							"    print(f\"Zaktualizowano {processed_files_updated.count()} list przetworzonych plikw w Delta.\")\n",
							"else:\n",
							"    print(\"Brak nowych plikw do przetworzenia.\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"##LOAD_DISTRIBUTORS_DATA_PY\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"from urllib.parse import unquote\n",
							"\n",
							"access_key = mssparkutils.credentials.getSecret('keylakehouse','s3-access-key')\n",
							"\n",
							"secret_key = mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key')\n",
							"\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
							"\n",
							"from pyspark.sql.utils import AnalysisException\n",
							"from pyspark.sql.functions import input_file_name\n",
							"\n",
							"# Path to DeltaTable with the list of processed files\n",
							"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/Processed_Files_Logs\"\n",
							"\n",
							"try:\n",
							"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
							"    print(\"Loaded existing Delta file with processed files.\")\n",
							"except AnalysisException:\n",
							"    processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
							"    print(\"Delta file does not exist. Creating a new empty DataFrame.\")\n",
							"\n",
							"\n",
							"# MinIO parameters\n",
							"bucket_name = \"distributors\"\n",
							"folder_path = \"data/country=*/closed_month=*/\"\n",
							"\n",
							"# Read all CSV files from MinIO (including subfolders)\n",
							"files_metadata = spark.read \\\n",
							"    .option(\"header\", \"true\") \\\n",
							"    .csv(f\"s3a://{bucket_name}/{folder_path}\") \\\n",
							"    .withColumn(\"input_file\", input_file_name())\n",
							"# Extract file names\n",
							"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
							"\n",
							"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
							"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
							"\n",
							"# List of already processed files\n",
							"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
							"\n",
							"# Find new files to process\n",
							"new_paths = [paths for paths in all_paths if unquote(paths) not in processed_filenames]\n",
							"\n",
							"print(f\"Found {len(new_paths)} new paths to process.\")\n",
							"# Process new files\n",
							"for path in new_paths:\n",
							"    file_path = path\n",
							"    file_path = file_path.split(\"?\")[0]\n",
							"    print(file_path)\n",
							"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
							"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
							"    print(f\"Loading path: {file_path}\")\n",
							"    print(f\"Loading file: {file_name}\")\n",
							"    print(f\"Loading path: {country_name}\")\n",
							"\n",
							"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/{country_name}/{file_name}\"\n",
							"\n",
							"    print(f\"Target path: {target_path}\")\n",
							"    print(f\"Loading file: {file_path}\")\n",
							"\n",
							"    try:\n",
							"        df_new = spark.read.option(\"header\", \"true\").csv(unquote(file_path))\n",
							"        df_new.write.option(\"header\", \"true\").mode(\"overwrite\").csv(unquote(target_path))\n",
							"        print(f\"Saved file to ADLS: {target_path}\")\n",
							"    except Exception as e:\n",
							"        print(f\"Error processing file {file_path}: {e}\")\n",
							"\n",
							"# Update the list of processed files\n",
							"if new_paths:\n",
							"    new_files_df = spark.createDataFrame([(path,) for path in new_paths], [\"filepaths\"])\n",
							"    processed_files_updated = processed_df.union(new_files_df)\n",
							"    processed_files_updated.show()\n",
							"    # Save updated list in Delta format\n",
							"    processed_files_updated.write.format(\"delta\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save(processed_files_path)\n",
							"\n",
							"    print(f\"Updated the processed files list in Delta with {processed_files_updated.count()} entries.\")\n",
							"else:\n",
							"    print(\"No new files to process.\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LOAD_PHARMACIES_DATA_PY')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Bronze"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "277dd1d3-2ed5-4d72-a5f9-97beeca46aa6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#LOAD_PHARMACIES_DATA_PY\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"from urllib.parse import unquote\n",
							"\n",
							"access_key = mssparkutils.credentials.getSecret('keylakehouse','s3-access-key')\n",
							"\n",
							"secret_key = mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key')\n",
							"\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
							"\n",
							"from pyspark.sql.utils import AnalysisException\n",
							"from pyspark.sql.functions import input_file_name\n",
							"\n",
							"# Path to DeltaTable with the list of processed files\n",
							"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/Processed_Files_Logs\"\n",
							"\n",
							"try:\n",
							"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
							"    print(\"Loaded existing Delta file with processed files.\")\n",
							"except AnalysisException:\n",
							"   processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
							"   print(\"Delta file does not exist. Creating a new empty DataFrame.\")\n",
							"\n",
							"\n",
							"# MinIO parameters\n",
							"bucket_name = \"pharmacies\"\n",
							"folder_path = \"data/country=*/closed_month=*/\"\n",
							"\n",
							"# Read all CSV files from MinIO (including subfolders)\n",
							"files_metadata = spark.read \\\n",
							"    .option(\"header\", \"true\") \\\n",
							"    .csv(f\"s3a://{bucket_name}/{folder_path}*\") \\\n",
							"    .withColumn(\"input_file\", input_file_name())\n",
							"\n",
							"# Extract file names\n",
							"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
							"\n",
							"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
							"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
							"\n",
							"# List of already processed files\n",
							"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
							"\n",
							"# Find new files to process\n",
							"new_paths = [paths for paths in all_paths if unquote(paths) not in processed_filenames]\n",
							"\n",
							"print(f\"Found {len(new_paths)} new paths to process.\")\n",
							"\n",
							"# Process new files\n",
							"for path in new_paths:\n",
							"    file_path = path\n",
							"    file_path = file_path.split(\"?\")[0]\n",
							"    print(file_path)\n",
							"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
							"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
							"    print(f\"Loading path: {file_path}\")\n",
							"    print(f\"Loading file: {file_name}\")\n",
							"    print(f\"Loading path: {country_name}\")\n",
							"\n",
							"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/{country_name}/{file_name}\"\n",
							"\n",
							"    print(f\"Loading file: {file_path}\")\n",
							"\n",
							"    try:\n",
							"        df_new = spark.read.option(\"header\", \"true\").csv(unquote(file_path))\n",
							"        df_new.write.option(\"header\", \"true\").mode(\"overwrite\").csv(unquote(target_path))\n",
							"        print(f\"Saved file to ADLS: {target_path}\")\n",
							"    except Exception as e:\n",
							"        print(f\"Error processing file {file_path}: {e}\")\n",
							"\n",
							"# Update the list of processed files\n",
							"if new_paths:\n",
							"    new_files_df = spark.createDataFrame([(unquote(path),) for path in new_paths], [\"pathname\"])\n",
							"    processed_files_updated = processed_df.union(new_files_df)\n",
							"    processed_files_updated.show()\n",
							"    # Save updated list in Delta format\n",
							"    processed_files_updated.write.format(\"delta\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"       .save(processed_files_path)\n",
							"\n",
							"    print(f\"Updated the processed files list in Delta with {processed_files_updated.count()} entries.\")\n",
							"else:\n",
							"    print(\"No new files to process.\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LOAD_PHARMACIES_DATA_PY_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d6956483-d446-4d46-900d-73d8b9743a67"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#LOAD_PHARMACIES_DATA_PY\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"from urllib.parse import unquote\n",
							"\n",
							"access_key=mssparkutils.credentials.getSecret('keylakehouse','s3-access-key')\n",
							"\n",
							"secret_key =mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key')\n",
							"\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
							"\n",
							"from pyspark.sql.utils import AnalysisException\n",
							"from pyspark.sql.functions import input_file_name\n",
							"\n",
							"# cieka do DeltaTable z list przetworzonych plikw\n",
							"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/Processed_Files_Logs\"\n",
							"\n",
							"try:\n",
							"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
							"    print(\"Zaadowano istniejcy plik Delta z przetworzonymi plikami.\")\n",
							"except AnalysisException:\n",
							"   processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
							"   print(\"Plik Delta nie istnieje. Tworzony nowy pusty DataFrame.\")\n",
							"\n",
							"\n",
							"\n",
							"# Parametry MinIO\n",
							"bucket_name = \"pharmacies\"\n",
							"folder_path = \"data/country=*/closed_month=*/\"\n",
							"\n",
							"# Wczytanie wszystkich plikw CSV z MinIO (z subfolderami)\n",
							"files_metadata = spark.read \\\n",
							"    .option(\"header\", \"true\") \\\n",
							"    .csv(f\"s3a://{bucket_name}/{folder_path}*\") \\\n",
							"    .withColumn(\"input_file\", input_file_name())\n",
							"\n",
							"# Wycignicie nazw plikw\n",
							"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
							"\n",
							"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
							"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
							"\n",
							"# Lista ju przetworzonych plikw\n",
							"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
							"\n",
							"# Wyszukanie nowych plikw do przetworzenia\n",
							"new_paths = [paths for paths in all_paths if  unquote(paths) not in processed_filenames]\n",
							"\n",
							"print(f\"Znaleziono {len(new_paths)} nowych sciezek do przetworzenia.\")\n",
							"\n",
							"# Przetwarzanie nowych plikw\n",
							"for path in new_paths:\n",
							"    file_path = path\n",
							"    file_path= file_path.split(\"?\")[0]\n",
							"    print(file_path)\n",
							"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
							"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
							"    print(f\"Wczytywanie sciezki: {file_path}\")\n",
							"    print(f\"Wczytywanie pliku: {file_name}\")\n",
							"    print(f\"Wczytywanie sciezki: {country_name}\")\n",
							"\n",
							"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/{country_name}/{file_name}\"\n",
							"\n",
							"\n",
							"    print(f\" Wczytywanie pliku: {file_path}\")\n",
							"\n",
							"    try:\n",
							"        df_new = spark.read.option(\"header\", \"true\").csv(unquote(file_path))\n",
							"        df_new.write.option(\"header\", \"true\").mode(\"overwrite\").csv(unquote(target_path))\n",
							"        print(f\"Zapisano plik do ADLS: {target_path}\")\n",
							"    except Exception as e:\n",
							"        print(f\"Bd podczas przetwarzania pliku {file_path}: {e}\")\n",
							"\n",
							"# Aktualizacja listy przetworzonych plikw\n",
							"if new_paths:\n",
							"    new_files_df = spark.createDataFrame([(unquote(path),) for path in new_paths], [\"pathname\"])\n",
							"    processed_files_updated = processed_df.union(new_files_df)\n",
							"    processed_files_updated.show()\n",
							"    # Zapis zaktualizowanej listy w formacie Delta\n",
							"    processed_files_updated.write.format(\"delta\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"       .save(processed_files_path)\n",
							"\n",
							"    print(f\"Zaktualizowano {processed_files_updated.count()} list przetworzonych plikw w Delta.\")\n",
							"else:\n",
							"    print(\"Brak nowych plikw do przetworzenia.\");\n",
							"\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"#LOAD_PHARMACIES_DATA_PY\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"from urllib.parse import unquote\n",
							"\n",
							"access_key = mssparkutils.credentials.getSecret('keylakehouse','s3-access-key')\n",
							"\n",
							"secret_key = mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key')\n",
							"\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
							"\n",
							"from pyspark.sql.utils import AnalysisException\n",
							"from pyspark.sql.functions import input_file_name\n",
							"\n",
							"# Path to DeltaTable with the list of processed files\n",
							"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/Processed_Files_Logs\"\n",
							"\n",
							"try:\n",
							"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
							"    print(\"Loaded existing Delta file with processed files.\")\n",
							"except AnalysisException:\n",
							"   processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
							"   print(\"Delta file does not exist. Creating a new empty DataFrame.\")\n",
							"\n",
							"\n",
							"# MinIO parameters\n",
							"bucket_name = \"pharmacies\"\n",
							"folder_path = \"data/country=*/closed_month=*/\"\n",
							"\n",
							"# Read all CSV files from MinIO (including subfolders)\n",
							"files_metadata = spark.read \\\n",
							"    .option(\"header\", \"true\") \\\n",
							"    .csv(f\"s3a://{bucket_name}/{folder_path}*\") \\\n",
							"    .withColumn(\"input_file\", input_file_name())\n",
							"\n",
							"# Extract file names\n",
							"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
							"\n",
							"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
							"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
							"\n",
							"# List of already processed files\n",
							"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
							"\n",
							"# Find new files to process\n",
							"new_paths = [paths for paths in all_paths if unquote(paths) not in processed_filenames]\n",
							"\n",
							"print(f\"Found {len(new_paths)} new paths to process.\")\n",
							"\n",
							"# Process new files\n",
							"for path in new_paths:\n",
							"    file_path = path\n",
							"    file_path = file_path.split(\"?\")[0]\n",
							"    print(file_path)\n",
							"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
							"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
							"    print(f\"Loading path: {file_path}\")\n",
							"    print(f\"Loading file: {file_name}\")\n",
							"    print(f\"Loading path: {country_name}\")\n",
							"\n",
							"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/{country_name}/{file_name}\"\n",
							"\n",
							"    print(f\"Loading file: {file_path}\")\n",
							"\n",
							"    try:\n",
							"        df_new = spark.read.option(\"header\", \"true\").csv(unquote(file_path))\n",
							"        df_new.write.option(\"header\", \"true\").mode(\"overwrite\").csv(unquote(target_path))\n",
							"        print(f\"Saved file to ADLS: {target_path}\")\n",
							"    except Exception as e:\n",
							"        print(f\"Error processing file {file_path}: {e}\")\n",
							"\n",
							"# Update the list of processed files\n",
							"if new_paths:\n",
							"    new_files_df = spark.createDataFrame([(unquote(path),) for path in new_paths], [\"pathname\"])\n",
							"    processed_files_updated = processed_df.union(new_files_df)\n",
							"    processed_files_updated.show()\n",
							"    # Save updated list in Delta format\n",
							"    processed_files_updated.write.format(\"delta\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"       .save(processed_files_path)\n",
							"\n",
							"    print(f\"Updated the processed files list in Delta with {processed_files_updated.count()} entries.\")\n",
							"else:\n",
							"    print(\"No new files to process.\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Run All Bronze')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c81777d9-d6d8-4cbe-9460-9aab669cd30c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##LOAD_DISTRIBUTORS_DATA_PY\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"\n",
							"access_key=mssparkutils.credentials.getSecret('keylakehouse','s3-access-key')\n",
							"\n",
							"secret_key =mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key')\n",
							"\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
							"\n",
							"from pyspark.sql.utils import AnalysisException\n",
							"from pyspark.sql.functions import input_file_name\n",
							"\n",
							"# cieka do DeltaTable z list przetworzonych plikw\n",
							"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/Processed_Files_Logs\"\n",
							"\n",
							"try:\n",
							"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
							"    print(\"Zaadowano istniejcy plik Delta z przetworzonymi plikami.\")\n",
							"except AnalysisException:\n",
							"   processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
							"   \n",
							"   print(\"Plik Delta nie istnieje. Tworzony nowy pusty DataFrame.\")\n",
							"   \n",
							"\n",
							"\n",
							"\n",
							"# Parametry MinIO\n",
							"bucket_name = \"distributors\"\n",
							"folder_path = \"data/country=*/closed_month=*/\"\n",
							"\n",
							"# Wczytanie wszystkich plikw CSV z MinIO (z subfolderami)\n",
							"files_metadata = spark.read \\\n",
							"    .option(\"header\", \"true\") \\\n",
							"    .csv(f\"s3a://{bucket_name}/{folder_path}\") \\\n",
							"    .withColumn(\"input_file\", input_file_name())\n",
							"# Wycignicie nazw plikw\n",
							"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
							"\n",
							"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
							"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
							"\n",
							"# Lista ju przetworzonych plikw\n",
							"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
							"\n",
							"# Wyszukanie nowych plikw do przetworzenia\n",
							"new_paths = [paths for paths in all_paths if  paths not in processed_filenames]\n",
							"\n",
							"print(f\"Znaleziono {len(new_paths)} nowych sciezek do przetworzenia.\")\n",
							"# Przetwarzanie nowych plikw\n",
							"for path in new_paths:\n",
							"    file_path = path\n",
							"    file_path= file_path.split(\"?\")[0]\n",
							"    print(file_path)\n",
							"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
							"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
							"    print(f\"Wczytywanie sciezki: {file_path}\")\n",
							"    print(f\"Wczytywanie pliku: {file_name}\")\n",
							"    print(f\"Wczytywanie sciezki: {country_name}\")\n",
							"\n",
							"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/{country_name}/{file_name}\"\n",
							"\n",
							"    print(f\"target pathn{target_path}\")\n",
							"    print(f\" Wczytywanie pliku: {file_path}\")\n",
							"\n",
							"    try:\n",
							"        df_new = spark.read.option(\"header\", \"true\").csv(file_path)\n",
							"        df_new.write.mode(\"overwrite\").csv(target_path)\n",
							"        print(f\"Zapisano plik do ADLS: {target_path}\")\n",
							"    except Exception as e:\n",
							"        print(f\"Bd podczas przetwarzania pliku {file_path}: {e}\")\n",
							"\n",
							"# Aktualizacja listy przetworzonych plikw\n",
							"if new_paths:\n",
							"    new_files_df = spark.createDataFrame([(path,) for path in new_paths], [\"filepaths\"])\n",
							"    processed_files_updated = processed_df.union(new_files_df)\n",
							"    processed_files_updated.show()\n",
							"    # Zapis zaktualizowanej listy w formacie Delta\n",
							"    processed_files_updated.write.format(\"delta\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"       .save(processed_files_path)\n",
							"\n",
							"    print(f\"Zaktualizowano {processed_files_updated.count()} list przetworzonych plikw w Delta.\")\n",
							"else:\n",
							"    print(\"Brak nowych plikw do przetworzenia.\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"#LOAD_PHARMACIES_DATA_PY\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"\n",
							"access_key=mssparkutils.credentials.getSecret('keylakehouse','s3-access-key')\n",
							"\n",
							"secret_key =mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key')\n",
							"\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
							"\n",
							"from pyspark.sql.utils import AnalysisException\n",
							"from pyspark.sql.functions import input_file_name\n",
							"\n",
							"# cieka do DeltaTable z list przetworzonych plikw\n",
							"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/Processed_Files_Logs\"\n",
							"\n",
							"try:\n",
							"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
							"    print(\"Zaadowano istniejcy plik Delta z przetworzonymi plikami.\")\n",
							"except AnalysisException:\n",
							"   processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
							"   print(\"Plik Delta nie istnieje. Tworzony nowy pusty DataFrame.\")\n",
							"\n",
							"\n",
							"\n",
							"# Parametry MinIO\n",
							"bucket_name = \"pharmacies\"\n",
							"folder_path = \"data/country=*/closed_month=*/\"\n",
							"\n",
							"# Wczytanie wszystkich plikw CSV z MinIO (z subfolderami)\n",
							"files_metadata = spark.read \\\n",
							"    .option(\"header\", \"true\") \\\n",
							"    .csv(f\"s3a://{bucket_name}/{folder_path}*\") \\\n",
							"    .withColumn(\"input_file\", input_file_name())\n",
							"\n",
							"# Wycignicie nazw plikw\n",
							"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
							"\n",
							"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
							"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
							"\n",
							"# Lista ju przetworzonych plikw\n",
							"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
							"\n",
							"# Wyszukanie nowych plikw do przetworzenia\n",
							"new_paths = [paths for paths in all_paths if  paths not in processed_filenames]\n",
							"\n",
							"print(f\"Znaleziono {len(new_paths)} nowych sciezek do przetworzenia.\")\n",
							"\n",
							"# Przetwarzanie nowych plikw\n",
							"for path in new_paths:\n",
							"    file_path = path\n",
							"    file_path= file_path.split(\"?\")[0]\n",
							"    print(file_path)\n",
							"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
							"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
							"    print(f\"Wczytywanie sciezki: {file_path}\")\n",
							"    print(f\"Wczytywanie pliku: {file_name}\")\n",
							"    print(f\"Wczytywanie sciezki: {country_name}\")\n",
							"\n",
							"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/{country_name}/{file_name}\"\n",
							"\n",
							"\n",
							"    print(f\" Wczytywanie pliku: {file_path}\")\n",
							"\n",
							"    try:\n",
							"        df_new = spark.read.option(\"header\", \"true\").csv(file_path)\n",
							"        df_new.write.mode(\"overwrite\").csv(target_path)\n",
							"        print(f\"Zapisano plik do ADLS: {target_path}\")\n",
							"    except Exception as e:\n",
							"        print(f\"Bd podczas przetwarzania pliku {file_path}: {e}\")\n",
							"\n",
							"# Aktualizacja listy przetworzonych plikw\n",
							"if new_paths:\n",
							"    new_files_df = spark.createDataFrame([(path,) for path in new_paths], [\"pathname\"])\n",
							"    processed_files_updated = processed_df.union(new_files_df)\n",
							"    processed_files_updated.show()\n",
							"    # Zapis zaktualizowanej listy w formacie Delta\n",
							"    processed_files_updated.write.format(\"delta\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"       .save(processed_files_path)\n",
							"\n",
							"    print(f\"Zaktualizowano {processed_files_updated.count()} list przetworzonych plikw w Delta.\")\n",
							"else:\n",
							"    print(\"Brak nowych plikw do przetworzenia.\")\n",
							"\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkpool32')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "polandcentral"
		}
	]
}