{
	"name": "D_DATES_PY",
	"properties": {
		"folder": {
			"name": "Load Gold"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool32",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "95632a5c-be7f-44a2-a3e2-4c1e27b65ea2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
				"name": "sparkpool32",
				"type": "Spark",
				"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import (\n",
					"    col, lit, year, month, quarter, weekofyear, dayofmonth,\n",
					"    dayofweek, dayofyear, date_format, current_timestamp, when, concat_ws\n",
					")\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"\n",
					"# Define the date range for the calendar table\n",
					"start_year = 2020\n",
					"end_year = 2025\n",
					"\n",
					"# Create a list of dates from start_date to end_date using pandas\n",
					"start_date = datetime(start_year, 1, 1)\n",
					"end_date = datetime(end_year, 12, 31)\n",
					"date_range = pd.date_range(start=start_date, end=end_date).to_frame(index=False, name=\"Date\")\n",
					"\n",
					"# Convert the pandas DataFrame to a Spark DataFrame\n",
					"df = spark.createDataFrame(date_range)\n",
					"\n",
					"# Add calendar-related columns to enrich the date dimension\n",
					"df = df.withColumn(\"Year\", year(\"Date\")) \\\n",
					"       .withColumn(\"Month\", month(\"Date\")) \\\n",
					"       .withColumn(\"QuarterNum\", quarter(\"Date\")) \\\n",
					"       .withColumn(\"WeekOfYear\", weekofyear(\"Date\")) \\\n",
					"       .withColumn(\"Day\", dayofmonth(\"Date\")) \\\n",
					"       .withColumn(\"DayOfWeek\", dayofweek(\"Date\")) \\\n",
					"       .withColumn(\"DayOfYear\", dayofyear(\"Date\")) \\\n",
					"       .withColumn(\"DayName\", date_format(\"Date\", \"EEEE\")) \\\n",
					"       .withColumn(\"MonthName\", date_format(\"Date\", \"MMMM\")) \\\n",
					"       .withColumn(\"NowMonth\", month(current_timestamp())) \\\n",
					"       .withColumn(\"YTD_BTG\", when(col(\"Month\") < col(\"NowMonth\"), \"YTD\").otherwise(\"BTG\")) \\\n",
					"       .withColumn(\"FY\", lit(\"FY\")) \\\n",
					"       .withColumn(\"Quarter\", concat_ws(\"\", lit(\"Q\"), col(\"QuarterNum\").cast(\"string\"))) \\\n",
					"       .withColumn(\"KeyYearMonth\", concat_ws(\"\", col(\"Year\").cast(\"string\"), col(\"Month\").cast(\"string\"))) \\\n",
					"       .withColumn(\"KeyYearQuarter\", concat_ws(\"\", col(\"Year\").cast(\"string\"), col(\"Quarter\"))) \\\n",
					"       .withColumn(\"KeyYearYTDBTG\", concat_ws(\"\", col(\"Year\").cast(\"string\"), col(\"YTD_BTG\"))) \\\n",
					"       .withColumn(\"KeyYearFY\", concat_ws(\"\", col(\"Year\").cast(\"string\"), col(\"FY\")))\n",
					"# Optional: reorder columns to desired output format\n",
					"columns_order = [\"Year\", \"Quarter\", \"Month\", \"Date\", \"YTD_BTG\",'FY']\n",
					"\n",
					"df = df.select(*columns_order)\n",
					"\n",
					"df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").save(\"abfss://gold@bigpharma.dfs.core.windows.net/d_dates\")"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# Remove unused composite key columns\n",
					"df = df.drop(\"KeyYearMonth\", \"KeyYearQuarter\", \"KeyYearYTDBTG\", \"KeyYearFY\")\n",
					"\n",
					"# Optional: reorder columns to desired output format\n",
					"columns_order = [\"Year\", \"Quarter\", \"Month\", \"Date\", \"YTD_BTG\"]\n",
					"\n",
					"df = df.select(*columns_order)\n",
					"\n",
					"# Save the resulting date dimension to a Delta Lake table in Azure Data Lake Storage\n",
					"df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").save(\"abfss://gold@bigpharma.dfs.core.windows.net/d_dates\")\n",
					"df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").save(\"abfss://gold@bigpharma.dfs.core.windows.net/d_dates\")"
				],
				"execution_count": null
			}
		]
	}
}