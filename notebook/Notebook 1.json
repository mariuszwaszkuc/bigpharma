{
	"name": "Notebook 1",
	"properties": {
		"folder": {
			"name": "Arch"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7a807791-4380-436f-940e-949726b48877"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col, max\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql import functions as F\n",
					"from delta.tables import DeltaTable\n",
					"\n",
					"# Ścieżki\n",
					"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\n",
					"bronze_prefix = \"Erp/\"\n",
					"file_name = \"customers\"\n",
					"bronze_path = f\"{bronze_bucket_name}{bronze_prefix}{file_name}/*.parquet\"\n",
					"join_path = f\"{bronze_bucket_name}{bronze_prefix}countries/*.parquet\"\n",
					"\n",
					"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\n",
					"silver_prefix = \"d_customers\"\n",
					"silver_path = f\"{silver_bucket_name}{silver_prefix}\"\n",
					"\n",
					"# Wczytanie danych\n",
					"bronze_df = spark.read.format(\"parquet\").load(bronze_path)\n",
					"join_bronze_df = spark.read.format(\"parquet\").load(join_path)\n",
					"\n",
					"# Zmiana nazw kolumn z kolizją\n",
					"join_bronze_df = join_bronze_df.select(\n",
					"    [col(c).alias(f\"{c}_right\") if c != \"country_id\" else col(c) for c in join_bronze_df.columns]\n",
					")\n",
					"\n",
					"# Join\n",
					"joined_df = bronze_df.join(join_bronze_df, on=\"country_id\", how=\"inner\")\n",
					"\n",
					"# Najnowsze dane wg customer_id\n",
					"latest_bronze_df = joined_df.withColumn(\n",
					"    \"max_order_date\", max(\"update_date\").over(Window.partitionBy(\"customer_id\"))\n",
					").filter(col(\"update_date\") == col(\"max_order_date\"))\n",
					"\n",
					"# Usuwanie sufiksów _right\n",
					"new_column_names = [col_name.replace(\"_right\", \"\") for col_name in latest_bronze_df.columns]\n",
					"new_bronze_df = latest_bronze_df.toDF(*new_column_names)\n",
					"\n",
					"# Kolumny końcowe\n",
					"new_bronze_df = new_bronze_df.select(\n",
					"    \"customer_id\",\n",
					"    \"country_id\", \n",
					"    \"company_name\", \n",
					"    \"address\", \n",
					"    \"city\",\n",
					"    \"region_description\", \n",
					"    \"country\", \n",
					"    \"cluster\"\n",
					")\n",
					"\n",
					"# Zapis z Delta Lake\n",
					"try:\n",
					"    delta_table = DeltaTable.forPath(spark, silver_path)\n",
					"    delta_table.alias(\"target\").merge(\n",
					"        new_bronze_df.alias(\"source\"),\n",
					"        \"target.customer_id = source.customer_id\"\n",
					"    ).whenMatchedUpdateAll() \\\n",
					"     .whenNotMatchedInsertAll() \\\n",
					"     .execute()\n",
					"\n",
					"    print(\"Zaktualizowano istniejącą tabelę Delta (UPSERT).\")\n",
					"\n",
					"except Exception as e:\n",
					"    print(f\"Nie znaleziono tabeli Delta — zostanie utworzona. Szczegóły: {str(e)}\")\n",
					"    new_bronze_df.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n",
					"    print(\"Zapisano nową tabelę Delta.\")"
				],
				"execution_count": null
			}
		]
	}
}