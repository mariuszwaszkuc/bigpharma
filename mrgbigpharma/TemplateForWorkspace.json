{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "mrgbigpharma"
		},
		"AmazonS3Compatible2_secretAccessKey": {
			"type": "secureString",
			"metadata": "Secure string for 'secretAccessKey' of 'AmazonS3Compatible2'"
		},
		"MinIoS3_secretAccessKey": {
			"type": "secureString",
			"metadata": "Secure string for 'secretAccessKey' of 'MinIoS3'"
		},
		"S3MinIO_secretAccessKey": {
			"type": "secureString",
			"metadata": "Secure string for 'secretAccessKey' of 'S3MinIO'"
		},
		"mrgbigpharma-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'mrgbigpharma-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:mrgbigpharma.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AmazonS3Compatible2_properties_typeProperties_accessKeyId": {
			"type": "string",
			"defaultValue": "minioadmin"
		},
		"MinIoS3_properties_typeProperties_accessKeyId": {
			"type": "string",
			"defaultValue": "minioadmin"
		},
		"MySqlErp_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "@{linkedService().Server}"
		},
		"MySqlErp_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "@{linkedService().Database}"
		},
		"MySqlErp_properties_typeProperties_username": {
			"type": "string",
			"defaultValue": "@{linkedService().User}"
		},
		"MySqlPassword_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://keylakehouse.vault.azure.net/"
		},
		"S3MinIO_properties_typeProperties_accessKeyId": {
			"type": "string",
			"defaultValue": "minioadmin"
		},
		"eceuropaeu_inflation_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/ei_cphi_m$defaultview/?format=TSV&compressed=false"
		},
		"mrgbigpharma-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bigpharma.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Extract_Load_Erp')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "New Watermark List of Tabels",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "MySqlSource",
								"query": "call GetMaxIdForAllTables();\n\nSELECT table_name, create_date, update_date, \n\nlast_date FROM logs;"
							},
							"dataset": {
								"referenceName": "MySql_Erp_List_Tables",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "For Each Table",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "New Watermark List of Tabels",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('New Watermark List of Tabels').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "If Condition1",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@equals(item().table_name, 'logs') ",
											"type": "Expression"
										},
										"ifFalseActivities": [
											{
												"name": "Export Table Copy",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "MySqlSource",
														"query": {
															"value": "@concat(\n    'SELECT * FROM ', \n    item().table_name,\n    ' WHERE \n    create_date <=  \n    (SELECT MAX(create_date) AS create_date  \n     FROM logs \n     WHERE table_name = ''', item().table_name, ''' )\n    AND create_date >= STR_TO_DATE(''', item().last_date, ''', ''%Y-%m-%dT%H:%i:%s'')\n    LIMIT 10;'\n)",
															"type": "Expression"
														}
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings",
															"copyBehavior": "FlattenHierarchy"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"translator": {
														"type": "TabularTranslator",
														"typeConversion": true,
														"typeConversionSettings": {
															"allowDataTruncation": true,
															"treatBooleanAsNumber": false
														}
													}
												},
												"inputs": [
													{
														"referenceName": "MySql_Erp_Dynamic_Load",
														"type": "DatasetReference",
														"parameters": {}
													}
												],
												"outputs": [
													{
														"referenceName": "Bronze_Parquet",
														"type": "DatasetReference",
														"parameters": {
															"filename": {
																"value": "@if( equals(item().table_name, 'logs') , \n     concat(item().table_name,'.parquet'),\n     concat(item().table_name,'_',formatDateTime(item().last_date,'yyyy-MM-dd'),'.parquet'))",
																"type": "Expression"
															},
															"foldername": "@item().table_name"
														}
													}
												]
											}
										],
										"ifTrueActivities": [
											{
												"name": "Export Table Logs",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "MySqlSource",
														"query": {
															"value": "@concat(\n 'Select * from ',\n item().table_name)\n",
															"type": "Expression"
														}
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"translator": {
														"type": "TabularTranslator",
														"typeConversion": true,
														"typeConversionSettings": {
															"allowDataTruncation": true,
															"treatBooleanAsNumber": false
														}
													}
												},
												"inputs": [
													{
														"referenceName": "MySql_Erp_Dynamic_Load",
														"type": "DatasetReference",
														"parameters": {}
													}
												],
												"outputs": [
													{
														"referenceName": "Bronze_Parquet",
														"type": "DatasetReference",
														"parameters": {
															"filename": {
																"value": "@if( equals(item().table_name, 'logs') , \n     concat(item().table_name,'.parquet'),\n     concat(item().table_name,'_',formatDateTime(item().create_date,'yyyy-MM-ddTHH:mm:s'),'.parquet'))",
																"type": "Expression"
															},
															"foldername": "@item().table_name"
														}
													}
												]
											}
										]
									}
								}
							]
						}
					},
					{
						"name": "Setup Max Id",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "For Each Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "MySqlSource",
								"query": "call serwer296707_pharma.SetMaxIdForAllTables;\n\nSELECT table_name, create_date, update_date, \n\nlast_date FROM logs;"
							},
							"dataset": {
								"referenceName": "MySql_Erp_List_Tables",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "Brozne"
				},
				"annotations": [],
				"lastPublishTime": "2025-03-27T05:52:42Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/MySql_Erp_List_Tables')]",
				"[concat(variables('workspaceId'), '/datasets/MySql_Erp_Dynamic_Load')]",
				"[concat(variables('workspaceId'), '/datasets/Bronze_Parquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Extract_Load_Eurostat')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Inflation_load",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET",
									"requestTimeout": ""
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "freq,unit,s_adj,indic,geo\\TIME_PERIOD",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "freq,unit,s_adj,indic,geo\\TIME_PERIOD",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-05 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-05 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-06 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-06 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-07 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-07 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-08 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-08 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-09 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-09 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-10 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-10 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-11 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-11 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2024-12 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2024-12 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2025-01 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2025-01 ",
											"type": "String",
											"physicalType": "String"
										}
									},
									{
										"source": {
											"name": "2025-02 ",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "2025-02 ",
											"type": "String",
											"physicalType": "String"
										}
									}
								],
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "Web_Eurostat_Inflation",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "External_Inflation_Csv",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "Brozne"
				},
				"annotations": [],
				"lastPublishTime": "2025-03-26T08:44:54Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Web_Eurostat_Inflation')]",
				"[concat(variables('workspaceId'), '/datasets/External_Inflation_Csv')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Transform_Silver')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "D_Customer_Load",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 2,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "D_CUSTOMERS_LOAD",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": false
							},
							"driverSize": "Small",
							"authentication": {
								"type": "MSI"
							}
						}
					},
					{
						"name": "D_Products",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "D_Customer_Load",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "D_PRODUCT_LOAD",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": false,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 2
							},
							"driverSize": "Small",
							"numExecutors": 2,
							"authentication": {
								"type": "UserAssignedManagedIdentity"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "Silver"
				},
				"annotations": [],
				"lastPublishTime": "2025-04-02T06:40:26Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/D_CUSTOMERS_LOAD')]",
				"[concat(variables('workspaceId'), '/notebooks/D_PRODUCT_LOAD')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Bronze_Csv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "mrgbigpharma-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filename": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Targets_Bronze"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().filename",
							"type": "Expression"
						},
						"folderPath": "Erp",
						"fileSystem": "bronze"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/mrgbigpharma-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Bronze_Log_Parquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "mrgbigpharma-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "Sources_Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "logs.parquet",
						"folderPath": "Erp/logs",
						"fileSystem": "bronze"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "table_name",
						"type": "UTF8"
					},
					{
						"name": "create_date",
						"type": "INT96"
					},
					{
						"name": "update_date",
						"type": "INT96"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/mrgbigpharma-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Bronze_Parquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "mrgbigpharma-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filename": {
						"type": "string"
					},
					"foldername": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Targets_Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().filename",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@concat('Erp/',dataset().foldername)",
							"type": "Expression"
						},
						"fileSystem": "bronze"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/mrgbigpharma-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ERP_DATA_LOCATION')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "mrgbigpharma-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"TblName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().TblName",
							"type": "Expression"
						},
						"folderPath": "Erp",
						"fileSystem": "bronze"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/mrgbigpharma-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/External_Inflation_Csv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "mrgbigpharma-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "Targets_Bronze"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "inflation.csv",
						"folderPath": "ExternalStatistics",
						"fileSystem": "bronze"
					},
					"columnDelimiter": ";",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/mrgbigpharma-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MinIoDataDistributors')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "MinIoS3",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AmazonS3CompatibleLocation",
						"bucketName": "distributors",
						"fileName": "part-00000-b20e9098-7b48-48f2-888f-57de2602203c.c000.csv"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/MinIoS3')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MySql_Erp_Dynamic_Load')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "MySqlErp",
					"type": "LinkedServiceReference",
					"parameters": {
						"User": "serwer296707_pharma",
						"Database": "serwer296707_pharma",
						"Server": "sql133.lh.pl"
					}
				},
				"folder": {
					"name": "Sources_Bronze"
				},
				"annotations": [],
				"type": "MySqlTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/MySqlErp')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MySql_Erp_List_Tables')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "MySqlErp",
					"type": "LinkedServiceReference",
					"parameters": {
						"User": "serwer296707_pharma",
						"Database": "serwer296707_pharma",
						"Server": "sql133.lh.pl"
					}
				},
				"folder": {
					"name": "Sources_Bronze"
				},
				"annotations": [],
				"type": "MySqlTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/MySqlErp')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SinkBronzeDistributors')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "mrgbigpharma-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"FileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().FileName",
							"type": "Expression"
						},
						"folderPath": "MinIoDistributors",
						"fileSystem": "bronze"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/mrgbigpharma-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Web_Eurostat_Inflation')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "eceuropaeu_inflation",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "Sources_Bronze"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					},
					"columnDelimiter": "\t",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "freq,unit,s_adj,indic,geo\\TIME_PERIOD",
						"type": "String"
					},
					{
						"name": "2024-05 ",
						"type": "String"
					},
					{
						"name": "2024-06 ",
						"type": "String"
					},
					{
						"name": "2024-07 ",
						"type": "String"
					},
					{
						"name": "2024-08 ",
						"type": "String"
					},
					{
						"name": "2024-09 ",
						"type": "String"
					},
					{
						"name": "2024-10 ",
						"type": "String"
					},
					{
						"name": "2024-11 ",
						"type": "String"
					},
					{
						"name": "2024-12 ",
						"type": "String"
					},
					{
						"name": "2025-01 ",
						"type": "String"
					},
					{
						"name": "2025-02 ",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/eceuropaeu_inflation')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AmazonS3Compatible2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AmazonS3Compatible",
				"typeProperties": {
					"serviceUrl": "http://20.215.33.25:9000",
					"accessKeyId": "[parameters('AmazonS3Compatible2_properties_typeProperties_accessKeyId')]",
					"secretAccessKey": {
						"type": "SecureString",
						"value": "[parameters('AmazonS3Compatible2_secretAccessKey')]"
					},
					"forcePathStyle": true
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MinIoS3')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AmazonS3Compatible",
				"typeProperties": {
					"serviceUrl": "http://20.215.33.25:9000",
					"accessKeyId": "[parameters('MinIoS3_properties_typeProperties_accessKeyId')]",
					"secretAccessKey": {
						"type": "SecureString",
						"value": "[parameters('MinIoS3_secretAccessKey')]"
					},
					"forcePathStyle": true
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MySqlErp')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"User": {
						"type": "String",
						"defaultValue": "serwer296707_pharma"
					},
					"Database": {
						"type": "String",
						"defaultValue": "serwer296707_pharma"
					},
					"Server": {
						"type": "String",
						"defaultValue": "sql133.lh.pl"
					}
				},
				"annotations": [],
				"type": "MySql",
				"typeProperties": {
					"server": "[parameters('MySqlErp_properties_typeProperties_server')]",
					"port": 3306,
					"database": "[parameters('MySqlErp_properties_typeProperties_database')]",
					"username": "[parameters('MySqlErp_properties_typeProperties_username')]",
					"sslMode": 1,
					"useSystemTrustStore": 0,
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "MySqlPassword",
							"type": "LinkedServiceReference"
						},
						"secretName": "mysqlpassword"
					},
					"driverVersion": "v2"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/MySqlPassword')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MySqlPassword')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('MySqlPassword_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/S3MinIO')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AmazonS3",
				"typeProperties": {
					"serviceUrl": "http://20.215.33.25:9000",
					"accessKeyId": "[parameters('S3MinIO_properties_typeProperties_accessKeyId')]",
					"secretAccessKey": {
						"type": "SecureString",
						"value": "[parameters('S3MinIO_secretAccessKey')]"
					},
					"authenticationType": "AccessKey"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/eceuropaeu_inflation')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('eceuropaeu_inflation_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mrgbigpharma-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('mrgbigpharma-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mrgbigpharma-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('mrgbigpharma-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create External Table D_Products')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'gold_bigpharma_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [gold_bigpharma_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://gold@bigpharma.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE dbo.d_products (\n\t[product_id] int,\n\t[product_name] nvarchar(4000),\n\t[brand_name] nvarchar(4000),\n\t[sub_brand_name] nvarchar(4000),\n\t[category_name] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'd_products/**',\n\tDATA_SOURCE = [gold_bigpharma_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.d_products\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sql_Serverless",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create External Table D_Regions')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'gold_bigpharma_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [gold_bigpharma_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://gold@bigpharma.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE dbo.d_regions (\n\t[country_id] int,\n\t[cluster] nvarchar(4000),\n\t[region_description] nvarchar(4000),\n\t[country] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'd_regions/**',\n\tDATA_SOURCE = [gold_bigpharma_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.d_regions\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sql_Serverless",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create External Table F_Planning_Book')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'gold_bigpharma_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [gold_bigpharma_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://gold@bigpharma.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE dbo.f_planning_book (\n\t[product_id] int,\n\t[country_id] int,\n\t[financial_date] date,\n\t[forecast_snapshot] date,\n\t[quantity] bigint,\n\t[amount] float,\n\t[unit_price] float,\n\t[whrs_sell_in_quantity] bigint,\n\t[whrs_open_quantity] bigint,\n\t[whrs_end_quantity] bigint,\n\t[whrs_sell_in_amount] float,\n\t[whrs_open_amount] float,\n\t[whrs_end_amount] float,\n\t[pos_sell_out_quantity] bigint,\n\t[pos_open_quantity] bigint,\n\t[pos_end_quantity] bigint,\n\t[pos_sell_out_amount] float,\n\t[pos_open_amount] float,\n\t[pos_end_amount] float,\n\t[discount] float\n\t)\n\tWITH (\n\tLOCATION = 'f_planning_book/**',\n\tDATA_SOURCE = [gold_bigpharma_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.f_planning_book\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sql_Serverless",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://bigpharma.dfs.core.windows.net/silver/d_customers/part-00000-9a15f7cc-1eac-449b-a175-79469d168038-c000.snappy.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://bigpharma.dfs.core.windows.net/silver/d_customers/',\n        FORMAT = 'DELTA'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    \nFROM\n    OPENROWSET(\n        BULK 'https://bigpharma.dfs.core.windows.net/bronze/Erp/orders/**',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 5')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://bigpharma.dfs.core.windows.net/bronze/Erp/order_details/**',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_CUSTOMERS_LOAD')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "35d38d31-6525-4f4c-9dcc-7d08f3be889e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Zmiana nazw kolumn w join_bronze_df, aby uniknąć duplikacji nazw\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"file_name = \"customers\"\r\n",
							"\r\n",
							"# Bucket dla warstwy Silver\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"d_customers\"\r\n",
							"\r\n",
							"# Ścieżka dla warstwy Silver\r\n",
							"silver_path =  f\"{silver_bucket_name}{silver_prefix}\"\r\n",
							"\r\n",
							"# Ścieżki do plików Parquet w MinIO\r\n",
							"path =  f\"{bronze_bucket_name}{bronze_prefix}{file_name}/*.parquet\"\r\n",
							"print(path )\r\n",
							"\r\n",
							"bronze_df = spark.read.load(path, format='parquet')\r\n",
							"join_path = f\"{bronze_bucket_name}{bronze_prefix}countries/*.parquet\"\r\n",
							"\r\n",
							"join_bronze_df = spark.read.load(join_path, format='parquet')\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"source": [
							"\r\n",
							"join_bronze_df = join_bronze_df.select(\r\n",
							"    [col(c).alias(f\"{c}_right\") if c != \"country_id\" else col(c) for c in join_bronze_df.columns]\r\n",
							")\r\n",
							"\r\n",
							"# Wykonanie JOIN po kolumnie \"country_id\"\r\n",
							"joined_df = bronze_df.join(join_bronze_df, on=\"country_id\", how=\"inner\")  # Możesz zmienić \"inner\" na \"left\", \"right\" itp.\r\n",
							"\r\n",
							"# Wyświetlenie wyników\r\n",
							"print(joined_df.columns)\r\n",
							"\r\n",
							"latest_bronze_df = joined_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"customer_id\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"# Wybór odpowiednich kolumn do zapisania w warstwie Silver\r\n",
							"new_bronze_df = latest_bronze_df.select(\r\n",
							"    \"customer_id\", \r\n",
							"    \"company_name\", \r\n",
							"    \"address\", \r\n",
							"    \"country_id\", \r\n",
							"    \"city\",\r\n",
							"    \"region_description_right\",\r\n",
							"    \"country_right\",\r\n",
							"    \"cluster_right\"\r\n",
							")\r\n",
							"new_column_names = [col_name.replace(\"_right\", \"\") for col_name in latest_bronze_df.columns]\r\n",
							"\r\n",
							"# Przypisz nowe nazwy kolumn do DataFrame\r\n",
							"new_bronze_df = latest_bronze_df.toDF(*new_column_names)\r\n",
							"# Wybierz poprawione kolumny\r\n",
							"new_bronze_df = new_bronze_df.select(\r\n",
							"    \"customer_id\",\r\n",
							"    \"country_id\", \r\n",
							"    \"company_name\", \r\n",
							"    \"address\", \r\n",
							"    \"city\",\r\n",
							"    \"region_description\",  # Poprawiona nazwa\r\n",
							"    \"country\",  # Poprawiona nazwa\r\n",
							"    \"cluster\"\r\n",
							")\r\n",
							"new_bronze_df.show()\r\n",
							"# Sprawdzenie, czy tabela Silver już istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"\r\n",
							"\r\n",
							"if silver_exists:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        new_bronze_df.alias(\"new_bronze_df\"),\r\n",
							"        (F.col(\"silver.customer_id\") == F.col(\"new_bronze_df.customer_id\")),\r\n",
							"        how=\"outer\"\r\n",
							"        )\r\n",
							"\r\n",
							"        # Wybór kolumn, które mają zostać zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_bronze_df.customer_id\", \"silver.customer_id\").alias(\"customer_id\"),\r\n",
							"        F.coalesce(\"new_bronze_df.country_id\", \"silver.country_id\").alias(\"country_id\"),\r\n",
							"        F.coalesce(\"new_bronze_df.company_name\", \"silver.company_name\").alias(\"company_name\"),\r\n",
							"        F.coalesce(\"new_bronze_df.address\", \"silver.address\").alias(\"address\"),\r\n",
							"        F.coalesce(\"new_bronze_df.city\", \"silver.city\").alias(\"city\"),\r\n",
							"        F.coalesce(\"new_bronze_df.region_description\", \"silver.region_description\").alias(\"region_description\"),\r\n",
							"        F.coalesce(\"new_bronze_df.country\", \"silver.country\").alias(\"country\"),\r\n",
							"        F.coalesce(\"new_bronze_df.cluster\", \"silver.cluster\").alias(\"cluster\")\r\n",
							"    )\r\n",
							"    final_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"else:\r\n",
							"\r\n",
							"    new_bronze_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"    print(\"Inkrementalne ładowanie zakończone!\")   \r\n",
							"\r\n",
							"spark.stop()"
						],
						"outputs": [],
						"execution_count": 30
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_CUSTOMERS_LOAD_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1b1e679b-6ac9-45b6-9948-20863ee3ff6f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false
							},
							"collapsed": false
						},
						"source": [
							"set path_bronze_customers = 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/customers/';\n",
							"set path_bronze_countries = 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/countries/';\n",
							"set path_silver_customers = 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers';\n",
							"\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"\n",
							"-- 1. Rejestracja bronze_customers jako tymczasowy widok\n",
							"CREATE OR REPLACE TEMP VIEW bronze_customers\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path ${path_bronze_customers}\n",
							");\n",
							"\n",
							"-- 2. Rejestracja bronze_countries jako tymczasowy widok\n",
							"CREATE OR REPLACE TEMP VIEW bronze_countries\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path ${path_bronze_countries}\n",
							");\n",
							"\n",
							"-- 3. Przygotowanie danych – JOIN + wybór najnowszych po customer_id\n",
							"CREATE OR REPLACE TEMP VIEW new_bronze_d_customers AS\n",
							"SELECT \n",
							"    c.customer_id,\n",
							"    c.country_id,\n",
							"    c.company_name,\n",
							"    c.address,\n",
							"    c.city,\n",
							"    co.region_description,\n",
							"    co.country,\n",
							"    co.cluster\n",
							"FROM bronze_customers c\n",
							"JOIN bronze_countries co\n",
							"  ON c.country_id = co.country_id;\n",
							"\n",
							"-- 4. Stworzenie tabeli docelowej jako Delta Lake (jeśli nie istnieje)\n",
							"CREATE TABLE IF NOT EXISTS silver.d_customers\n",
							"USING delta\n",
							"LOCATION ${path_silver_customers}\n",
							"AS\n",
							"SELECT * FROM new_bronze_d_customers\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- 5. Merge (UPSERT) danych do tabeli docelowej\n",
							"MERGE INTO silver.d_customers AS target\n",
							"USING new_bronze_d_customers AS source\n",
							"ON target.customer_id = source.customer_id\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.country_id = source.country_id,\n",
							"    target.company_name = source.company_name,\n",
							"    target.address = source.address,\n",
							"    target.city = source.city,\n",
							"    target.region_description = source.region_description,\n",
							"    target.country = source.country,\n",
							"    target.cluster = source.cluster\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    customer_id,\n",
							"    country_id,\n",
							"    company_name,\n",
							"    address,\n",
							"    city,\n",
							"    region_description,\n",
							"    country,\n",
							"    cluster\n",
							"  )\n",
							"  VALUES (\n",
							"    source.customer_id,\n",
							"    source.country_id,\n",
							"    source.company_name,\n",
							"    source.address,\n",
							"    source.city,\n",
							"    source.region_description,\n",
							"    source.country,\n",
							"    source.cluster\n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_DIM')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load Gold"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4cab2b50-6e67-4a17-9cea-e8df25564fe8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc, lit, add_months\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql.functions import to_date\r\n",
							"\r\n",
							"# Tworzenie sesji Spark\r\n",
							"spark = SparkSession.builder.appName(\"AzureSynapseMigration\").getOrCreate()\r\n",
							"\r\n",
							"# Parametry dla warstwy Silver (ADLS Gen2)\r\n",
							"bucket_name_silver = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"\r\n",
							"# Parametry dla warstwy Gold (ADLS Gen2)\r\n",
							"bucket_name_gold = \"abfss://gold@bigpharma.dfs.core.windows.net/\"\r\n",
							"\r\n",
							"# Ścieżki do plików Parquet w ADLS Gen2 dla warstwy Silver\r\n",
							"d_products_silver_path = f\"{bucket_name_silver}/d_products\"\r\n",
							"d_regions_silver_path = f\"{bucket_name_silver}/d_customers\"\r\n",
							"\r\n",
							"# Załaduj dane z warstwy Silver\r\n",
							"d_products = spark.read.parquet(d_products_silver_path)\r\n",
							"d_regions = spark.read.parquet(d_regions_silver_path)\r\n",
							"\r\n",
							"# Ścieżki do plików Parquet w ADLS Gen2 dla warstwy Gold\r\n",
							"d_products_gold_path = f\"{bucket_name_gold}/d_products\"\r\n",
							"d_regions_gold_path = f\"{bucket_name_gold}/d_regions\"\r\n",
							"\r\n",
							"# Można również załadować dane do warstwy Gold, jeśli konieczne\r\n",
							"d_products = spark.read.parquet(d_products_silver_path)\r\n",
							"d_regions = spark.read.parquet(d_regions_silver_path)\r\n",
							"\r\n",
							"\r\n",
							"d_products = d_products.withColumnRenamed('product_id','IdProduct')\\\r\n",
							"                       .withColumnRenamed('product_name','Name')\\\r\n",
							"                       .withColumnRenamed('brand_name','Brand')\\\r\n",
							"                       .withColumnRenamed('sub_brand_name','SubBrand')\\\r\n",
							"                       .withColumnRenamed('category_name','Category')\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"d_products.write.mode(\"overwrite\").parquet(d_products_gold_path)\r\n",
							"\r\n",
							"selected_columns = d_regions[[\"country_id\", \"cluster\", \"region_description\", \"country\"]]\r\n",
							"\r\n",
							"selected_columns = selected_columns.withColumnRenamed('country_id', 'IDCountry').withColumnRenamed('region_description', 'Region').withColumnRenamed('cluster', 'Claster').withColumnRenamed('country', 'Country')\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"unique_d_regions = selected_columns.drop_duplicates()\r\n",
							"unique_d_regions.write.mode(\"overwrite\").parquet(d_regions_gold_path)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_DIM_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load Gold"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "928eb249-4f88-4365-a26e-879dbf76002a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"set path_silver_d_products = 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers';\n",
							"set path_silver_d_products = 'abfss://silver@bigpharma.dfs.core.windows.net/f_wh_data/';\n",
							"\n",
							"\n",
							"-- Załadowanie danych z warstwy Silver do tabeli Delta\n",
							"-- Tworzenie tabel Delta w warstwie Silver\n",
							"CREATE OR REPLACE TABLE silver_d_products\n",
							"USING DELTA\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/d_products';\n",
							"\n",
							"CREATE OR REPLACE TABLE silver_d_regions\n",
							"USING DELTA\n",
							"LOCATION 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers';\n",
							"\n",
							"-- Tworzenie tabel w warstwie Gold\n",
							"CREATE OR REPLACE TABLE d_products\n",
							"USING DELTA\n",
							"LOCATION 'abfss://gold@bigpharma.dfs.core.windows.net/d_products';\n",
							"\n",
							"CREATE OR REPLACE TABLE d_regions_gold\n",
							"USING DELTA\n",
							"LOCATION 'abfss://gold@bigpharma.dfs.core.windows.net/d_regions';\n",
							"\n",
							"-- Zmiana nazw kolumn w tabeli d_products i zapis do warstwy Gold\n",
							"INSERT OVERWRITE TABLE gold.d_products_gold\n",
							"SELECT \n",
							"    product_id AS IdProduct,\n",
							"    product_name AS Name,\n",
							"    brand_name AS Brand,\n",
							"    sub_brand_name AS SubBrand,\n",
							"    category_name AS Category\n",
							"FROM silver.d_products;\n",
							"\n",
							"-- Wybór odpowiednich kolumn w tabeli d_regions, zmiana nazw kolumn i zapis do warstwy Gold\n",
							"INSERT OVERWRITE TABLE gold.d_regions_gold\n",
							"SELECT \n",
							"    country_id AS IDCountry,\n",
							"    cluster AS Claster,\n",
							"    region_description AS Region,\n",
							"    country AS Country\n",
							"FROM silver.d_regions;\n",
							"\n",
							"-- Usuwanie duplikatów w tabeli d_regions i zapis do nowej tabeli Gold\n",
							"CREATE OR REPLACE TABLE gold.d_regions_gold_unique\n",
							"USING DELTA\n",
							"AS\n",
							"SELECT DISTINCT \n",
							"    country_id AS IDCountry,\n",
							"    cluster AS Claster,\n",
							"    region_description AS Region,\n",
							"    country AS Country\n",
							"FROM silver.d_regions;\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_POS_LOAD')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c61abd19-cd7f-4fd8-9d9e-1f8b7dc3514b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"\r\n",
							"spark = SparkSession.builder.appName(\"AzureSynapseMigration\").getOrCreate()\r\n",
							"\r\n",
							"# Parametry dla warstwy Bronze (ADLS Gen2)\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"file_name = \"inventory_pos_history_details\"\r\n",
							"\r\n",
							"# Parametry dla warstwy Silver (ADLS Gen2)\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"f_pos_data/\"\r\n",
							"\r\n",
							"# Ścieżki do plików Parquet w ADLS Gen2\r\n",
							"path = f\"{bronze_bucket_name}{bronze_prefix}{file_name}\"\r\n",
							"\r\n",
							"# Ścieżka do warstwy Silver\r\n",
							"silver_path = f\"{silver_bucket_name}{silver_prefix}\"\r\n",
							"\r\n",
							"# Załaduj dane z ADLS (tabela inventory_pos_history_details)\r\n",
							"bronze_df = spark.read.parquet(path)\r\n",
							"\r\n",
							"\r\n",
							"bronze_df = spark.read.parquet(path)\r\n",
							"\r\n",
							"latest_bronze_df = bronze_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"product_id\", \"country_id\",\"update_date\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"new_bronze_df = latest_bronze_df.select(\r\n",
							"    \"product_id\",\r\n",
							"    \"country_id\", \r\n",
							"    \"transaction_date\", \r\n",
							"    \"unit_price\", \r\n",
							"    \"pos_sell_out_quantity\",\r\n",
							"    \"pos_open_quantity\",  # Poprawiona nazwa\r\n",
							"    \"pos_end_quantity\"\r\n",
							")\r\n",
							"\r\n",
							"new_bronze_df = new_bronze_df.withColumn(\r\n",
							"    \"transaction_date\", trunc(\"transaction_date\",\"month\"))\r\n",
							"\r\n",
							"aggregated_bronze_df = new_bronze_df.groupBy(\"product_id\", \"country_id\", \"transaction_date\").agg(\r\n",
							"                                    # Summing quantity, discount, and amount\r\n",
							"                                     F.sum(\"pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"                                     F.sum(\"pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"                                     F.sum(\"pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_sell_out_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_sell_out_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_open_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_open_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_end_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_end_amount\"),\r\n",
							"                                     F.round((F.sum(F.col(\"pos_sell_out_quantity\") * F.col(\"unit_price\")) / F.sum(\"pos_sell_out_quantity\")),2).alias(\"unit_price\"))\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver już istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"\r\n",
							"\r\n",
							"# Jeśli tabela Silver istnieje, wykonaj operację \"MERGE\" na podstawie DataFrame\r\n",
							"if silver_exists:\r\n",
							"    # Załaduj dane z tabeli Silver\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    # Połącz dane Silver z nowymi danymi (na podstawie order_id, customer_id, product_id)\r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        aggregated_bronze_df.alias(\"new_pod\"),\r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_pod.product_id\")) & \r\n",
							"        (F.col(\"silver.country_id\") == F.col(\"new_pod.country_id\")) & \r\n",
							"        (F.col(\"silver.transaction_date\") == F.col(\"new_pod.transaction_date\")),\r\n",
							"        how=\"outer\"\r\n",
							"    )\r\n",
							"\r\n",
							"    # Wybór kolumn, które mają zostać zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_pod.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_pod.country_id\", \"silver.country_id\").alias(\"country_id\"),\r\n",
							"        F.coalesce(\"new_pod.transaction_date\", \"silver.transaction_date\").alias(\"transaction_date\"),\r\n",
							"        F.coalesce(\"new_pod.pos_sell_out_quantity\", \"silver.pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_open_quantity\", \"silver.pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_end_quantity\", \"silver.pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_sell_out_amount\", \"silver.pos_sell_out_amount\").alias(\"pos_sell_out_amount\"),\r\n",
							"        F.coalesce(\"new_pod.pos_open_amount\", \"silver.pos_open_amount\").alias(\"pos_open_amount\"),\r\n",
							"        F.coalesce(\"new_pod.pos_end_amount\", \"silver.pos_end_amount\").alias(\"pos_end_amount\"),\r\n",
							"        F.coalesce(\"new_pod.unit_price\", \"silver.unit_price\").alias(\"unit_price\")\r\n",
							"        )\r\n",
							"    # Display or save the resulting DataFrame\r\n",
							"    # Zapisanie zaktualizowanego DataFrame do warstwy Silver\r\n",
							"    final_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"else:\r\n",
							"    aggregated_bronze_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakończenie\r\n",
							"print(\"Inkrementalne ładowanie zakończone!\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"\r\n",
							"bronze_df = spark.read.parquet(path)\r\n",
							"\r\n",
							"latest_bronze_df = bronze_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"product_id\", \"country_id\",\"update_date\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"new_bronze_df = latest_bronze_df.select(\r\n",
							"    \"product_id\",\r\n",
							"    \"country_id\", \r\n",
							"    \"transaction_date\", \r\n",
							"    \"unit_price\", \r\n",
							"    \"pos_sell_out_quantity\",\r\n",
							"    \"pos_open_quantity\",  # Poprawiona nazwa\r\n",
							"    \"pos_end_quantity\"\r\n",
							")\r\n",
							"\r\n",
							"new_bronze_df = new_bronze_df.withColumn(\r\n",
							"    \"transaction_date\", trunc(\"transaction_date\",\"month\"))\r\n",
							"\r\n",
							"aggregated_bronze_df = new_bronze_df.groupBy(\"product_id\", \"country_id\", \"transaction_date\").agg(\r\n",
							"                                    # Summing quantity, discount, and amount\r\n",
							"                                     F.sum(\"pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"                                     F.sum(\"pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"                                     F.sum(\"pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_sell_out_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_sell_out_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_open_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_open_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_end_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_end_amount\"),\r\n",
							"                                     F.round((F.sum(F.col(\"pos_sell_out_quantity\") * F.col(\"unit_price\")) / F.sum(\"pos_sell_out_quantity\")),2).alias(\"unit_price\"))\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver już istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"\r\n",
							"\r\n",
							"# Jeśli tabela Silver istnieje, wykonaj operację \"MERGE\" na podstawie DataFrame\r\n",
							"if silver_exists:\r\n",
							"    # Załaduj dane z tabeli Silver\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    # Połącz dane Silver z nowymi danymi (na podstawie order_id, customer_id, product_id)\r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        aggregated_bronze_df.alias(\"new_pod\"),\r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_pod.product_id\")) & \r\n",
							"        (F.col(\"silver.country_id\") == F.col(\"new_pod.country_id\")) & \r\n",
							"        (F.col(\"silver.transaction_date\") == F.col(\"new_pod.transaction_date\")),\r\n",
							"        how=\"outer\"\r\n",
							"    )\r\n",
							"\r\n",
							"    # Wybór kolumn, które mają zostać zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_pod.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_pod.country_id\", \"silver.country_id\").alias(\"country_id\"),\r\n",
							"        F.coalesce(\"new_pod.transaction_date\", \"silver.transaction_date\").alias(\"transaction_date\"),\r\n",
							"        F.coalesce(\"new_pod.pos_sell_out_quantity\", \"silver.pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_open_quantity\", \"silver.pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_end_quantity\", \"silver.pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_sell_out_amount\", \"silver.pos_sell_out_amount\").alias(\"pos_sell_out_amount\"),\r\n",
							"        F.coalesce(\"new_pod.pos_open_amount\", \"silver.pos_open_amount\").alias(\"pos_open_amount\"),\r\n",
							"        F.coalesce(\"new_pod.pos_end_amount\", \"silver.pos_end_amount\").alias(\"pos_end_amount\"),\r\n",
							"        F.coalesce(\"new_pod.unit_price\", \"silver.unit_price\").alias(\"unit_price\")\r\n",
							"        )\r\n",
							"    # Display or save the resulting DataFrame\r\n",
							"    # Zapisanie zaktualizowanego DataFrame do warstwy Silver\r\n",
							"    final_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"else:\r\n",
							"    aggregated_bronze_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakończenie\r\n",
							"print(\"Inkrementalne ładowanie zakończone!\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_PRODUCT_LOAD')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d3080dcd-c316-4e2a-9aed-0f0945691bd3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"# Parametry dla warstwy Bronze (ADLS Gen2)\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"file_name = \"products\"  # Zmieniamy na odpowiednią nazwę pliku\r\n",
							"\r\n",
							"# Parametry dla warstwy Silver (ADLS Gen2)\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"d_products/\"\r\n",
							"\r\n",
							"# Ścieżki do plików Parquet w ADLS Gen2\r\n",
							"path = f\"{bronze_bucket_name}{bronze_prefix}{file_name}\"\r\n",
							"\r\n",
							"# Ścieżka do warstwy Silver\r\n",
							"silver_path = f\"{silver_bucket_name}{silver_prefix}\"\r\n",
							"\r\n",
							"# Załaduj dane z ADLS (tabela products)\r\n",
							"products_df = spark.read.parquet(path)\r\n",
							"\r\n",
							"\r\n",
							"#products_df.show()\r\n",
							"\r\n",
							"latest_products_df = products_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"product_id\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"\r\n",
							"# Wybór odpowiednich kolumn do zapisania w warstwie Silver\r\n",
							"new_products_df = latest_products_df.select(\r\n",
							"    \"product_id\", \r\n",
							"    \"product_name\", \r\n",
							"    \"brand_name\", \r\n",
							"    \"sub_brand_name\", \r\n",
							"    \"category_name\")\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver już istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"if silver_exists:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        new_products_df.alias(\"new_products\"),\r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_products.product_id\")),\r\n",
							"        how=\"outer\"\r\n",
							"        )\r\n",
							"\r\n",
							"        # Wybór kolumn, które mają zostać zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_products.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_products.product_name\", \"silver.product_name\").alias(\"product_name\"),\r\n",
							"        F.coalesce(\"new_products.brand_name\", \"silver.brand_name\").alias(\"brand_name\"),\r\n",
							"        F.coalesce(\"new_products.sub_brand_name\", \"silver.sub_brand_name\").alias(\"sub_brand_name\"),\r\n",
							"        F.coalesce(\"new_products.category_name\", \"silver.category_name\").alias(\"category_name\")\r\n",
							"    )\r\n",
							"else:\r\n",
							"\r\n",
							"    new_products_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakończenie\r\n",
							"print(\"Inkrementalne ładowanie zakończone!\")    \r\n",
							"\r\n",
							"\r\n",
							"spark.stop()\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"\r\n",
							"#products_df.show()\r\n",
							"\r\n",
							"latest_products_df = products_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"product_id\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"\r\n",
							"# Wybór odpowiednich kolumn do zapisania w warstwie Silver\r\n",
							"new_products_df = latest_products_df.select(\r\n",
							"    \"product_id\", \r\n",
							"    \"product_name\", \r\n",
							"    \"brand_name\", \r\n",
							"    \"sub_brand_name\", \r\n",
							"    \"category_name\")\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver już istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"if silver_exists:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        new_products_df.alias(\"new_products\"),\r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_products.product_id\")),\r\n",
							"        how=\"outer\"\r\n",
							"        )\r\n",
							"\r\n",
							"        # Wybór kolumn, które mają zostać zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_products.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_products.product_name\", \"silver.product_name\").alias(\"product_name\"),\r\n",
							"        F.coalesce(\"new_products.brand_name\", \"silver.brand_name\").alias(\"brand_name\"),\r\n",
							"        F.coalesce(\"new_products.sub_brand_name\", \"silver.sub_brand_name\").alias(\"sub_brand_name\"),\r\n",
							"        F.coalesce(\"new_products.category_name\", \"silver.category_name\").alias(\"category_name\")\r\n",
							"    )\r\n",
							"else:\r\n",
							"\r\n",
							"    new_products_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakończenie\r\n",
							"print(\"Inkrementalne ładowanie zakończone!\")    \r\n",
							"\r\n",
							"\r\n",
							"spark.stop()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/D_PRODUCT_LOAD_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "39fb2b16-adf6-48a8-8619-2f2e063389c4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"set path_bronze_products = 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/products';\n",
							"set path_silver_products = 'abfss://silver@bigpharma.dfs.core.windows.net/d_products';\n",
							"\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- ============================================\n",
							"-- 1. Rejestracja danych z Bronze jako tymczasowy widok\n",
							"-- ============================================\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW bronze_products\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path ${path_bronze_products}\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 2. Wybór najnowszych danych wg update_date\n",
							"-- ============================================\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW new_d_products AS\n",
							"SELECT \n",
							"    product_id,\n",
							"    product_name,\n",
							"    brand_name,\n",
							"    sub_brand_name,\n",
							"    category_name\n",
							"FROM bronze_products;\n",
							"\n",
							"-- ============================================\n",
							"-- 3. Stworzenie tabeli docelowej (Silver) jeśli nie istnieje\n",
							"-- ============================================\n",
							"CREATE TABLE IF NOT EXISTS silver.d_products\n",
							"USING delta\n",
							"LOCATION ${path_silver_products}\n",
							"AS\n",
							"SELECT * FROM d_products\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- ============================================\n",
							"-- 4. MERGE INTO (UPSERT) danych do tabeli Silver\n",
							"-- ============================================\n",
							"MERGE INTO silver.d_products AS target\n",
							"USING new_d_products AS source\n",
							"ON target.product_id = source.product_id\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.product_name = source.product_name,\n",
							"    target.brand_name = source.brand_name,\n",
							"    target.sub_brand_name = source.sub_brand_name,\n",
							"    target.category_name = source.category_name\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id,\n",
							"    product_name,\n",
							"    brand_name,\n",
							"    sub_brand_name,\n",
							"    category_name\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id,\n",
							"    source.product_name,\n",
							"    source.brand_name,\n",
							"    source.sub_brand_name,\n",
							"    source.category_name\n",
							"  )"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_FORCAST_LOAD')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "169a480a-accc-4e84-95e7-967e7c19a178"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"\r\n",
							"# Definicja ścieżek dla warstwy Bronze (ADLS Gen2)\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"file_name = \"forecast_details\"\r\n",
							"\r\n",
							"# Definicja ścieżek dla warstwy Silver (ADLS Gen2)\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"f_forecast/\"\r\n",
							"\r\n",
							"silver_path = f\"{silver_bucket_name}{silver_prefix}\"\r\n",
							"\r\n",
							"# Ścieżki do plików Parquet w ADLS Gen2\r\n",
							"path = f\"{bronze_bucket_name}{bronze_prefix}{file_name}\"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"\r\n",
							"bronze_df = spark.read.parquet(path)\r\n",
							"\r\n",
							"new_bronze_df = bronze_df.withColumn(\r\n",
							"    \"forecast_date\", trunc(\"forecast_date\",\"month\")).withColumn(\r\n",
							"    \"forecast_snapshot\", trunc(\"forecast_snapshot\",\"month\"))\r\n",
							"\r\n",
							"latest_bronze_df = new_bronze_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"product_id\", \"country_id\",\"forecast_date\",\"forecast_snapshot\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"new_bronze_df = latest_bronze_df.select(\r\n",
							"    \"product_id\",\r\n",
							"    \"country_id\", \r\n",
							"    \"forecast_date\", \r\n",
							"    \"forecast_snapshot\",\r\n",
							"    \"unit_price\",\r\n",
							"    \"quantity\",\r\n",
							"    \"whrs_sell_in_quantity\",\r\n",
							"    \"whrs_open_quantity\",\r\n",
							"    \"whrs_end_quantity\",\r\n",
							"    \"pos_sell_out_quantity\",\r\n",
							"    \"pos_open_quantity\",\r\n",
							"    \"pos_end_quantity\"\r\n",
							")\r\n",
							"\r\n",
							"\r\n",
							"aggregated_bronze_df = new_bronze_df.groupBy(\"product_id\", \"country_id\", \"forecast_date\",\"forecast_snapshot\").agg(\r\n",
							"                                    # Summing quantity, discount, and amount\r\n",
							"                                     F.sum(\"quantity\").alias(\"quantity\"),\r\n",
							"                                     F.sum(\"whrs_sell_in_quantity\").alias(\"whrs_sell_in_quantity\"),\r\n",
							"                                     F.sum(\"whrs_open_quantity\").alias(\"whrs_open_quantity\"),\r\n",
							"                                     F.sum(\"whrs_end_quantity\").alias(\"whrs_end_quantity\"),\r\n",
							"                                     F.sum(\"pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"                                     F.sum(\"pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"                                     F.sum(\"pos_end_quantity\").alias(\"pos_end_quantity\"),         \r\n",
							"                                     F.round(F.sum(F.col(\"quantity\") * F.col(\"unit_price\")),2).alias(\"amount\"),                                                    \r\n",
							"                                     F.round(F.sum(F.col(\"whrs_sell_in_quantity\") * F.col(\"unit_price\")),2).alias(\"whrs_sell_in_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"whrs_open_quantity\") * F.col(\"unit_price\")),2).alias(\"whrs_open_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"whrs_end_quantity\") * F.col(\"unit_price\")),2).alias(\"whrs_end_amount\"),                                                                     \r\n",
							"                                     F.round(F.sum(F.col(\"pos_sell_out_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_sell_out_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_open_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_open_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"pos_end_quantity\") * F.col(\"unit_price\")),2).alias(\"pos_end_amount\"),\r\n",
							"                                     F.round((F.sum(F.col(\"quantity\") * F.col(\"unit_price\")) / F.sum(\"quantity\")),2).alias(\"unit_price\"))\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver już istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"\r\n",
							"\r\n",
							"# Jeśli tabela Silver istnieje, wykonaj operację \"MERGE\" na podstawie DataFrame\r\n",
							"if silver_exists:\r\n",
							"    # Załaduj dane z tabeli Silver\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    # Połącz dane Silver z nowymi danymi (na podstawie order_id, customer_id, product_id)\r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        aggregated_bronze_df.alias(\"new_pod\"),\r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_pod.product_id\")) & \r\n",
							"        (F.col(\"silver.forecast_snapshot\") == F.col(\"new_pod.forecast_snapshot\")) & \r\n",
							"        (F.col(\"silver.country_id\") == F.col(\"new_pod.country_id\")) & \r\n",
							"        (F.col(\"silver.forecast_date\") == F.col(\"new_pod.forecast_date\")),\r\n",
							"        how=\"outer\"\r\n",
							"    )\r\n",
							"\r\n",
							"    # Wybór kolumn, które mają zostać zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_pod.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_pod.country_id\", \"silver.country_id\").alias(\"country_id\"),\r\n",
							"        F.coalesce(\"new_pod.forecast_date\", \"silver.forecast_date\").alias(\"forecast_date\"),\r\n",
							"        F.coalesce(\"new_pod.forecast_snapshot\", \"silver.forecast_snapshot\").alias(\"forecast_snapshot\"),\r\n",
							"   \r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.quantity\", \"silver.quantity\").alias(\"quantity\"),\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.whrs_sell_in_quantity\", \"silver.whrs_sell_in_quantity\").alias(\"whrs_sell_in_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_open_quantity\", \"silver.whrs_open_quantity\").alias(\"whrs_open_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_end_quantity\", \"silver.whrs_end_quantity\").alias(\"whrs_end_quantity\"),  \r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.pos_sell_out_quantity\", \"silver.pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_open_quantity\", \"silver.pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.pos_end_quantity\", \"silver.pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.amount\", \"silver.amount\").alias(\"amount\"),\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.whrs_sell_in_amount\", \"silver.whrs_sell_in_amount\").alias(\"whrs_sell_in_amount\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_open_amount\", \"silver.whrs_open_amount\").alias(\"whrs_open_amount\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_end_amount\", \"silver.whrs_end_amount\").alias(\"whrs_end_amount\"),\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.pos_sell_out_amount\", \"silver.pos_sell_out_amount\").alias(\"pos_sell_out_amount\"),\r\n",
							"        F.coalesce(\"new_pod.pos_open_amount\", \"silver.pos_open_amount\").alias(\"pos_open_amount\"),\r\n",
							"        F.coalesce(\"new_pod.pos_end_amount\", \"silver.pos_end_amount\").alias(\"pos_end_amount\"),\r\n",
							"\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.unit_price\", \"silver.unit_price\").alias(\"unit_price\")\r\n",
							"        )\r\n",
							"    # Display or save the resulting DataFrame\r\n",
							"    # Zapisanie zaktualizowanego DataFrame do warstwy Silver\r\n",
							"    final_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"else:\r\n",
							"    aggregated_bronze_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakończenie\r\n",
							"print(\"Inkrementalne ładowanie zakończone!\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_FORCAST_LOAD_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "51f22f9b-d335-4e5e-bc7e-161145d83829"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"set path_bronze_forecast = 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/forecast_details';\n",
							"set path_silver_f_forecast = 'abfss://silver@bigpharma.dfs.core.windows.net/f_forecast';\n",
							"\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- ============================================\n",
							"-- 1. Załaduj dane z warstwy Bronze jako tymczasowy widok\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW bronze_forecast\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path ${path_bronze_forecast}\n",
							");\n",
							"\n",
							"-- ============================================\n",
							"-- 2. Agregacja najnowszych danych na poziomie miesiąca\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_forecast AS\n",
							"SELECT \n",
							"    product_id,\n",
							"    country_id,\n",
							"    TRUNC(forecast_date, \"MM\") AS forecast_date,\n",
							"    TRUNC(forecast_snapshot, \"MM\") AS forecast_snapshot,\n",
							"    \n",
							"    SUM(quantity) AS quantity,\n",
							"    SUM(whrs_sell_in_quantity) AS whrs_sell_in_quantity,\n",
							"    SUM(whrs_open_quantity) AS whrs_open_quantity,\n",
							"    SUM(whrs_end_quantity) AS whrs_end_quantity,\n",
							"    \n",
							"    SUM(pos_sell_out_quantity) AS pos_sell_out_quantity,\n",
							"    SUM(pos_open_quantity) AS pos_open_quantity,\n",
							"    SUM(pos_end_quantity) AS pos_end_quantity,\n",
							"    \n",
							"    ROUND(SUM(quantity * unit_price), 2) AS amount,\n",
							"    ROUND(SUM(whrs_sell_in_quantity * unit_price), 2) AS whrs_sell_in_amount,\n",
							"    ROUND(SUM(whrs_open_quantity * unit_price), 2) AS whrs_open_amount,\n",
							"    ROUND(SUM(whrs_end_quantity * unit_price), 2) AS whrs_end_amount,\n",
							"    \n",
							"    ROUND(SUM(pos_sell_out_quantity * unit_price), 2) AS pos_sell_out_amount,\n",
							"    ROUND(SUM(pos_open_quantity * unit_price), 2) AS pos_open_amount,\n",
							"    ROUND(SUM(pos_end_quantity * unit_price), 2) AS pos_end_amount,\n",
							"    \n",
							"    ROUND(SUM(quantity * unit_price) / NULLIF(SUM(quantity), 0), 2) AS unit_price\n",
							"FROM  bronze_forecast\n",
							"GROUP BY product_id, country_id, TRUNC(forecast_date, \"MM\"), TRUNC(forecast_snapshot, \"MM\");\n",
							"\n",
							"-- ============================================\n",
							"-- 3. Stwórz tabelę Silver, jeśli nie istnieje\n",
							"-- ============================================\n",
							"CREATE TABLE IF NOT EXISTS silver.f_forecast\n",
							"USING delta\n",
							"LOCATION ${path_silver_f_forecast}\n",
							"AS\n",
							"SELECT * FROM aggregated_forecast\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- ============================================\n",
							"-- 4. Wykonaj MERGE INTO, aby zaktualizować lub dodać nowe dane\n",
							"-- ============================================\n",
							"MERGE INTO silver.f_forecast AS target\n",
							"USING aggregated_forecast AS source\n",
							"ON target.product_id = source.product_id\n",
							"   AND target.country_id = source.country_id\n",
							"   AND target.forecast_date = source.forecast_date\n",
							"   AND target.forecast_snapshot = source.forecast_snapshot\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.quantity = source.quantity,\n",
							"    target.whrs_sell_in_quantity = source.whrs_sell_in_quantity,\n",
							"    target.whrs_open_quantity = source.whrs_open_quantity,\n",
							"    target.whrs_end_quantity = source.whrs_end_quantity,\n",
							"    \n",
							"    target.pos_sell_out_quantity = source.pos_sell_out_quantity,\n",
							"    target.pos_open_quantity = source.pos_open_quantity,\n",
							"    target.pos_end_quantity = source.pos_end_quantity,\n",
							"    \n",
							"    target.amount = source.amount,\n",
							"    target.whrs_sell_in_amount = source.whrs_sell_in_amount,\n",
							"    target.whrs_open_amount = source.whrs_open_amount,\n",
							"    target.whrs_end_amount = source.whrs_end_amount,\n",
							"    \n",
							"    target.pos_sell_out_amount = source.pos_sell_out_amount,\n",
							"    target.pos_open_amount = source.pos_open_amount,\n",
							"    target.pos_end_amount = source.pos_end_amount,\n",
							"    \n",
							"    target.unit_price = source.unit_price\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id, country_id, forecast_date, forecast_snapshot,\n",
							"    quantity, whrs_sell_in_quantity, whrs_open_quantity, whrs_end_quantity,\n",
							"    pos_sell_out_quantity, pos_open_quantity, pos_end_quantity,\n",
							"    amount, whrs_sell_in_amount, whrs_open_amount, whrs_end_amount,\n",
							"    pos_sell_out_amount, pos_open_amount, pos_end_amount, unit_price\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id, source.country_id, source.forecast_date, source.forecast_snapshot,\n",
							"    source.quantity, source.whrs_sell_in_quantity, source.whrs_open_quantity, source.whrs_end_quantity,\n",
							"    source.pos_sell_out_quantity, source.pos_open_quantity, source.pos_end_quantity,\n",
							"    source.amount, source.whrs_sell_in_amount, source.whrs_open_amount, source.whrs_end_amount,\n",
							"    source.pos_sell_out_amount, source.pos_open_amount, source.pos_end_amount, source.unit_price\n",
							"  );\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_PLANNING_BOOK')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load Gold"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "eac3e774-7edc-46a8-bc4e-272484c2b33d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc, lit, add_months\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql.functions import to_date\r\n",
							"\r\n",
							"# Tworzenie sesji Spark\r\n",
							"spark = SparkSession.builder.appName(\"AzureSynapseMigration\").getOrCreate()\r\n",
							"\r\n",
							"# Parametry dla warstwy Silver (ADLS Gen2)\r\n",
							"bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"bucket_name_gold = \"abfss://gold@bigpharma.dfs.core.windows.net/\"\r\n",
							"\r\n",
							"# Ścieżki do plików Parquet w ADLS Gen2\r\n",
							"f_sales_path = f\"{bucket_name}/f_sales\"\r\n",
							"f_forecast_path = f\"{bucket_name}/f_forecast\"\r\n",
							"f_pos_data_path = f\"{bucket_name}/f_pos_data\"\r\n",
							"f_wh_data_path = f\"{bucket_name}/f_wh_data\"\r\n",
							"\r\n",
							"# Ścieżka do warstwy Gold (f_planning_book)\r\n",
							"f_sales_path_gold = f\"{bucket_name_gold}/f_planning_book\"\r\n",
							"\r\n",
							"# Załaduj dane z ADLS (tabele f_sales, f_forecast, f_pos_data, f_wh_data)\r\n",
							"f_sales = spark.read.parquet(f_sales_path)\r\n",
							"f_forecast = spark.read.parquet(f_forecast_path)\r\n",
							"f_pos_data = spark.read.parquet(f_pos_data_path)\r\n",
							"f_wh_data = spark.read.parquet(f_wh_data_path)\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"\r\n",
							"# Pobranie maksymalnej wartości forecast_snapshot z f_forecast\r\n",
							"current_month_snapshot = f_forecast.agg(F.max(\"forecast_snapshot\")).collect()[0][0]\r\n",
							"current_month_snapshot = to_date(lit(current_month_snapshot), \"yyyy-MM-dd\")\r\n",
							"last_month_snapshot = F.add_months(F.lit(current_month_snapshot), -1)\r\n",
							"\r\n",
							"f_forecast_filtered = f_forecast.filter(F.col(\"forecast_snapshot\").isin(current_month_snapshot, last_month_snapshot))\r\n",
							"\r\n",
							"# Filtrowanie dla obecnego i poprzedniego miesiąca\r\n",
							"f_forecast_max = f_forecast_filtered.filter((F.col(\"forecast_date\") >= current_month_snapshot) &(F.col(\"forecast_snapshot\") == current_month_snapshot))\r\n",
							"f_forecast_last = f_forecast_filtered.filter((F.col(\"forecast_date\") >= last_month_snapshot) &  (F.col(\"forecast_snapshot\") == last_month_snapshot))\r\n",
							"\r\n",
							"# Filtrowanie f_sales\r\n",
							"\r\n",
							"f_sales_max = f_sales.filter(F.col(\"shipped_date\") < current_month_snapshot)\r\n",
							"f_sales_last = f_sales.filter(F.col(\"shipped_date\") < last_month_snapshot)\r\n",
							"\r\n",
							"f_pos_data_max = f_pos_data.filter(F.col(\"transaction_date\") < current_month_snapshot)\r\n",
							"f_pos_data_last = f_pos_data.filter(F.col(\"transaction_date\") < last_month_snapshot)\r\n",
							"\r\n",
							"f_wh_data_max = f_wh_data.filter(F.col(\"transaction_date\") < current_month_snapshot)\r\n",
							"f_wh_data_last = f_wh_data.filter(F.col(\"transaction_date\") < last_month_snapshot)\r\n",
							"\r\n",
							"\r\n",
							"# Ujednolicona lista kolumn\r\n",
							"common_columns = [\r\n",
							"    \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"    \"quantity\", \"amount\", \"unit_price\",\r\n",
							"    \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"    \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"    \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"    \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"    \"discount\"\r\n",
							"]\r\n",
							"\r\n",
							"# Normalizacja f_forecast (zachowuje oryginalny forecast_snapshot)\r\n",
							"f_forecast_max_norm = f_forecast_max \\\r\n",
							"    .withColumnRenamed(\"forecast_date\", \"financial_date\") \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0.0 as discount\"  # Jeżeli discount jest typu double\r\n",
							"    )\r\n",
							"\r\n",
							"f_forecast_last_norm = f_forecast_last \\\r\n",
							"    .withColumnRenamed(\"forecast_date\", \"financial_date\") \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0.0 as discount\"  # Jeżeli discount jest typu double\r\n",
							"    )\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Normalizacja f_sales\r\n",
							"f_sales_max_norm = f_sales_max \\\r\n",
							"    .withColumnRenamed(\"shipped_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\",  current_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"f_sales_last_norm = f_sales_last \\\r\n",
							"    .withColumnRenamed(\"shipped_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", last_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"quantity\", \"amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"# Normalizacja f_pos_data_path\r\n",
							"f_pos_data_max_norm = f_pos_data_max \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", current_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"f_pos_data_last_norm = f_pos_data_last \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", last_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"0 as whrs_sell_in_quantity\", \"0 as whrs_open_quantity\", \"0 as whrs_end_quantity\",\r\n",
							"        \"0 as whrs_sell_in_amount\", \"0 as whrs_open_amount\", \"0 as whrs_end_amount\",\r\n",
							"        \"pos_sell_out_quantity\", \"pos_open_quantity\", \"pos_end_quantity\",\r\n",
							"        \"pos_sell_out_amount\", \"pos_open_amount\", \"pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"# Normalizacja f_wh_data_path\r\n",
							"f_wh_data_max_norm = f_wh_data_max \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\",  current_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"f_wh_data_last_norm = f_wh_data_last \\\r\n",
							"    .withColumnRenamed(\"transaction_date\", \"financial_date\") \\\r\n",
							"    .withColumn(\"forecast_snapshot\", last_month_snapshot) \\\r\n",
							"    .selectExpr(\r\n",
							"        \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\",\r\n",
							"        \"0 as quantity\", \"0 as amount\", \"unit_price\",\r\n",
							"        \"whrs_sell_in_quantity\", \"whrs_open_quantity\", \"whrs_end_quantity\",\r\n",
							"        \"whrs_sell_in_amount\", \"whrs_open_amount\", \"whrs_end_amount\",\r\n",
							"        \"0 as pos_sell_out_quantity\", \"0 as pos_open_quantity\", \"0 as pos_end_quantity\",\r\n",
							"        \"0 as pos_sell_out_amount\", \"0 as pos_open_amount\", \"0 as pos_end_amount\",\r\n",
							"        \"0 as discount\"\r\n",
							"    )\r\n",
							"\r\n",
							"\r\n",
							"# UNION wszystkich DataFrame'ów\r\n",
							"\r\n",
							"f_forecast_max_norm\r\n",
							"f_forecast_last_norm\r\n",
							"f_sales_max_norm \r\n",
							"f_sales_last_norm\r\n",
							"f_pos_data_max_norm\r\n",
							"f_pos_data_last_norm\r\n",
							"f_wh_data_max_norm\r\n",
							"f_wh_data_last_norm\r\n",
							"\r\n",
							"final_df = f_forecast_max_norm.unionByName(f_forecast_last_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_sales_max_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_sales_last_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_pos_data_max_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_pos_data_last_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_wh_data_max_norm, allowMissingColumns=True) \\\r\n",
							"    .unionByName(f_wh_data_last_norm, allowMissingColumns=True) \r\n",
							"\r\n",
							"\r\n",
							"bucket_name_gold = \"gold\"\r\n",
							"\r\n",
							"# Load Data\r\n",
							"f_sales_path = f\"s3a://{bucket_name}/f_planning_book\"\r\n",
							"\r\n",
							"aggregated_df = final_df.groupBy(\r\n",
							"    \"product_id\", \"country_id\", \"financial_date\", \"forecast_snapshot\"\r\n",
							").agg(\r\n",
							"    # Suma dla kolumn liczbowych\r\n",
							"    F.sum(\"quantity\").alias(\"quantity\"),\r\n",
							"    F.sum(\"amount\").alias(\"amount\"),\r\n",
							"    F.avg(\"unit_price\").alias(\"unit_price\"),\r\n",
							"    \r\n",
							"    # Suma dla innych kolumn\r\n",
							"    F.sum(\"whrs_sell_in_quantity\").alias(\"whrs_sell_in_quantity\"),\r\n",
							"    F.sum(\"whrs_open_quantity\").alias(\"whrs_open_quantity\"),\r\n",
							"    F.sum(\"whrs_end_quantity\").alias(\"whrs_end_quantity\"),\r\n",
							"    F.sum(\"whrs_sell_in_amount\").alias(\"whrs_sell_in_amount\"),\r\n",
							"    F.sum(\"whrs_open_amount\").alias(\"whrs_open_amount\"),\r\n",
							"    F.sum(\"whrs_end_amount\").alias(\"whrs_end_amount\"),\r\n",
							"    \r\n",
							"    F.sum(\"pos_sell_out_quantity\").alias(\"pos_sell_out_quantity\"),\r\n",
							"    F.sum(\"pos_open_quantity\").alias(\"pos_open_quantity\"),\r\n",
							"    F.sum(\"pos_end_quantity\").alias(\"pos_end_quantity\"),\r\n",
							"    F.sum(\"pos_sell_out_amount\").alias(\"pos_sell_out_amount\"),\r\n",
							"    F.sum(\"pos_open_amount\").alias(\"pos_open_amount\"),\r\n",
							"    F.sum(\"pos_end_amount\").alias(\"pos_end_amount\"),\r\n",
							"    \r\n",
							"    # Agregacja dla discount (średnia)\r\n",
							"    F.sum(\"discount\").alias(\"discount\")\r\n",
							")\r\n",
							"\r\n",
							"\r\n",
							"aggregated_df.write.mode(\"overwrite\").parquet(f_sales_path_gold)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\r\n",
							"\r\n",
							"# Załaduj dane\r\n",
							"df_pandas = aggregated_df.toPandas()\r\n",
							"\r\n",
							"df_pandas = df_pandas.drop(columns=['discount', 'unit_price'])\r\n",
							"\r\n",
							"# Unpivotowanie (melt)\r\n",
							"df_melted = df_pandas.melt(\r\n",
							"   id_vars=['product_id', 'country_id', 'financial_date', 'forecast_snapshot'],\r\n",
							"    var_name=\"original_column\",\r\n",
							"    value_name=\"value\"\r\n",
							")\r\n",
							"\r\n",
							"\r\n",
							"# Tworzenie nowej kolumny 'measure' na podstawie obecności 'quantity' lub 'amount' w nazwie kolumny\r\n",
							"df_melted[\"Measure\"] = df_melted[\"original_column\"].apply(lambda x: \"EA\" if \"quantity\" in x.lower() else \"GTS\" if \"amount\" in x.lower() else \"unknown\")\r\n",
							"\r\n",
							"# Mapowanie nazw kolumn na nowe formaty\r\n",
							"measure_mapping = {\r\n",
							"    'quantity': 'Ex-Factory',\r\n",
							"    'amount': 'Ex-Factory',\r\n",
							"    'whrs_sell_in_quantity': 'Sales to pharmacies',\r\n",
							"    'whrs_open_quantity': 'Open Stock',\r\n",
							"    'whrs_end_quantity': 'Close Stock',\r\n",
							"    'whrs_sell_in_amount': 'Sales to pharmacies',\r\n",
							"    'whrs_open_amount': 'Open Stock',\r\n",
							"    'whrs_end_amount': 'Close Stock',\r\n",
							"    'pos_sell_out_quantity': 'Sales to pharmacies',\r\n",
							"    'pos_open_quantity': 'Open Stock Pharmacies',\r\n",
							"    'pos_end_quantity': 'Close Stock Pharmacies',\r\n",
							"    'pos_sell_out_amount': 'Sales to pharmacies',\r\n",
							"    'pos_open_amount': 'Open Stock Pharmacies',\r\n",
							"    'pos_end_amount': 'Close Stock Pharmacies'\r\n",
							"}\r\n",
							"\r\n",
							"# Zastosowanie mapowania\r\n",
							"df_melted[\"original_column\"] = df_melted[\"original_column\"].replace(measure_mapping)\r\n",
							"\r\n",
							"# Usuwamy wiersze z brakującymi wartościami\r\n",
							"df_melted = df_melted.dropna(subset=[\"original_column\", \"value\"])\r\n",
							"\r\n",
							"# Pivotowanie danych\r\n",
							"df_pivoted = df_melted.pivot_table(\r\n",
							"    index=['product_id', 'country_id', 'financial_date', 'forecast_snapshot', 'Measure'],  \r\n",
							"    columns='original_column',\r\n",
							"    values='value',\r\n",
							"    aggfunc='first'  # lub 'sum', jeśli chcesz sumować wartości\r\n",
							").reset_index()\r\n",
							"\r\n",
							"# Usunięcie nazwy kolumny po pivotowaniu\r\n",
							"df_pivoted.columns.name = None\r\n",
							"\r\n",
							"\r\n",
							"# Znalezienie maksymalnej daty\r\n",
							"max_forecast_snapshot = df_pivoted[\"forecast_snapshot\"].max()\r\n",
							"\r\n",
							"# Dodanie kolumny status\r\n",
							"df_pivoted[\"Version\"] =  df_pivoted[\"forecast_snapshot\"].apply(lambda x: \"Current\" if x == max_forecast_snapshot  else \"Last\")\r\n",
							"\r\n",
							"bucket_name_gold = \"abfss://gold@bigpharma.dfs.core.windows.net/\"\r\n",
							"\r\n",
							"# Ścieżka do warstwy Gold (f_planning_book)\r\n",
							"f_sales_path_gold = f\"{bucket_name_gold}/f_planning_book_unpivot\"\r\n",
							"\r\n",
							"df_spark = spark.createDataFrame(df_pivoted)\r\n",
							"df_spark=df_spark.withColumnRenamed('product_id', 'IdProduct').withColumnRenamed('country_id', 'IDCountry').withColumnRenamed('financial_date', 'Date')\r\n",
							"\r\n",
							"df_spark.write.mode(\"overwrite\").parquet(f_sales_path_gold)\r\n",
							"df_spark.show()"
						],
						"outputs": [],
						"execution_count": 39
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_PLANNING_BOOK_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load Gold"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3afcc781-cdbd-4a1b-bbb7-e60fecf1cd97"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"-- Tworzenie sesji Spark i załadowanie danych z Parquet\n",
							"CREATE OR REPLACE TEMP VIEW f_sales AS \n",
							"SELECT * FROM parquet.`abfss://silver@bigpharma.dfs.core.windows.net/f_sales`;\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW f_forecast AS \n",
							"SELECT * FROM parquet.`abfss://silver@bigpharma.dfs.core.windows.net/f_forecast`;\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW f_pos_data AS \n",
							"SELECT * FROM parquet.`abfss://silver@bigpharma.dfs.core.windows.net/f_pos_data`;\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW f_wh_data AS \n",
							"SELECT * FROM parquet.`abfss://silver@bigpharma.dfs.core.windows.net/f_wh_data`;\n",
							"\n",
							"-- Pobranie maksymalnej wartości forecast_snapshot\n",
							"WITH forecast_snapshots AS (\n",
							"  SELECT MAX(forecast_snapshot) AS current_month_snapshot\n",
							"  FROM f_forecast\n",
							"),\n",
							"last_month_snapshot AS (\n",
							"  SELECT ADD_MONTHS(current_month_snapshot, -1) AS last_month_snapshot\n",
							"  FROM forecast_snapshots\n",
							")\n",
							"\n",
							"-- Filtrowanie dla obecnego i poprzedniego miesiąca\n",
							"CREATE OR REPLACE TEMP VIEW f_forecast_filtered AS\n",
							"SELECT *\n",
							"FROM f_forecast\n",
							"WHERE forecast_snapshot IN (SELECT current_month_snapshot FROM forecast_snapshots UNION SELECT last_month_snapshot FROM last_month_snapshot);\n",
							"\n",
							"-- Normalizacja f_forecast dla bieżącego i poprzedniego miesiąca\n",
							"CREATE OR REPLACE TEMP VIEW f_forecast_max_norm AS \n",
							"SELECT \n",
							"  product_id,\n",
							"  country_id,\n",
							"  forecast_date AS financial_date,\n",
							"  forecast_snapshot,\n",
							"  quantity,\n",
							"  amount,\n",
							"  unit_price,\n",
							"  0.0 AS discount,\n",
							"  whrs_sell_in_quantity,\n",
							"  whrs_open_quantity,\n",
							"  whrs_end_quantity,\n",
							"  whrs_sell_in_amount,\n",
							"  whrs_open_amount,\n",
							"  whrs_end_amount,\n",
							"  pos_sell_out_quantity,\n",
							"  pos_open_quantity,\n",
							"  pos_end_quantity,\n",
							"  pos_sell_out_amount,\n",
							"  pos_open_amount,\n",
							"  pos_end_amount\n",
							"FROM f_forecast_filtered\n",
							"WHERE forecast_snapshot = (SELECT current_month_snapshot FROM forecast_snapshots);\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW f_forecast_last_norm AS \n",
							"SELECT \n",
							"  product_id,\n",
							"  country_id,\n",
							"  forecast_date AS financial_date,\n",
							"  forecast_snapshot,\n",
							"  quantity,\n",
							"  amount,\n",
							"  unit_price,\n",
							"  0.0 AS discount,\n",
							"  whrs_sell_in_quantity,\n",
							"  whrs_open_quantity,\n",
							"  whrs_end_quantity,\n",
							"  whrs_sell_in_amount,\n",
							"  whrs_open_amount,\n",
							"  whrs_end_amount,\n",
							"  pos_sell_out_quantity,\n",
							"  pos_open_quantity,\n",
							"  pos_end_quantity,\n",
							"  pos_sell_out_amount,\n",
							"  pos_open_amount,\n",
							"  pos_end_amount\n",
							"FROM f_forecast_filtered\n",
							"WHERE forecast_snapshot = (SELECT last_month_snapshot FROM last_month_snapshot);\n",
							"\n",
							"-- Normalizacja f_sales, f_pos_data, f_wh_data (analogiczne operacje)\n",
							"-- Filtrowanie i tworzenie normowanych widoków dla innych tabel (f_sales, f_pos_data, f_wh_data)...\n",
							"\n",
							"-- Przykład dla f_sales_max_norm\n",
							"CREATE OR REPLACE TEMP VIEW f_sales_max_norm AS\n",
							"SELECT \n",
							"  product_id,\n",
							"  country_id,\n",
							"  shipped_date AS financial_date,\n",
							"  forecast_snapshot,\n",
							"  quantity,\n",
							"  amount,\n",
							"  unit_price,\n",
							"  0 AS whrs_sell_in_quantity,\n",
							"  0 AS whrs_open_quantity,\n",
							"  0 AS whrs_end_quantity,\n",
							"  0 AS whrs_sell_in_amount,\n",
							"  0 AS whrs_open_amount,\n",
							"  0 AS whrs_end_amount,\n",
							"  0 AS pos_sell_out_quantity,\n",
							"  0 AS pos_open_quantity,\n",
							"  0 AS pos_end_quantity,\n",
							"  0 AS pos_sell_out_amount,\n",
							"  0 AS pos_open_amount,\n",
							"  0 AS pos_end_amount,\n",
							"  discount\n",
							"FROM f_sales\n",
							"WHERE shipped_date < (SELECT current_month_snapshot FROM forecast_snapshots);\n",
							"\n",
							"-- Dla f_pos_data_max_norm i f_wh_data_max_norm również tworzysz widoki normalizujące\n",
							"-- Przykład dla f_pos_data_max_norm\n",
							"CREATE OR REPLACE TEMP VIEW f_pos_data_max_norm AS\n",
							"SELECT \n",
							"  product_id,\n",
							"  country_id,\n",
							"  transaction_date AS financial_date,\n",
							"  forecast_snapshot,\n",
							"  0 AS quantity,\n",
							"  0 AS amount,\n",
							"  unit_price,\n",
							"  0 AS whrs_sell_in_quantity,\n",
							"  0 AS whrs_open_quantity,\n",
							"  0 AS whrs_end_quantity,\n",
							"  0 AS whrs_sell_in_amount,\n",
							"  0 AS whrs_open_amount,\n",
							"  0 AS whrs_end_amount,\n",
							"  pos_sell_out_quantity,\n",
							"  pos_open_quantity,\n",
							"  pos_end_quantity,\n",
							"  pos_sell_out_amount,\n",
							"  pos_open_amount,\n",
							"  pos_end_amount,\n",
							"  0 AS discount\n",
							"FROM f_pos_data\n",
							"WHERE transaction_date < (SELECT current_month_snapshot FROM forecast_snapshots);\n",
							"\n",
							"-- Podobnie tworzysz normowane dane dla f_wh_data_max_norm\n",
							"\n",
							"-- Łączenie wszystkich znormalizowanych danych\n",
							"CREATE OR REPLACE TEMP VIEW final_data AS\n",
							"SELECT * FROM f_forecast_max_norm\n",
							"UNION ALL\n",
							"SELECT * FROM f_forecast_last_norm\n",
							"UNION ALL\n",
							"SELECT * FROM f_sales_max_norm\n",
							"UNION ALL\n",
							"SELECT * FROM f_pos_data_max_norm\n",
							"UNION ALL\n",
							"SELECT * FROM f_wh_data_max_norm;\n",
							"\n",
							"-- Agregowanie danych\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_data AS\n",
							"SELECT \n",
							"  product_id,\n",
							"  country_id,\n",
							"  financial_date,\n",
							"  forecast_snapshot,\n",
							"  SUM(quantity) AS quantity,\n",
							"  SUM(amount) AS amount,\n",
							"  AVG(unit_price) AS unit_price,\n",
							"  SUM(whrs_sell_in_quantity) AS whrs_sell_in_quantity,\n",
							"  SUM(whrs_open_quantity) AS whrs_open_quantity,\n",
							"  SUM(whrs_end_quantity) AS whrs_end_quantity,\n",
							"  SUM(whrs_sell_in_amount) AS whrs_sell_in_amount,\n",
							"  SUM(whrs_open_amount) AS whrs_open_amount,\n",
							"  SUM(whrs_end_amount) AS whrs_end_amount,\n",
							"  SUM(pos_sell_out_quantity) AS pos_sell_out_quantity,\n",
							"  SUM(pos_open_quantity) AS pos_open_quantity,\n",
							"  SUM(pos_end_quantity) AS pos_end_quantity,\n",
							"  SUM(pos_sell_out_amount) AS pos_sell_out_amount,\n",
							"  SUM(pos_open_amount) AS pos_open_amount,\n",
							"  SUM(pos_end_amount) AS pos_end_amount,\n",
							"  SUM(discount) AS discount\n",
							"FROM final_data\n",
							"GROUP BY product_id, country_id, financial_date, forecast_snapshot;\n",
							"\n",
							"-- Unpivotowanie danych (jeśli chcesz przekształcić dane do formatu long)\n",
							"CREATE OR REPLACE TEMP VIEW unpivoted_data AS\n",
							"SELECT \n",
							"  product_id,\n",
							"  country_id,\n",
							"  financial_date,\n",
							"  forecast_snapshot,\n",
							"  original_column,\n",
							"  value,\n",
							"  CASE \n",
							"    WHEN original_column LIKE '%quantity%' THEN 'EA' \n",
							"    WHEN original_column LIKE '%amount%' THEN 'GTS' \n",
							"    ELSE 'unknown' \n",
							"  END AS Measure\n",
							"FROM aggregated_data\n",
							"LATERAL VIEW explode(array('quantity', 'amount', 'whrs_sell_in_quantity', 'whrs_open_quantity', 'whrs_end_quantity', 'whrs_sell_in_amount', 'whrs_open_amount', 'whrs_end_amount', 'pos_sell_out_quantity', 'pos_open_quantity', 'pos_end_quantity', 'pos_sell_out_amount', 'pos_open_amount', 'pos_end_amount', 'discount')) AS original_column\n",
							"WHERE value IS NOT NULL;\n",
							"\n",
							"-- Pivotowanie danych z unpivotowanych\n",
							"CREATE OR REPLACE TEMP VIEW pivoted_data AS\n",
							"SELECT \n",
							"  product_id,\n",
							"  country_id,\n",
							"  financial_date,\n",
							"  forecast_snapshot,\n",
							"  Measure,\n",
							"  MAX(CASE WHEN original_column = 'quantity' THEN value END) AS ExFactory,\n",
							"  MAX(CASE WHEN original_column = 'amount' THEN value END) AS ExFactoryAmount,\n",
							"  -- Dodaj inne kolumny na podstawie potrzeb\n",
							"FROM unpivoted_data\n",
							"GROUP BY product_id, country_id, financial_date, forecast_snapshot, Measure;\n",
							"\n",
							"-- Zapisywanie wyników do tabeli Delta w warstwie Gold\n",
							"CREATE OR REPLACE TABLE delta.`abfss://gold@bigpharma.dfs.core.windows.net/f_planning_book`\n",
							"USING DELTA\n",
							"AS \n",
							"SELECT * FROM pivoted_data;\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_POS_LOAD_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a3a09c18-6348-408a-93e8-e3e459feed88"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"set path_bronze_pos = 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/inventory_pos_history_details';\n",
							"set path_silver_f_pos_data = 'abfss://silver@bigpharma.dfs.core.windows.net/f_pos_data';\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- ============================================\n",
							"-- 1. Rejestracja danych z Bronze jako widok tymczasowy\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW bronze_pos\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path '${path_bronze_pos}'\n",
							");\n",
							"-- ============================================\n",
							"-- 2. Przygotowanie danych – najnowsze update + agregacja miesięczna\n",
							"-- ============================================\n",
							"CREATE OR REPLACE TEMP VIEW aggregated_bronze_pos AS\n",
							"SELECT \n",
							"    product_id,\n",
							"    country_id,\n",
							"    TRUNC(transaction_date, \"MM\") AS transaction_date,\n",
							"    SUM(pos_sell_out_quantity) AS pos_sell_out_quantity,\n",
							"    SUM(pos_open_quantity) AS pos_open_quantity,\n",
							"    SUM(pos_end_quantity) AS pos_end_quantity,\n",
							"    ROUND(SUM(pos_sell_out_quantity * unit_price), 2) AS pos_sell_out_amount,\n",
							"    ROUND(SUM(pos_open_quantity * unit_price), 2) AS pos_open_amount,\n",
							"    ROUND(SUM(pos_end_quantity * unit_price), 2) AS pos_end_amount,\n",
							"    ROUND(SUM(pos_sell_out_quantity * unit_price) / NULLIF(SUM(pos_sell_out_quantity), 0), 2) AS unit_price\n",
							"FROM  bronze_pos\n",
							"GROUP BY product_id, country_id, TRUNC(transaction_date, \"MM\");\n",
							"\n",
							"\n",
							"-- ============================================\n",
							"-- 3. Stworzenie tabeli docelowej (Silver) jeśli nie istnieje\n",
							"-- ============================================\n",
							"CREATE TABLE IF NOT EXISTS silver.f_pos_data\n",
							"USING delta\n",
							"LOCATION '${path_silver_f_pos_data}'\n",
							"AS\n",
							"SELECT * FROM aggregated_bronze_pos\n",
							"WHERE 1 = 0;\n",
							"\n",
							"-- ============================================\n",
							"-- 4. MERGE INTO (UPSERT) danych do tabeli Delta\n",
							"-- ============================================\n",
							"MERGE INTO silver.f_pos_data AS target\n",
							"USING aggregated_bronze_pos AS source\n",
							"ON target.product_id = source.product_id\n",
							"   AND target.country_id = source.country_id\n",
							"   AND target.transaction_date = source.transaction_date\n",
							"\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    target.pos_sell_out_quantity = source.pos_sell_out_quantity,\n",
							"    target.pos_open_quantity = source.pos_open_quantity,\n",
							"    target.pos_end_quantity = source.pos_end_quantity,\n",
							"    target.pos_sell_out_amount = source.pos_sell_out_amount,\n",
							"    target.pos_open_amount = source.pos_open_amount,\n",
							"    target.pos_end_amount = source.pos_end_amount,\n",
							"    target.unit_price = source.unit_price\n",
							"\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id,\n",
							"    country_id,\n",
							"    transaction_date,\n",
							"    pos_sell_out_quantity,\n",
							"    pos_open_quantity,\n",
							"    pos_end_quantity,\n",
							"    pos_sell_out_amount,\n",
							"    pos_open_amount,\n",
							"    pos_end_amount,\n",
							"    unit_price\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id,\n",
							"    source.country_id,\n",
							"    source.transaction_date,\n",
							"    source.pos_sell_out_quantity,\n",
							"    source.pos_open_quantity,\n",
							"    source.pos_end_quantity,\n",
							"    source.pos_sell_out_amount,\n",
							"    source.pos_open_amount,\n",
							"    source.pos_end_amount,\n",
							"    source.unit_price\n",
							"  );"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_SALES_LOAD')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c4b7f2d3-9034-4bbf-8b7a-6f34134a40fb"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"\r\n",
							"#dla warstwy Bronze (ADLS Gen2)\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"\r\n",
							"orders_path = f\"{bronze_bucket_name}{bronze_prefix}orders\"\r\n",
							"order_details_path = f\"{bronze_bucket_name}{bronze_prefix}order_details\"\r\n",
							"\r\n",
							"# Definicja ścieżek dla warstwy Silver (ADLS Gen2)\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"f_sales/\"\r\n",
							"\r\n",
							"silver_path = f\"{silver_bucket_name}{silver_prefix}\"\r\n",
							"silver_customers = f\"{silver_bucket_name}d_customers\"\r\n",
							"\r\n",
							"# Załaduj dane z ADLS (tabele orders, order_details, customers)\r\n",
							"orders_df = spark.read.parquet(orders_path)\r\n",
							"order_details_df = spark.read.parquet(order_details_path)\r\n",
							"customers_df = spark.read.parquet(silver_customers)\r\n",
							"\r\n",
							"# Wyświetlenie pierwszych wierszy dla weryfikacji\r\n",
							"orders_df.show(5)\r\n",
							"order_details_df.show(5)\r\n",
							"customers_df.show(5)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"orders_df = orders_df.withColumn(\r\n",
							"    \"order_date\", trunc(\"order_date\",\"month\")\r\n",
							").withColumn(\r\n",
							"    \"required_date\", trunc(\"required_date\",\"month\")\r\n",
							").withColumn(\r\n",
							"    \"shipped_date\", trunc(\"shipped_date\",\"month\")\r\n",
							")\r\n",
							"\r\n",
							"\r\n",
							"joined_df = orders_df.join(order_details_df, \"order_id\").join(customers_df,\"customer_id\")\r\n",
							"\r\n",
							"# Użycie okna do filtrowania najnowszych zamówień\r\n",
							"latest_order_df = joined_df.withColumn(\r\n",
							"    \"max_order_date\", F.max(\"order_date\").over(Window.partitionBy(\"order_id\"))\r\n",
							").filter(F.col(\"order_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"# Wybór odpowiednich kolumn do zapisania w warstwie Silver\r\n",
							"new_order_details_df = latest_order_df.select(\r\n",
							"    \"country_id\",\r\n",
							"    \"product_id\",  \r\n",
							"    \"shipped_date\", \r\n",
							"    \"unit_price\", \r\n",
							"    \"quantity\", \r\n",
							"    \"discount\"\r\n",
							")\r\n",
							"\r\n",
							"\r\n",
							"aggregated_df = new_order_details_df.groupBy(\"country_id\", \"product_id\", \"shipped_date\").agg(\r\n",
							"                                     F.sum(\"quantity\").alias(\"quantity\"),\r\n",
							"                                     F.sum(\"discount\").alias(\"discount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"quantity\") * F.col(\"unit_price\")),2).alias(\"amount\"),\r\n",
							"                                     F.round((F.sum(F.col(\"quantity\") * F.col(\"unit_price\")) / F.sum(\"quantity\")),2).alias(\"unit_price\"))\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver już istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"\r\n",
							"\r\n",
							"# Jeśli tabela Silver istnieje, wykonaj operację \"MERGE\" na podstawie DataFrame\r\n",
							"if silver_exists:\r\n",
							"    # Załaduj dane z tabeli Silver\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    # Połącz dane Silver z nowymi danymi (na podstawie order_id, customer_id, product_id)\r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        aggregated_df.alias(\"new_orders\"),\r\n",
							"        (F.col(\"silver.shipped_date\") == F.col(\"new_orders.shipped_date\")) & \r\n",
							"        (F.col(\"silver.country_id\") == F.col(\"new_orders.country_id\")) & \r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_orders.product_id\")),\r\n",
							"        how=\"outer\"\r\n",
							"    )\r\n",
							"\r\n",
							"    # Wybór kolumn, które mają zostać zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_orders.country_id\", \"silver.country_id\").alias(\"country_id\"),\r\n",
							"        F.coalesce(\"new_orders.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_orders.shipped_date\", \"silver.shipped_date\").alias(\"shipped_date\"), \r\n",
							"        F.coalesce(\"new_orders.quantity\", \"silver.quantity\").alias(\"quantity\"),\r\n",
							"        F.coalesce(\"new_orders.discount\", \"silver.discount\").alias(\"discount\"),\r\n",
							"        F.coalesce(\"new_orders.amount\", \"silver.amount\").alias(\"amount\"),\r\n",
							"       F.coalesce(\"new_orders.unit_price\", \"silver.unit_price\").alias(\"unit_price\")\r\n",
							"    )\r\n",
							"\r\n",
							"    # Zapisanie zaktualizowanego DataFrame do warstwy Silver\r\n",
							"\r\n",
							"    final_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"else:\r\n",
							"\r\n",
							"    # Zapisanie zaktualizowanego DataFrame do warstwy Silver\r\n",
							"\r\n",
							"    aggregated_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakończenie\r\n",
							"print(\"Inkrementalne ładowanie zakończone!\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_SALES_LOAD_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "55b9b954-d4e2-48fd-b67c-1b9aa2a74d8e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": true
							},
							"collapsed": false
						},
						"source": [
							"set path_bronze_orders = 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/orders';\n",
							"set path_bronze_order_details = 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/order_details';\n",
							"set path_silver_d_customers = 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers';\n",
							"set path_silver_f_sales = 'abfss://silver@bigpharma.dfs.core.windows.net/f_sales/';\n",
							"\n",
							"\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"-- 1. Tymczasowe widoki na pliki z ADLS Gen2\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW bronze_orders\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path ${path_bronze_orders }\n",
							");\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW bronze_order_details\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path ${path_bronze_order_details}\n",
							");\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW d_customers\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path ${path_silver_d_customers}\n",
							");\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS silver.f_sales (\n",
							"    country_id STRING,\n",
							"    product_id STRING,\n",
							"    shipped_date DATE,\n",
							"    quantity DOUBLE,\n",
							"    discount DOUBLE,\n",
							"    amount DOUBLE,\n",
							"    unit_price DOUBLE\n",
							")\n",
							"USING DELTA\n",
							"LOCATION ${path_silver_f_sales};\n",
							"\n",
							"-- 3. Przekształcenie i zapis danych do tabeli Silver z użyciem MERGE\n",
							"MERGE INTO silver.f_sales AS target\n",
							"USING (\n",
							"    WITH orders_trunc AS (\n",
							"        SELECT\n",
							"            order_id,\n",
							"            customer_id,\n",
							"            TRUNC(order_date, 'MM') AS order_date,\n",
							"            TRUNC(required_date, 'MM') AS required_date,\n",
							"            TRUNC(shipped_date, 'MM') AS shipped_date\n",
							"        FROM bronze_orders\n",
							"    ),\n",
							"    joined_orders AS (\n",
							"        SELECT\n",
							"            o.order_id,\n",
							"            c.country_id,\n",
							"            od.product_id,\n",
							"            o.shipped_date,\n",
							"            od.unit_price,\n",
							"            od.quantity,\n",
							"            od.discount,\n",
							"            o.order_date\n",
							"        FROM orders_trunc o\n",
							"        JOIN bronze_order_details od ON o.order_id = od.order_id\n",
							"        JOIN d_customers c ON o.customer_id = c.customer_id\n",
							"    ),\n",
							"    final_orders AS (\n",
							"        SELECT\n",
							"            country_id,\n",
							"            product_id,\n",
							"            shipped_date,\n",
							"            SUM(quantity) AS quantity,\n",
							"            SUM(discount) AS discount,\n",
							"            ROUND(SUM(quantity * unit_price), 2) AS amount,\n",
							"            ROUND(SUM(quantity * unit_price) / NULLIF(SUM(quantity), 0), 2) AS unit_price\n",
							"        FROM joined_orders\n",
							"        GROUP BY country_id, product_id, shipped_date\n",
							"    )\n",
							"    SELECT * FROM  final_orders\n",
							") AS source\n",
							"ON target.country_id = source.country_id\n",
							"   AND target.product_id = source.product_id\n",
							"   AND target.shipped_date = source.shipped_date\n",
							"WHEN MATCHED THEN\n",
							"    UPDATE SET\n",
							"        target.quantity = source.quantity,\n",
							"        target.discount = source.discount,\n",
							"        target.amount = source.amount,\n",
							"        target.unit_price = source.unit_price\n",
							"WHEN NOT MATCHED THEN\n",
							"    INSERT (\n",
							"        target.country_id, target.product_id, target.shipped_date,\n",
							"        target.quantity, target.discount, target.amount, target.unit_price\n",
							"    )\n",
							"    VALUES (\n",
							"        source.country_id, source.product_id, source.shipped_date,\n",
							"        source.quantity, source.discount, source.amount, source.unit_price\n",
							"    );\n",
							""
						],
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_WH_DATA_LOAD')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Silver"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0f42fa21-f94e-48a0-8ece-d7d0c1644aef"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"\r\n",
							"# Tworzenie sesji Spark\r\n",
							"spark = SparkSession.builder.appName(\"AzureSynapseMigration\").getOrCreate()\r\n",
							"\r\n",
							"# Parametry dla warstwy Bronze (ADLS Gen2)\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"file_name = \"inventory_wholesaler_history_details\"\r\n",
							"\r\n",
							"# Parametry dla warstwy Silver (ADLS Gen2)\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"f_wh_data/\"\r\n",
							"\r\n",
							"silver_path = f\"{silver_bucket_name}/{silver_prefix}\"\r\n",
							"\r\n",
							"# Ścieżki do plików Parquet w ADLS Gen2\r\n",
							"path = f\"{bronze_bucket_name}{bronze_prefix}{file_name}\"\r\n",
							"path_customers =  f\"{silver_bucket_name}d_customers\"\r\n",
							"\r\n",
							"# Załaduj dane z ADLS (tabela inventory_wholesaler_history_details)\r\n",
							"bronze_df = spark.read.parquet(path)\r\n",
							"customers_df = spark.read.parquet(path_customers)\r\n",
							"\r\n",
							"# Wykonaj JOIN na obu DataFrame'ach na podstawie customer_id\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, max, coalesce, trunc\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"\r\n",
							"# Tworzenie sesji Spark\r\n",
							"spark = SparkSession.builder.appName(\"AzureSynapseMigration\").getOrCreate()\r\n",
							"\r\n",
							"# Parametry dla warstwy Bronze (ADLS Gen2)\r\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\r\n",
							"bronze_prefix = \"Erp/\"\r\n",
							"file_name = \"inventory_wholesaler_history_details\"\r\n",
							"\r\n",
							"# Parametry dla warstwy Silver (ADLS Gen2)\r\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\r\n",
							"silver_prefix = \"f_wh_data/\"\r\n",
							"\r\n",
							"silver_path = f\"{silver_bucket_name}/{silver_prefix}\"\r\n",
							"\r\n",
							"# Ścieżki do plików Parquet w ADLS Gen2\r\n",
							"path = f\"{bronze_bucket_name}{bronze_prefix}{file_name}\"\r\n",
							"path_customers =  f\"{silver_bucket_name}d_customers\"\r\n",
							"\r\n",
							"# Załaduj dane z ADLS (tabela inventory_wholesaler_history_details)\r\n",
							"bronze_df = spark.read.parquet(path)\r\n",
							"customers_df = spark.read.parquet(path_customers)\r\n",
							"\r\n",
							"# Wykonaj JOIN na obu DataFrame'ach na podstawie customer_id\r\n",
							"\r\n",
							"\r\n",
							"bronze_df=bronze_df.join(customers_df,\"customer_id\")\r\n",
							"\r\n",
							"latest_bronze_df = bronze_df.withColumn(\"max_order_date\", F.max(\"update_date\").over(Window.partitionBy(\"product_id\", \"customer_id\",\"update_date\"))).filter(F.col(\"update_date\") == F.col(\"max_order_date\"))\r\n",
							"\r\n",
							"new_bronze_df = latest_bronze_df.select(\r\n",
							"    \"product_id\",\r\n",
							"    \"country_id\", \r\n",
							"    \"transaction_date\", \r\n",
							"    \"unit_price\", \r\n",
							"    \"whrs_sell_in_quantity\",\r\n",
							"    \"whrs_open_quantity\",  # Poprawiona nazwa\r\n",
							"    \"whrs_end_quantity\"\r\n",
							")\r\n",
							"\r\n",
							"new_bronze_df = new_bronze_df.withColumn(\r\n",
							"    \"transaction_date\", trunc(\"transaction_date\",\"month\"))\r\n",
							"\r\n",
							"aggregated_bronze_df = new_bronze_df.groupBy(\"product_id\", \"country_id\", \"transaction_date\").agg(\r\n",
							"                                    # Summing quantity, discount, and amount\r\n",
							"                                     F.sum(\"whrs_sell_in_quantity\").alias(\"whrs_sell_in_quantity\"),\r\n",
							"                                     F.sum(\"whrs_open_quantity\").alias(\"whrs_open_quantity\"),\r\n",
							"                                     F.sum(\"whrs_end_quantity\").alias(\"whrs_end_quantity\"),\r\n",
							"                                     F.round(F.sum(F.col(\"whrs_sell_in_quantity\") * F.col(\"unit_price\")),2).alias(\"whrs_sell_in_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"whrs_open_quantity\") * F.col(\"unit_price\")),2).alias(\"whrs_open_amount\"),\r\n",
							"                                     F.round(F.sum(F.col(\"whrs_end_quantity\") * F.col(\"unit_price\")),2).alias(\"whrs_end_amount\"),\r\n",
							"                                     F.round((F.sum(F.col(\"whrs_sell_in_quantity\") * F.col(\"unit_price\")) / F.sum(\"whrs_sell_in_quantity\")),2).alias(\"unit_price\"))\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Sprawdzenie, czy tabela Silver już istnieje\r\n",
							"try:\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    silver_exists = True\r\n",
							"except:\r\n",
							"    silver_exists = False\r\n",
							"\r\n",
							"\r\n",
							"# Jeśli tabela Silver istnieje, wykonaj operację \"MERGE\" na podstawie DataFrame\r\n",
							"if silver_exists:\r\n",
							"    # Załaduj dane z tabeli Silver\r\n",
							"    silver_df = spark.read.parquet(silver_path)\r\n",
							"    \r\n",
							"    # Połącz dane Silver z nowymi danymi (na podstawie order_id, customer_id, product_id)\r\n",
							"    merged_df = silver_df.alias(\"silver\").join(\r\n",
							"        aggregated_bronze_df.alias(\"new_pod\"),\r\n",
							"        (F.col(\"silver.product_id\") == F.col(\"new_pod.product_id\")) & \r\n",
							"        (F.col(\"silver.country_id\") == F.col(\"new_pod.country_id\")) & \r\n",
							"        (F.col(\"silver.transaction_date\") == F.col(\"new_pod.transaction_date\")),\r\n",
							"        how=\"outer\"\r\n",
							"    )\r\n",
							"\r\n",
							"    # Wybór kolumn, które mają zostać zaktualizowane lub dodane\r\n",
							"    final_df = merged_df.select(\r\n",
							"        F.coalesce(\"new_pod.product_id\", \"silver.product_id\").alias(\"product_id\"),\r\n",
							"        F.coalesce(\"new_pod.country_id\", \"silver.country_id\").alias(\"country_id\"),\r\n",
							"        F.coalesce(\"new_pod.transaction_date\", \"silver.transaction_date\").alias(\"transaction_date\"),\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.whrs_sell_in_quantity\", \"silver.whrs_sell_in_quantity\").alias(\"whrs_sell_in_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_open_quantity\", \"silver.whrs_open_quantity\").alias(\"whrs_open_quantity\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_end_quantity\", \"silver.whrs_end_quantity\").alias(\"whrs_end_quantity\"),\r\n",
							"        \r\n",
							"        F.coalesce(\"new_pod.whrs_sell_in_amount\", \"silver.whrs_sell_in_amount\").alias(\"whrs_sell_in_amount\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_open_amount\", \"silver.whrs_open_amount\").alias(\"whrs_open_amount\"),\r\n",
							"        F.coalesce(\"new_pod.whrs_end_amount\", \"silver.whrs_end_amount\").alias(\"whrs_end_amount\"),\r\n",
							"        F.coalesce(\"new_pod.unit_price\", \"silver.unit_price\").alias(\"unit_price\")\r\n",
							"        )\r\n",
							"    # Display or save the resulting DataFrame\r\n",
							"    # Zapisanie zaktualizowanego DataFrame do warstwy Silver\r\n",
							"    final_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"else:\r\n",
							"    aggregated_bronze_df.write.mode(\"overwrite\").parquet(silver_path)\r\n",
							"\r\n",
							"# Zakończenie\r\n",
							"print(\"Inkrementalne ładowanie zakończone!\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/F_WH_DATA_LOAD_SQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "05bb69f0-9035-4488-9d2d-c3941ff12599"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"set path_bronze_inventory = 'abfss://bronze@bigpharma.dfs.core.windows.net/Erp/inventory_wholesaler_history_details';\n",
							"set path_silver_d_customers = 'abfss://silver@bigpharma.dfs.core.windows.net/d_customers';\n",
							"set path_silver_f_wh_data = 'abfss://silver@bigpharma.dfs.core.windows.net/f_wh_data/';\n",
							"\n",
							"\n",
							"CREATE DATABASE IF NOT EXISTS silver;\n",
							"\n",
							"-- 1. Załaduj dane z Bronze i Customers jako tymczasowe widoki\n",
							"CREATE OR REPLACE TEMP VIEW bronze_inventory\n",
							"USING parquet\n",
							"OPTIONS (\n",
							"  path ${path_bronze_inventory}\n",
							");\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW silver_d_customers\n",
							"USING delta\n",
							"OPTIONS (\n",
							"  path ${path_silver_d_customers}\n",
							");\n",
							"\n",
							"-- 2. Utwórz tabelę docelową Silver jako Delta (jeśli nie istnieje)\n",
							"CREATE TABLE IF NOT EXISTS silver.f_wh_data (\n",
							"  product_id STRING,\n",
							"  country_id STRING,\n",
							"  transaction_date DATE,\n",
							"  whrs_sell_in_quantity DOUBLE,\n",
							"  whrs_open_quantity DOUBLE,\n",
							"  whrs_end_quantity DOUBLE,\n",
							"  whrs_sell_in_amount DOUBLE,\n",
							"  whrs_open_amount DOUBLE,\n",
							"  whrs_end_amount DOUBLE,\n",
							"  unit_price DOUBLE\n",
							")\n",
							"USING DELTA\n",
							"LOCATION {path_silver_f_wh_data};\n",
							"\n",
							"-- 3. MERGE danych do tabeli Silver z agregacją i filtrowaniem\n",
							"MERGE INTO silver.f_wh_data AS target\n",
							"USING (\n",
							"    WITH enriched AS (\n",
							"        SELECT\n",
							"            inv.*,\n",
							"            c.country_id\n",
							"        FROM bronze_inventory inv\n",
							"        JOIN silver_customers c ON inv.customer_id = c.customer_id\n",
							"    ),\n",
							"    filtered AS (\n",
							"        SELECT\n",
							"            product_id,\n",
							"            country_id,\n",
							"            TRUNC(transaction_date, 'MM') AS transaction_date,\n",
							"            unit_price,\n",
							"            whrs_sell_in_quantity,\n",
							"            whrs_open_quantity,\n",
							"            whrs_end_quantity\n",
							"        FROM enriched\n",
							"    ),\n",
							"    aggregated AS (\n",
							"        SELECT\n",
							"            product_id,\n",
							"            country_id,\n",
							"            transaction_date,\n",
							"            SUM(whrs_sell_in_quantity) AS whrs_sell_in_quantity,\n",
							"            SUM(whrs_open_quantity) AS whrs_open_quantity,\n",
							"            SUM(whrs_end_quantity) AS whrs_end_quantity,\n",
							"            ROUND(SUM(whrs_sell_in_quantity * unit_price), 2) AS whrs_sell_in_amount,\n",
							"            ROUND(SUM(whrs_open_quantity * unit_price), 2) AS whrs_open_amount,\n",
							"            ROUND(SUM(whrs_end_quantity * unit_price), 2) AS whrs_end_amount,\n",
							"            ROUND(SUM(whrs_sell_in_quantity * unit_price) / NULLIF(SUM(whrs_sell_in_quantity), 0), 2) AS unit_price\n",
							"        FROM filtered\n",
							"        GROUP BY product_id, country_id, transaction_date\n",
							"    )\n",
							"    SELECT * FROM aggregated\n",
							") AS source\n",
							"ON target.product_id = source.product_id\n",
							"   AND target.country_id = source.country_id\n",
							"   AND target.transaction_date = source.transaction_date\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET\n",
							"    whrs_sell_in_quantity = source.whrs_sell_in_quantity,\n",
							"    whrs_open_quantity = source.whrs_open_quantity,\n",
							"    whrs_end_quantity = source.whrs_end_quantity,\n",
							"    whrs_sell_in_amount = source.whrs_sell_in_amount,\n",
							"    whrs_open_amount = source.whrs_open_amount,\n",
							"    whrs_end_amount = source.whrs_end_amount,\n",
							"    unit_price = source.unit_price\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT (\n",
							"    product_id, country_id, transaction_date,\n",
							"    whrs_sell_in_quantity, whrs_open_quantity, whrs_end_quantity,\n",
							"    whrs_sell_in_amount, whrs_open_amount, whrs_end_amount,\n",
							"    unit_price\n",
							"  )\n",
							"  VALUES (\n",
							"    source.product_id, source.country_id, source.transaction_date,\n",
							"    source.whrs_sell_in_quantity, source.whrs_open_quantity, source.whrs_end_quantity,\n",
							"    source.whrs_sell_in_amount, source.whrs_open_amount, source.whrs_end_amount,\n",
							"    source.unit_price\n",
							"  );\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load_Distributors_Data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Bronze"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "acbd9008-a401-454a-b644-a14771acf57f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%configure -f\n",
							"{\n",
							"    \"conf\": {\n",
							"        \"spark.jars.packages\": \"org.apache.hadoop:hadoop-aws:3.3.1\"\n",
							"    }\n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils\n",
							"\n",
							"access_key=mssparkutils.credentials.getSecret('keylakehouse','s3-access-key')\n",
							"\n",
							"secret_key =mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key')\n",
							"\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.utils import AnalysisException\n",
							"from pyspark.sql.functions import input_file_name\n",
							"\n",
							"# Ścieżka do DeltaTable z listą przetworzonych plików\n",
							"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/Processed_Files_Logs\"\n",
							"\n",
							"try:\n",
							"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
							"    print(\"Załadowano istniejący plik Delta z przetworzonymi plikami.\")\n",
							"except AnalysisException:\n",
							"   processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
							"   print(\"Plik Delta nie istnieje. Tworzony nowy pusty DataFrame.\")\n",
							"\n",
							"\n",
							"\n",
							"# Parametry MinIO\n",
							"bucket_name = \"distributors\"\n",
							"folder_path = \"data/\"\n",
							"\n",
							"# Wczytanie wszystkich plików CSV z MinIO (z subfolderami)\n",
							"files_metadata = spark.read \\\n",
							"    .option(\"header\", \"true\") \\\n",
							"    .csv(f\"s3a://{bucket_name}/{folder_path}*\") \\\n",
							"    .withColumn(\"input_file\", input_file_name())\n",
							"\n",
							"# Wyciągnięcie nazw plików\n",
							"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
							"\n",
							"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
							"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
							"\n",
							"# Lista już przetworzonych plików\n",
							"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
							"\n",
							"# Wyszukanie nowych plików do przetworzenia\n",
							"new_paths = [paths for paths in all_paths if  paths not in processed_filenames]\n",
							"\n",
							"print(f\"Znaleziono {len(new_paths)} nowych sciezek do przetworzenia.\")\n",
							"\n",
							"# Przetwarzanie nowych plików\n",
							"for path in new_paths:\n",
							"    file_path = path\n",
							"    file_path= file_path.split(\"?\")[0]\n",
							"    print(file_path)\n",
							"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
							"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
							"    print(f\"Wczytywanie sciezki: {file_path}\")\n",
							"    print(f\"Wczytywanie pliku: {file_name}\")\n",
							"    print(f\"Wczytywanie sciezki: {country_name}\")\n",
							"\n",
							"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Distributors/{country_name}/{file_name}\"\n",
							"\n",
							"\n",
							"    print(f\" Wczytywanie pliku: {file_path}\")\n",
							"\n",
							"    try:\n",
							"        df_new = spark.read.option(\"header\", \"true\").csv(file_path)\n",
							"        df_new.write.mode(\"overwrite\").csv(target_path)\n",
							"        print(f\"Zapisano plik do ADLS: {target_path}\")\n",
							"    except Exception as e:\n",
							"        print(f\"Błąd podczas przetwarzania pliku {file}: {e}\")\n",
							"\n",
							"# Aktualizacja listy przetworzonych plików\n",
							"if new_paths:\n",
							"    new_files_df = spark.createDataFrame([(path,) for path in new_paths], [\"pathname\"])\n",
							"    processed_files_updated = processed_df.union(new_files_df)\n",
							"    processed_files_updated.show()\n",
							"    # Zapis zaktualizowanej listy w formacie Delta\n",
							"    processed_files_updated.write.format(\"delta\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"       .save(processed_files_path)\n",
							"\n",
							"    print(f\"Zaktualizowano {processed_files_updated.count()} listę przetworzonych plików w Delta.\")\n",
							"else:\n",
							"    print(\"Brak nowych plików do przetworzenia.\")\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"processed_files_updated.show()"
						],
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load_Pharmacies_Data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Bronze"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d3291823-7460-455c-a667-82f5ad040976"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/825011b4-60a2-4e32-a6f1-221c4bc009ef/resourceGroups/lakehouse/providers/Microsoft.Synapse/workspaces/mrgbigpharma/bigDataPools/sparkpool32",
						"name": "sparkpool32",
						"type": "Spark",
						"endpoint": "https://mrgbigpharma.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%configure -f\n",
							"{\n",
							"    \"conf\": {\n",
							"        \"spark.jars.packages\": \"org.apache.hadoop:hadoop-aws:3.3.1\"\n",
							"    }\n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils\n",
							"\n",
							"access_key=mssparkutils.credentials.getSecret('keylakehouse','s3-access-key')\n",
							"\n",
							"secret_key =mssparkutils.credentials.getSecret('keylakehouse','s3-secret-key')\n",
							"\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://20.215.33.25:9000\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
							"spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.utils import AnalysisException\n",
							"from pyspark.sql.functions import input_file_name\n",
							"\n",
							"# Ścieżka do DeltaTable z listą przetworzonych plików\n",
							"processed_files_path = \"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/Processed_Files_Logs\"\n",
							"\n",
							"try:\n",
							"    processed_df = spark.read.format(\"delta\").load(processed_files_path)\n",
							"    print(\"Załadowano istniejący plik Delta z przetworzonymi plikami.\")\n",
							"except AnalysisException:\n",
							"   processed_df = spark.createDataFrame([], \"filepaths STRING\")\n",
							"   print(\"Plik Delta nie istnieje. Tworzony nowy pusty DataFrame.\")\n",
							"\n",
							"\n",
							"\n",
							"# Parametry MinIO\n",
							"bucket_name = \"pharmacies\"\n",
							"folder_path = \"data/\"\n",
							"\n",
							"# Wczytanie wszystkich plików CSV z MinIO (z subfolderami)\n",
							"files_metadata = spark.read \\\n",
							"    .option(\"header\", \"true\") \\\n",
							"    .csv(f\"s3a://{bucket_name}/{folder_path}*\") \\\n",
							"    .withColumn(\"input_file\", input_file_name())\n",
							"\n",
							"# Wyciągnięcie nazw plików\n",
							"all_files = files_metadata.select(\"input_file\").rdd.flatMap(lambda x: x).collect()\n",
							"\n",
							"all_filenames = [file.split(\"/\")[-1] for file in all_files]\n",
							"all_paths = list(set([file.split(\",\")[-1] for file in all_files]))\n",
							"\n",
							"# Lista już przetworzonych plików\n",
							"processed_filenames = [row.filepaths for row in processed_df.collect()]\n",
							"\n",
							"# Wyszukanie nowych plików do przetworzenia\n",
							"new_paths = [paths for paths in all_paths if  paths not in processed_filenames]\n",
							"\n",
							"print(f\"Znaleziono {len(new_paths)} nowych sciezek do przetworzenia.\")\n",
							"\n",
							"# Przetwarzanie nowych plików\n",
							"for path in new_paths:\n",
							"    file_path = path\n",
							"    file_path= file_path.split(\"?\")[0]\n",
							"    print(file_path)\n",
							"    file_name = [part for part in file_path.split(\"/\") if \"closed_month=\" in part][0].split(\"=\")[1] #file_path.split(\"/\")[-1]\n",
							"    country_name = [segment for segment in file_path.split(\"/\") if segment.startswith(\"country=\")][0].split(\"=\")[1]\n",
							"    print(f\"Wczytywanie sciezki: {file_path}\")\n",
							"    print(f\"Wczytywanie pliku: {file_name}\")\n",
							"    print(f\"Wczytywanie sciezki: {country_name}\")\n",
							"\n",
							"    target_path = f\"abfss://bronze@bigpharma.dfs.core.windows.net/MinIo_Pharmacies/{country_name}/{file_name}\"\n",
							"\n",
							"\n",
							"    print(f\" Wczytywanie pliku: {file_path}\")\n",
							"\n",
							"    try:\n",
							"        df_new = spark.read.option(\"header\", \"true\").csv(file_path)\n",
							"        df_new.write.mode(\"overwrite\").csv(target_path)\n",
							"        print(f\"Zapisano plik do ADLS: {target_path}\")\n",
							"    except Exception as e:\n",
							"        print(f\"Błąd podczas przetwarzania pliku {file}: {e}\")\n",
							"\n",
							"# Aktualizacja listy przetworzonych plików\n",
							"if new_paths:\n",
							"    new_files_df = spark.createDataFrame([(path,) for path in new_paths], [\"pathname\"])\n",
							"    processed_files_updated = processed_df.union(new_files_df)\n",
							"    processed_files_updated.show()\n",
							"    # Zapis zaktualizowanej listy w formacie Delta\n",
							"    processed_files_updated.write.format(\"delta\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"       .save(processed_files_path)\n",
							"\n",
							"    print(f\"Zaktualizowano {processed_files_updated.count()} listę przetworzonych plików w Delta.\")\n",
							"else:\n",
							"    print(\"Brak nowych plików do przetworzenia.\")\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"processed_files_updated.show()"
						],
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Load_Bronze"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "656cc32b-4b19-49cb-9c47-044763fc3dc4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import col, max\n",
							"from pyspark.sql.window import Window\n",
							"from pyspark.sql import functions as F\n",
							"from delta.tables import DeltaTable\n",
							"\n",
							"# Ścieżki\n",
							"bronze_bucket_name = \"abfss://bronze@bigpharma.dfs.core.windows.net/\"\n",
							"bronze_prefix = \"Erp/\"\n",
							"file_name = \"customers\"\n",
							"bronze_path = f\"{bronze_bucket_name}{bronze_prefix}{file_name}/*.parquet\"\n",
							"join_path = f\"{bronze_bucket_name}{bronze_prefix}countries/*.parquet\"\n",
							"\n",
							"silver_bucket_name = \"abfss://silver@bigpharma.dfs.core.windows.net/\"\n",
							"silver_prefix = \"d_customers\"\n",
							"silver_path = f\"{silver_bucket_name}{silver_prefix}\"\n",
							"\n",
							"# Wczytanie danych\n",
							"bronze_df = spark.read.format(\"parquet\").load(bronze_path)\n",
							"join_bronze_df = spark.read.format(\"parquet\").load(join_path)\n",
							"\n",
							"# Zmiana nazw kolumn z kolizją\n",
							"join_bronze_df = join_bronze_df.select(\n",
							"    [col(c).alias(f\"{c}_right\") if c != \"country_id\" else col(c) for c in join_bronze_df.columns]\n",
							")\n",
							"\n",
							"# Join\n",
							"joined_df = bronze_df.join(join_bronze_df, on=\"country_id\", how=\"inner\")\n",
							"\n",
							"# Najnowsze dane wg customer_id\n",
							"latest_bronze_df = joined_df.withColumn(\n",
							"    \"max_order_date\", max(\"update_date\").over(Window.partitionBy(\"customer_id\"))\n",
							").filter(col(\"update_date\") == col(\"max_order_date\"))\n",
							"\n",
							"# Usuwanie sufiksów _right\n",
							"new_column_names = [col_name.replace(\"_right\", \"\") for col_name in latest_bronze_df.columns]\n",
							"new_bronze_df = latest_bronze_df.toDF(*new_column_names)\n",
							"\n",
							"# Kolumny końcowe\n",
							"new_bronze_df = new_bronze_df.select(\n",
							"    \"customer_id\",\n",
							"    \"country_id\", \n",
							"    \"company_name\", \n",
							"    \"address\", \n",
							"    \"city\",\n",
							"    \"region_description\", \n",
							"    \"country\", \n",
							"    \"cluster\"\n",
							")\n",
							"\n",
							"# Zapis z Delta Lake\n",
							"try:\n",
							"    delta_table = DeltaTable.forPath(spark, silver_path)\n",
							"    delta_table.alias(\"target\").merge(\n",
							"        new_bronze_df.alias(\"source\"),\n",
							"        \"target.customer_id = source.customer_id\"\n",
							"    ).whenMatchedUpdateAll() \\\n",
							"     .whenNotMatchedInsertAll() \\\n",
							"     .execute()\n",
							"\n",
							"    print(\"Zaktualizowano istniejącą tabelę Delta (UPSERT).\")\n",
							"\n",
							"except Exception as e:\n",
							"    print(f\"Nie znaleziono tabeli Delta — zostanie utworzona. Szczegóły: {str(e)}\")\n",
							"    new_bronze_df.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n",
							"    print(\"Zapisano nową tabelę Delta.\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkpool32')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "polandcentral"
		}
	]
}